@incollection{KRISHNARAJ2022413,
title = {Chapter Fifteen - EDGE/FOG computing paradigm: Concept, platforms and toolchains},
editor = {Pethuru Raj and Kavita Saini and Chellammal Surianarayanan},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {127},
pages = {413-436},
year = {2022},
booktitle = {Edge/Fog Computing Paradigm: The Concept Platforms and Applications},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2022.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0065245822000419},
author = {N. Krishnaraj and A. Daniel and Kavita Saini and Kiranmai Bellam},
keywords = {Cloud computing, Edge computing, Fog computing, Internet of things, Mobility analytics, Security},
abstract = {At the moment, Web applications operating on smart phones create huge amounts of data that may be processed on the Cloud. Yet, one of a Cloud's core limitations is its connection to endpoint devices. Through the use of distributed compute, communication, and storage services along the Cloud to Things (C2T) continuum, fog computing overcomes this constraint and empowers new application possibilities such as smart cities, augmented reality (AR), and virtual reality (VR) (VR). Furthermore, the use of Fog-based computing resources and its incorporation with the Cloud brings new resource management issues, necessitating the development of new techniques to ensure application quality of service (QoS) compliance. In this setting, a critical challenge is how to link application QoS needs to Fog and Cloud resources. One possibility is to classify the applications that arrive at the Fog into Classes of Service (CoS). Thus, this article provides a set of CoS for fog applications that incorporates the QoS criteria that most accurately describe these fog applications. Additionally, this article suggests the use of a standard machine learning classification approach to differentiate Fog computing applications based on their QoS needs. Additionally, this technique is demonstrated through the evaluation of classifiers' efficiency, accuracy, and robustness to noise. Adopting a technique for machine learning-based categorization is a first step in defining methods for providing QoS in fog computing. Additionally, categorizing Fog computing applications can aid the Fog scheduler's decision-making process.}
}
@article{TANG2023101730,
title = {Combining reinforcement learning method to enhance LEDBAT++ over diversified network environments},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {9},
pages = {101730},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101730},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002847},
author = {Siyuan Tang and Xianliang Jiang and Menghan Zhang and Guang Jin and Haiming Chen},
keywords = {Congestion control, Diversified networks, Low priority, Reinforcement learning, Heuristics},
abstract = {LEDBAT++ is a novel less-than-best-effort congestion control algorithm. However, it still shows aggressiveness when competing with CUBIC in shallow buffer networks or with BBRv2 in low latency networks. In order to maintain its low-priority performance over diversified network environments, we propose PeaceKeeper, which combines reinforcement learning algorithm to dynamically adjust the target based on the network state. Extensive simulations show that, compared to LEDBAT++, the throughput of the primary flow competing with PeaceKeeper improved by 30.76% to 173.63%. Additionally, compared to heuristics adjusting the target, PeaceKeeper increases the link bandwidth utilization by 50.91%.}
}
@article{VITALI2022423,
title = {Special issue on co-design of data and computation management in Fog Computing},
journal = {Future Generation Computer Systems},
volume = {129},
pages = {423-424},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004301},
author = {Monica Vitali and Pierluigi Plebani and David Bermbach and Erik Elmroth}
}
@article{YEH2022100,
title = {Realizing dynamic resource orchestration on cloud systems in the cloud-to-edge continuum},
journal = {Journal of Parallel and Distributed Computing},
volume = {160},
pages = {100-109},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521002045},
author = {Tsozen Yeh and Shengchieh Yu},
keywords = {Cloud computing, Edge computing, Hadoop, HDFS},
abstract = {Cloud computing has been widely utilized to handle the huge volume of data from many cutting-edge research areas such as Big Data and Internet of Things (IoT). The fast growing of edge devices makes it difficult for cloud systems to process all data and jobs originating from edge devices, which leads to the development of edge computing by completing jobs on edges instead of clouds. Unfortunately, edge devices generally possess only limited computing power. Therefore, jobs demanding heavy computation under strict time constraints could have more difficulties to successfully complete their work on edges than on clouds in the Cloud-to-Edge continuum. If cloud systems could dynamically orchestrate cloud resources to expedite the execution of those jobs, not only their timely execution could be assured, also the loading of edge devices could be reduced. The Apache Hadoop is considered one of the most popular cloud systems in industry and academia. However, it does not support dynamic resource allocation. Previously we proposed and implemented a new model which can dynamically adjust the computing resources assigned to given jobs in the Hadoop cloud system to speed up their execution. Like other computer software, cloud systems completely rely on their underlying operating systems to access hardware components such as CPUs and hard drives. In this paper, we report our efforts to improve our model to collaborate with the Linux operating system to accelerate the execution of jobs with high priority to a greater extent. Compared with what our original model achieved, experiments show that our ameliorated model could further quicken the execution of prioritized jobs in Hadoop by up to around 21%. As a result, jobs from edges that require substantial computing resources promptly could have better chances to get accomplished on cloud systems.}
}
@article{COLONNELLI2022282,
title = {Distributed workflows with Jupyter},
journal = {Future Generation Computer Systems},
volume = {128},
pages = {282-298},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003976},
author = {Iacopo Colonnelli and Marco Aldinucci and Barbara Cantalupo and Luca Padovani and Sergio Rabellino and Concetto Spampinato and Roberto Morelli and Rosario {Di Carlo} and Nicolò Magini and Carlo Cavazzoni},
keywords = {Distributed computing, Jupyter notebooks, Streamflow, Workflow management systems},
abstract = {The designers of a new coordination interface enacting complex workflows have to tackle a dichotomy: choosing a language-independent or language-dependent approach. Language-independent approaches decouple workflow models from the host code’s business logic and advocate portability. Language-dependent approaches foster flexibility and performance by adopting the same host language for business and coordination code. Jupyter Notebooks, with their capability to describe both imperative and declarative code in a unique format, allow taking the best of the two approaches, maintaining a clear separation between application and coordination layers but still providing a unified interface to both aspects. We advocate the Jupyter Notebooks’ potential to express complex distributed workflows, identifying the general requirements for a Jupyter-based Workflow Management System (WMS) and introducing a proof-of-concept portable implementation working on hybrid Cloud-HPC infrastructures. As a byproduct, we extended the vanilla IPython kernel with workflow-based parallel and distributed execution capabilities. The proposed Jupyter-workflow (Jw) system is evaluated on common scenarios for High Performance Computing (HPC) and Cloud, showing its potential in lowering the barriers between prototypical Notebooks and production-ready implementations.}
}
@article{GOSCINSKI2023157,
title = {Special issue on Distributed Intelligence at the Edge for the Future Internet of Things},
journal = {Journal of Parallel and Distributed Computing},
volume = {171},
pages = {157-162},
year = {2023},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S074373152200209X},
author = {Andrzej Goscinski and Flavia C. Delicato and Giancarlo Fortino and Anna Kobusińska and Gautam Srivastava}
}
@article{WANG2023105529,
title = {A coupled FEM-DEM study on mechanical behaviors of granular soils considering particle breakage},
journal = {Computers and Geotechnics},
volume = {160},
pages = {105529},
year = {2023},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2023.105529},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X23002860},
author = {Yu Wang and Jia-Yan Nie and Shiwei Zhao and Hao Wang},
keywords = {Coupled FEM-DEM modeling, Particle breakage, Biaxial compression test, Particle strength, Particle shape, Confining pressure},
abstract = {Particle breakage often occurs in the fields of geological, geotechnical and hydraulic engineering like rock avalanche, landslides and high earth-rock dam, which affects the design and countermeasures of relevant engineering structures. Investigation of particle breakage’s effect on mechanical behaviors of granular soils has always been the hotspot within the community of particulate materials. In this study, a coupled finite element method (FEM) and discrete element method (DEM) modeling has been developed to tackle boundary value problems (BVPs) related with particle breakage. In the method, the particle breakage simulation is implemented in DEM through the conventional bonded particle modeling (BPM), while the stress–strain characteristics of representative volume element (RVE) captured by DEM is directly transmitted into the FEM solver to avoid the complicated and phenomenological constitutive equations incorporating into particle breakage. As an illustration, we apply the proposed multiscale modeling to investigating mechanical behaviors of quartz sands subjected to particle breakage through the high confining pressure biaxial compression simulation. And effects of particle strength, particle shape and confining pressure on particle breakage characteristics and further mechanical behaviors of quartz sands are also discussed accompanied with the underlying micromechanisms. Results demonstrate that the coupled FEM-DEM modeling can be able to deal with the complex geomechanics problems involving particle breakage.}
}
@article{ZHANG2023105432,
title = {A new creep contact model for frozen soils and its application},
journal = {Computers and Geotechnics},
volume = {159},
pages = {105432},
year = {2023},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2023.105432},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X23001891},
author = {Ge Zhang and Enlong Liu and Ruihong Wang and Bingtang Song},
keywords = {Frozen soils, Contact model, Discrete element simulation, Creep mechanical properties, Secondary development},
abstract = {To study the bond effect on the creep mechanical properties of frozen soils, a creep contact model for frozen soils is proposed based on the generalized Kelvin body. Tensile and shear strength micro-parameters were introduced to reflect the cementation properties of frozen soils. A subroutine (DLL), called by the particle flow program (PFC3D), was generated using the language C++. Triaxial creep tests were carried out on frozen soils under different axial stress conditions. A comparison of the numerical simulation and laboratory test results showed that the proposed contact model can well reflect the creep mechanical properties of frozen soils. Based on the calibrated model micro-parameters, a series of triaxial creep simulations were carried out to study the effects of the cementation state, confining pressure, and damage state on the creep mechanical properties. The simulation results showed that the cementation state, confining pressure, and damage state have important influences on the creep mechanical properties of frozen soils. The characteristics of the creep curves, number of cracks, and number of force chains with the creep time were thoroughly analyzed. These results can provide a basis for studies on macro-meso creep mechanical properties of frozen soils.}
}
@article{ROBLESENCISO2023109476,
title = {A multi-layer guided reinforcement learning-based tasks offloading in edge computing},
journal = {Computer Networks},
volume = {220},
pages = {109476},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109476},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622005102},
author = {Alberto Robles-Enciso and Antonio F. Skarmeta},
keywords = {Internet of Things, Fog computing, Edge computing, Task offloading, Resource allocation, Markov decision process, Reinforcement learning, Q-learning},
abstract = {The breakthrough in Machine Learning (ML) techniques and the popularity of the Internet of Things (IoT) has increased interest in applying Artificial Intelligence (AI) techniques to the new paradigm of Edge Computing. One of the challenges in edge computing architectures is the optimal distribution of the generated tasks between the devices in each layer (i.e., cloud-fog-edge). In this paper, we propose to use Reinforcement Learning (RL) to solve the Task Assignment Problem (TAP) at the edge layer and then we propose a novel multi-layer extension of RL (ML-RL) techniques that allows edge agents to query an upper-level agent with more knowledge to improve the performance in complex and uncertain situations. We first formulate the task assignment process considering the trade-off between energy consumption and execution time. We then present a greedy solution as a baseline and implement our multi-layer RL proposal in the PureEdgeSim simulator. Finally several simulations of each algorithm are evaluated with different numbers of devices to verify scalability. The simulation results show that reinforcement learning solutions outperformed the heuristic-based solutions and our multi-layer approach can significantly improve performance in high device density scenarios.}
}
@article{GARCIA2021792,
title = {Data-flow driven optimal tasks distribution for global heterogeneous systems},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {792-805},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002806},
author = {Jordi Garcia and Francesc Aguiló and Adrià Asensio and Ester Simó and Marisa Zaragozá and Xavi Masip-Bruin},
keywords = {Edge computing, Distributed computing, Heterogeneous systems, Task distribution, Task offloading, Resources allocation},
abstract = {As a result of advances in technology and highly demanding users expectations, more and more applications require intensive computing resources and, most importantly, high consumption of data distributed throughout the environment. For this reason, there has been an increasing number of research efforts to cooperatively use geographically distributed resources, working in parallel and sharing resources and data. In fact, an application can be structured into a set of tasks organized through interdependent relationships, some of which can be effectively executed in parallel, notably speeding up the execution time. In this work a model is proposed aimed at offloading tasks execution in heterogeneous environments, considering different nodes computing capacity connected through distinct network bandwidths, and located at different distances. In the envisioned model, the focus is on the overhead produced when accessing remote data sources as well as the data transfer cost generated between tasks at run-time. The novelty of this approach is that the mechanism proposed for tasks allocation is data-flow aware, considering the geographical location of both, computing nodes and data sources, ending up in an optimal solution to a highly complex problem. Two optimization strategies are proposed, the Optimal Matching Model and the Staged Optimization Model, as two different approaches to obtain a solution to the task scheduling problem. In the optimal model approach a global solution for all application’s tasks is considered, finding an optimal solution. Differently, the staged model approach is designed to obtain a local optimal solution by stages. In both cases, a mixed integer linear programming model has been designed intended to minimizing the application execution time. In the studies carried out to evaluate this proposal, the staged model provides the optimal solution in 76% of the simulated scenarios, while it also dramatically reduces the solving time with respect to optimal. Both models have pros and cons and, in fact, can be used together to complement each other. The optimal model finds the global optimal solution at high running time cost, which makes this model unpractical on some scenarios. The staged model instead, is faster enough to be used on those scenarios; however, the given solution might not be optimal in some cases.}
}
@article{PELLE2023122,
title = {P4-assisted seamless migration of serverless applications towards the edge continuum},
journal = {Future Generation Computer Systems},
volume = {146},
pages = {122-138},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001450},
author = {István Pelle and Francesco Paolucci and Balázs Sonkoly and Filippo Cugini},
keywords = {Serverless, FaaS, Edge, AWS Lambda, AWS Greengrass, P4},
abstract = {Serverless computing has recently been presented as an effective technology for handling short-lived compute tasks in the cloud. It has the potential of becoming an attractive option also in the context of edge computing where resource-aware deployment, constrained by both limited edge computing resources and experienced latency, plays a vital role. In this paper, we present and experimentally validate a framework that oversees serverless applications in an edge computing scenario. It completely automates serverless application deployment and provides hitless dynamic migration of application compute tasks between a pair of edge nodes, paving the way for handling significantly more complex cases. The framework relies on an integrated deployment, monitoring and offloading infrastructure that enhances AWS IoT Greengrass features and performance. Our implementation provides two separate options for relocating compute tasks by steering application traffic towards the most suitable node. One builds on an on-the-fly application component reconfiguration, while the other selects the suitable node through P4 in-network processing of resource metrics emitted by the nodes. Our experimental demonstration evaluates the migration performance using a latency-sensitive application decomposed to serverless functions. Results reveal extremely fast dynamic reconfiguration and traffic rerouting operations. The used methods avoid congestion peaks at the edge and show no end-to-end latency increase upon migration between the nodes.}
}
@article{WANG2022323,
title = {An edge–cloud integrated framework for flexible and dynamic stream analytics},
journal = {Future Generation Computer Systems},
volume = {137},
pages = {323-335},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002576},
author = {Xin Wang and Azim Khan and Jianwu Wang and Aryya Gangopadhyay and Carl Busart and Jade Freeman},
keywords = {Edge computing, Internet of Things (IoT), Cloud computing, Edge–cloud integration, Stream data analytics, Concept drift, Hybrid learning, Long short-term memory (LSTM)},
abstract = {With the popularity of Internet of Things (IoT), edge computing and cloud computing, more and more stream analytics applications are being developed including real-time trend prediction and object detection on top of IoT sensing data. One popular type of stream analytics is the recurrent neural network (RNN) deep learning model based time series or sequence data prediction and forecasting. Different from traditional analytics that assumes data are available ahead of time and will not change, stream analytics deals with data that are being generated continuously and data trend/distribution could change (a.k.a. concept drift), which will cause prediction/forecasting accuracy to drop over time. One other challenge is to find the best resource provisioning for stream analytics to achieve good overall latency. In this paper, we study how to best leverage edge and cloud resources to achieve better accuracy and latency for stream analytics using a type of RNN model called long short-term memory (LSTM). We propose a novel edge–cloud integrated framework for hybrid stream analytics that supports low latency inference on the edge and high capacity training on the cloud. To achieve flexible deployment, we study different approaches of deploying our hybrid learning framework including edge-centric, cloud-centric and edge–cloud integrated. Further, our hybrid learning framework can dynamically combine inference results from an LSTM model pre-trained based on historical data and another LSTM model re-trained periodically based on the most recent data. Using real-world and simulated stream datasets, our experiments show the proposed edge–cloud deployment is the best among all three deployment types in terms of latency. For accuracy, the experiments show our dynamic learning approach performs the best among all learning approaches for all three concept drift scenarios.}
}
@article{SENANAYAKE2022104862,
title = {An experiment-based cohesive-frictional constitutive model for cemented materials},
journal = {Computers and Geotechnics},
volume = {149},
pages = {104862},
year = {2022},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2022.104862},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X22002099},
author = {S.M.C.U. Senanayake and A. Haque and H.H. Bui},
keywords = {Constitutive model, Cemented soils, Discrete element method, X-Ray CT, Damage, Plasticity},
abstract = {Experiment-based damage laws at the contact bond level are often absent in the computational modelling of cemented materials using cohesive contact-bond models. In this paper, in-situ X-Ray Computed Tomography on element-scale cement bonds subjected to pure compression, shear and tensile loading was conducted to investigate their failure mechanisms and associated damage laws. A new cohesive-frictional constitutive model was subsequently developed using results obtained from X-Ray CT tests. The new constitutive model was capable of describing the behaviour of contact bonds subjected to complex loading conditions, including mixed-mode and compression damage. The new model was then implemented in an open-access Discrete Element Code (YADE) to simulate several challenging laboratory-scale tests (e.g., triaxial, Brazilian and Splitting tests) and very good agreements with the experimental data were achieved.}
}
@article{DOGANI2023120,
title = {Auto-scaling techniques in container-based cloud and edge/fog computing: Taxonomy and survey},
journal = {Computer Communications},
volume = {209},
pages = {120-150},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423002086},
author = {Javad Dogani and Reza Namvar and Farshad Khunjush},
keywords = {Cloud computing, Fog computing, Edge computing, Auto-scaling, Container-based virtualization},
abstract = {The long-held dream of computing as a service was realized with the emergence of cloud computing. Recently, fog and edge computing have been introduced as extensions of cloud networks, providing networking, processing, data management, and storage on network nodes near Internet of Things (IoT) devices to bridge the gap between the cloud and IoT devices. As the foundation of distributed computing, virtualization enables more effective use of physical computer hardware. Operating system virtualization through containers has recently been proposed as a promising alternative to virtual machines (VMs). Containers are lightweight packages that contain all application dependencies, system libraries, and third-party software packages. This research aims to review auto-scaling solutions for container-based virtualization in cloud and edge/fog computing applications. Auto-scaling plays a crucial role in the broad adoption of cloud computing by allocating and releasing computing resources in response to fluctuating resource requirements. However, designing and implementing an efficient auto-scaler for container-based applications in cloud and edge/fog computing presents challenges due to diverse application resource requirements and dynamic workload characteristics. Our research presents a comprehensive classification system for articles, covering key parameters such as auto-scaling techniques, experiments, workloads, and metrics, among others. We provide a detailed analysis of the results, offering valuable insights into open challenges and identifying promising directions for future research in this field.}
}
@article{ARCHETTI2023343,
title = {Scaling survival analysis in healthcare with federated survival forests: A comparative study on heart failure and breast cancer genomics},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {343-358},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.07.036},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23002935},
author = {Alberto Archetti and Francesca Ieva and Matteo Matteucci},
keywords = {Survival analysis, Federated learning, Random survival forest, Heart failure, Breast cancer},
abstract = {Survival analysis is a fundamental tool in medicine, modeling the time until an event of interest occurs in a population. However, in real-world applications, survival data are often incomplete, censored, distributed, and confidential, especially in healthcare settings where privacy is critical. The scarcity of data can severely limit the scalability of survival models to distributed applications that rely on large data pools. Federated learning is a promising technique that enables machine learning models to be trained on multiple datasets without compromising user privacy, making it particularly well-suited for addressing the challenges of survival data and large-scale survival applications. Despite significant developments in federated learning for classification and regression, many directions remain unexplored in the context of survival analysis. In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method constructs random survival forests in heterogeneous federations. Specifically, we investigate several new tree sampling methods from client forests and compare the results with state-of-the-art survival models based on neural networks. The key advantage of FedSurF++ is its ability to achieve comparable performance to existing methods while requiring only a single communication round to complete. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private. We also present results on two real-world datasets – a heart failure dataset from the Lombardy HFData project and Fed-TCGA-BRCA from the Falmby suite – demonstrating the success of FedSurF++ in real-world healthcare studies. Our results underscore the potential of FedSurF++ to improve the scalability and effectiveness of survival analysis in distributed settings while preserving user privacy.}
}
@article{PAPRZYCKI20213,
title = {Towards Edge-Fog-Cloud Continuum},
journal = {Procedia Computer Science},
volume = {179},
pages = {3},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921001228},
author = {Marcin Paprzycki},
keywords = {Cloud Computing, Cloud Continuum},
abstract = {Over time, two counteracting trends have been observed in the “world of computing”. One of them was a push from centralized towards decentralized solutions. The first wave of this process can be associated with the introduction of the Internet and personal computers and took place in early 1980th. The second was the move in the opposite direction. Here, the first wave of centralization can be associated with the ascent of cloud computing. These two seem to be similar to the thesis and antithesis, in Hegel’s philosophy. Interestingly, similarly to Hegel’s synthesis, we are approaching a unified model of edge-fog-cloud continuum. My talk will reflect on the journey and outline the proposed way forward.}
}
@article{KHOEI2021113660,
title = {Computational homogenization of fully coupled multiphase flow in deformable porous media},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {376},
pages = {113660},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2020.113660},
url = {https://www.sciencedirect.com/science/article/pii/S0045782520308458},
author = {A.R. Khoei and S. Saeedmonir},
keywords = {Computational homogenization, Multiphase flow, Fully coupled multi-physics, Partially saturated, Heterogeneous porous media},
abstract = {In this paper, a computational modeling tool is developed for fully coupled multiphase flow in deformable heterogeneous porous medium that consists of complex and non-uniform micro-structures using the dual continuum scales based on the computational homogenization approach. The first-order homogenization technique is employed to perform the multi-scale analysis. The governing equations of two-phase flow of immiscible fluids, including an equilibrium equation and two mass continuity equations, are considered based on the appropriate main variables. According to the well-known Hill–Mandel principle of macro-homogeneity, the proper energy types are defined instead of conventional stress power for linking micro- and macro-scales, which plays a significant role in determination of consistent microscopic fields. The finite element squared strategy is utilized to resolve the two scales simultaneously. The periodic and linear boundary conditions are exploited in the micro-scale analysis, and the macroscopic quantities such as stress tensor, inertial force vector, flux vectors and fluid contents are determined from the boundary information of microscopic domain. Moreover, a general approach is defined depending on the type of boundary condition in which the macroscopic tangent operators can be extracted directly from the converged microscopic Jacobian matrix. Finally, in order to illustrate the efficiency and accuracy of the proposed computational algorithm, several numerical examples are solved, and the effects of various parameters, such as boundary conditions, RVE types, RVE length scale, and volume fraction of heterogeneities are investigated.}
}
@article{ZANELLA2022100663,
title = {BarMan: A run-time management framework in the resource continuum},
journal = {Sustainable Computing: Informatics and Systems},
volume = {35},
pages = {100663},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100663},
url = {https://www.sciencedirect.com/science/article/pii/S2210537922000075},
author = {Michele Zanella and Filippo Sciamanna and William Fornaciari},
keywords = {Fog computing, Resource management, Programming model, Task allocation strategy},
abstract = {Over the last years, the number of IoT devices has grown exponentially, highlighting the current Cloud infrastructure limitations. In this regard, Fog and Edge computing began to move part of the computation closer to data sources by exploiting interconnected devices as part of a single heterogeneous and distributed system in a computing continuum viewpoint. Since these devices are typically heterogeneous in terms of performance, features, and capabilities, this perspective should encompass programming models and run-time management layers. This work presents and evaluates the BarMan open-source framework by implementing a Fog video surveillance use-case. BarMan leverages a task-based programming model combined with a run-time resource manager and the novel BeeR framework to deploy the application’s tasks transparently. This enables the possibility of considering aspects related to the energy and power dissipated by the devices and the single application. Moreover, we developed a task allocation policy to maximize application performance, considering run-time aspects, such as load and connectivity, of the time-varying available devices. Through an experimental evaluation performed on a real cluster equipped with heterogeneous embedded boards, we evaluated different execution scenarios to show the framework’s functionality and the benefit of a distributed approach, leading up to an improvement of 66% on the frame processing latency w.r.t. a monolithic solution.}
}
@article{PALLEWATTA2022121,
title = {QoS-aware placement of microservices-based IoT applications in Fog computing environments},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {121-136},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000206},
author = {Samodha Pallewatta and Vassilis Kostakos and Rajkumar Buyya},
keywords = {Fog computing, Microservice applications, Internet of Things, Application placement, QoS-awareness},
abstract = {The Fog computing paradigm, offering cloud-like services at the edge of the network, has become a feasible model to support computing and storage capabilities required by latency-sensitive and bandwidth-hungry Internet of Things (IoT) applications. As fog devices are distributed, heterogeneous and resource-constrained, efficient application scheduling mechanisms are required to harvest the full potential of such computing environments. Due to the rapid evolution in IoT ecosystems and also to better suit fog environment characteristics, IoT application development has moved from the monolithic architecture towards the microservices architecture that enhances scalability, maintainability and extensibility of the applications. This architecture improves the granularity of service decomposition, thus providing scope for improvement in QoS-aware placement policies. Existing application placement policies lack proper utilisation of these features of microservices architecture, thus failing to produce efficient placements. In this paper, we harvest the characteristics of microservice architecture to propose a scalable QoS-aware application scheduling policy for batch placement of microservices-based IoT applications within fog environments. Our proposed policy, QoS-aware Multi-objective Set-based Particle Swarm Optimisation (QMPSO), aims at maximising the satisfaction of multiple QoS parameters (makespan, budget and throughput) while focusing on the utilisation of limited fog resources. Besides, QMPSO adapts and improves the Set-based Comprehensive Learning Particle Swarm Optimisation (S-CLPSO) algorithm to achieve better convergence in the fog application placement problem. We evaluate our policy in a simulated fog environment. The results show that compared to the state-of-the-art solutions, our placement algorithm significantly improves QoS in terms of makespan satisfaction (up to 35% improvement) and budget satisfaction (up to 70% improvement) and ensures optimum usage of computing and network resources, thus providing a robust approach for QoS-aware placement of microservices-based heterogeneous applications within fog environments.}
}
@article{MARTIN202215,
title = {Kafka-ML: Connecting the data stream with ML/AI frameworks},
journal = {Future Generation Computer Systems},
volume = {126},
pages = {15-33},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.07.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002995},
author = {Cristian Martín and Peter Langendoerfer and Pouya Soltani Zarrin and Manuel Díaz and Bartolomé Rubio},
keywords = {Kafka-ML, Apache Kafka, Machine Learning, Artificial Intelligence, Data streams, Docker, Kubernetes, Distributed systems},
abstract = {Machine Learning (ML) and Artificial Intelligence (AI) depend on data sources to train, improve, and make predictions through their algorithms. With the digital revolution and current paradigms like the Internet of Things, this information is turning from static data to continuous data streams. However, most of the ML/AI frameworks used nowadays are not fully prepared for this revolution. In this paper, we propose Kafka-ML, a novel and open-source framework that enables the management of ML/AI pipelines through data streams. Kafka-ML provides an accessible and user-friendly Web user interface where users can easily define ML models, to then train, evaluate, and deploy them for inferences. Kafka-ML itself and the components it deploys are fully managed through containerization technologies, which ensure their portability, easy distribution, and other features such as fault-tolerance and high availability. Finally, a novel approach has been introduced to manage and reuse data streams, which may eliminate the need for data storage or file systems.}
}
@article{ALWASEL2021101956,
title = {IoTSim-Osmosis: A framework for modeling and simulating IoT applications over an edge-cloud continuum},
journal = {Journal of Systems Architecture},
volume = {116},
pages = {101956},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101956},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120302083},
author = {Khaled Alwasel and Devki Nandan Jha and Fawzy Habeeb and Umit Demirbaga and Omer Rana and Thar Baker and Scharam Dustdar and Massimo Villari and Philip James and Ellis Solaiman and Rajiv Ranjan},
keywords = {Osmosis computing, Internet of Things (IoT), Edge computing, Cloud computing, Software-Defined Network (SDN), Simulation.},
abstract = {The osmotic computing paradigm sets out the principles and algorithms for simplifying the deployment of Internet of Things (IoT) applications in integrated edge-cloud environments. Various existing simulation frameworks can be used to support integration of cloud and edge computing environments. However, none of these can directly support an osmotic computing environment due to the complexity of IoT applications and heterogeneity of integrated edge-cloud environments. Osmotic computing suggests the migration of workload to/from a cloud data center to edge devices, based on performance and security trigger events. We propose ‘IoTSim-Osmosis– a simulation framework to support the testing and validation of osmotic computing applications. In particular, our detailed related work analysis demonstrates that IoTSim-Osmosis is the first simulation framework to enable unified modeling and simulation of complex IoT applications over heterogeneous edge-cloud environments. IoTSim-Osmosis is demonstrated using an electricity management and billing application case study, for benchmarking various run-time QoS parameters, such as IoT battery use, execution time, network transmission time and consumed energy.}
}
@article{LUO2023102968,
title = {BeeFlow: Behavior tree-based Serverless workflow modeling and scheduling for resource-constrained edge clusters},
journal = {Journal of Systems Architecture},
volume = {143},
pages = {102968},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2023.102968},
url = {https://www.sciencedirect.com/science/article/pii/S1383762123001479},
author = {Ke Luo and Tao Ouyang and Zhi Zhou and Xu Chen},
keywords = {Edge computing, Serverless computing, Serverless workflow, Behavior tree, Workflow modeling, Workflow scheduling},
abstract = {Serverless computing has gained popularity in edge computing due to its flexible features, including the pay-per-use pricing model, auto-scaling capabilities, and multi-tenancy support. Complex Serverless-based applications typically rely on Serverless workflows (also known as Serverless function orchestration) to express task execution logic, and numerous application- and system-level optimization techniques have been developed for Serverless workflow scheduling. However, there has been limited exploration of optimizing Serverless workflow scheduling in edge computing systems, particularly in high-density, resource-constrained environments such as system-on-chip clusters and single-board-computer clusters. In this work, we discover that existing Serverless workflow scheduling techniques typically assume models with limited expressiveness and cause significant resource contention. To address these issues, we propose modeling Serverless workflows using behavior trees, a novel and fundamentally different approach from existing directed-acyclic-graph- and state machine-based models. Behavior tree-based modeling allows for easy analysis without compromising workflow expressiveness. We further present observations derived from the inherent tree structure of behavior trees for contention-free function collections and awareness of exact and empirical concurrent function invocations. Based on these observations, we introduce BeeFlow, a behavior tree-based Serverless workflow system tailored for resource-constrained edge clusters. Experimental results demonstrate that BeeFlow achieves up to 3.2× speedup in a high-density, resource-constrained edge testbed and 2.5× speedup in a high-profile cloud testbed, compared with the state-of-the-art. BeeFlow also demonstrates superior robustness in scenarios with heavy system workloads.}
}
@article{YUAN2023179,
title = {ELECT: Energy-efficient intelligent edge–cloud collaboration for remote IoT services},
journal = {Future Generation Computer Systems},
volume = {147},
pages = {179-194},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001681},
author = {Jingling Yuan and Hua Xiao and Zhishu Shen and Tiehua Zhang and Jiong Jin},
keywords = {Edge–cloud collaboration, Dynamic management of IoT nodes, Workflow scheduling, Services in remote areas, Structural health monitoring, Deep Q-network},
abstract = {Existing centralized cloud-based solution is challenging to cope with the explosive growth of data generated from massive IoT devices to meet the requirements of critical services, especially for remote IoT services provided in remote/rural areas where the resource and energy capacity of local data processing infrastructure is limited. To solve this issue, we propose ELECT, an energy-efficient intelligent edge-cloud collaboration scheme that achieves satisfactory data processing performance. ELECT includes a platform that utilizes edge computing node as a core element that locates close to the IoT nodes for coordinating the data processing between the cloud and the IoT devices. Specifically, based on the importance of a node’s collected data in the overall data processing performance, a dynamic IoT node management algorithm is developed to manage each node’s active/inactive status to reduce energy consumption. Moreover, a deep Q-network (DQN)-based workflow scheduling algorithm that fully utilizes the data-centric device–edge–cloud continuum is introduced to reduce makespan and energy consumption for obtaining a compromising solution. For verification, we develop an experimental environment simulating structural health monitoring (SHM) services in remote areas. Extensive experiments verify the effectiveness of ELECT in terms of various service requirements, including makespan, energy consumption and communication cost.}
}
@article{DEBAUCHE20227494,
title = {Cloud and distributed architectures for data management in agriculture 4.0 : Review and future trends},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {9},
pages = {7494-7514},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821002664},
author = {Olivier Debauche and Saïd Mahmoudi and Pierre Manneback and Frédéric Lebeau},
keywords = {Agriculture 4.0, Smart farming, Smart agriculture, Lambda architecture, Kappa architecture, Edge computing, Fog computing, Micro-service architecture, Data lake, Data house, Blockchain, Osmotic computing, Dew computing},
abstract = {The Agriculture 4.0, also called Smart Agriculture or Smart Farming, is at the origin of the production of a huge amount of data that must be collected, stored, and processed in a very short time. Processing this massive quantity of data needs to use specific infrastructure that use adapted IoT architectures. Our review offers a comparative panorama of Central Cloud, Distributed Cloud Architectures, Collaborative Computing Strategies, and new trends used in the context of Agriculture 4.0. In this review, we try to answer 4 research questions: (1) Which storage and processing architectures are best suited to Agriculture 4.0 applications and respond to its peculiarities? (2) Can generic architectures meet the needs of Agriculture 4.0 application cases? (3) What are the horizontal development possibilities that allow the transition from research to industrialization? (4) What are the vertical valuations possibilities to move from algorithms trained in the cloud to embedded or stand-alone products? For this, we compare architectures with 8 criteria (User Proximity, Latency & Jitter, Network stability, high throughput, Reliability, Scalability, Cost Effectiveness, Maintainability), and analyze the advantages and disadvantages of each of them.}
}
@article{MENON2022115511,
title = {Updated Lagrangian unsaturated periporomechanics for extreme large deformation in unsaturated porous media},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {400},
pages = {115511},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.115511},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522005199},
author = {Shashank Menon and Xiaoyu Song},
keywords = {Periporomechanics, Unsaturated porous media, Peridynamics, Extreme large deformation, Stabilization, Updated Lagrangian},
abstract = {Unsaturated periporomechanics is a strong nonlocal poromechanics based on peridynamic state and effective force state concept. In the previous periporomechnics the total Lagrangian formulation is adopted for the solid skeleton of porous media. In this article as a new contribution we formulate and implement an updated Lagrangian unsaturated periporomechanics framework for modeling extreme large deformation in unsaturated soils under drained conditions. In this new framework the so-called bond-associated sub-horizon concept is utilized to enhance the stability and accuracy at extreme large deformation of the solid skeleton. The stabilized nonlocal velocity gradient in the deformed configuration is used to update the effective force state from a critical state based visco-plastic constitutive model for unsaturated soils. The updated Lagrangian periporomechanics paradigm is numerically implemented through an explicit Newmark scheme for high-performance computing. Numerical examples are presented to demonstrate the stability of the computational updated Lagrangian periporomechanics paradigm and its efficacy and robustness in modeling extreme large deformation in porous media under drained conditions.}
}
@article{FORTAS2022102859,
title = {Formal verification of IoT applications using rewriting logic: An MDE-based approach},
journal = {Science of Computer Programming},
volume = {222},
pages = {102859},
year = {2022},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2022.102859},
url = {https://www.sciencedirect.com/science/article/pii/S0167642322000922},
author = {Abdelouahab Fortas and Elhillali Kerkouche and Allaoua Chaoui},
keywords = {Internet of things, Formal verification, Rewriting logic, Model checking, Model driven engineering},
abstract = {Internet of Things (IoT) systems are complex assemblies of components that collaborate to achieve common goals. These components are based on heterogeneous technologies, and they communicate using various communication protocols. This heterogeneity makes the design and the development of IoT applications a challenging issue. Diverse approaches based on Model-Driven Engineering (MDE) have been proposed to overcome this major issue using suitable modeling languages. ThingML is a promising UML profile for modeling IoT applications that aims to address the challenges of heterogeneity. However, ThingML does not have rigorous semantics, making it unsuitable for formal verification and analysis of system designs. This paper proposes an MDE-based formal approach to define the formal semantics of the ThingML language using rewriting logic and its language Maude. The main advantage of our approach over other approaches lies in the universality and versatility of Maude's mathematical notation, which implements all ThingML concepts and their behavioral aspects in a unified formal logic. The existing Maude language verification tools provide powerful analysis techniques, including simulation and model checking, which enable rigorous analysis and verification of ThingML designs. The contributions of this work include the following: (i) we propose a semantics mapping between ThingML concepts and Maude constructs, (ii) we define and implement an operational semantics for the ThingML action language in the Maude language, and (iii) we develop a tool that enables the automatic transformation of ThingML specifications into Maude. Our approach is illustrated through a case study.}
}
@article{CAMPIONI2023181,
title = {Enabling civil–military collaboration for disaster relief operations in smart city environments},
journal = {Future Generation Computer Systems},
volume = {139},
pages = {181-195},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003041},
author = {Lorenzo Campioni and Filippo Poltronieri and Cesare Stefanelli and Niranjan Suri and Mauro Tortonesi and Konrad Wrona},
keywords = {Humanitarian assistance and disaster recovery (HADR), Internet of things (ioT), Fog computing, Context-aware services, Distributed system, Location based services (LBS)},
abstract = {To address the aftermath of ever more frequent natural disasters striking highly-populated urban areas, Humanitarian Assistance and Disaster Relief (HADR) operations increasingly involve the coordinated efforts of multiple agencies and in particular Civil–Military Cooperation (CIMIC). In such scenarios, rescuers operate in a disrupted environment, must maintain a high operational tempo, and need to quickly make high-impact decisions. Effectively supporting emergency responders through IT services that implement accurate situation awareness presents a formidable set of challenges, including the discovery and integration of surviving IT assets with purposely deployed ones; effective information prioritization; resilient communications; and secure information sharing. We believe that in the highly digital modern urban environment, often referred to as smart cities, these challenges can be effectively addressed only by integrating by design HADR support into the smart city middleware. This paper presents Aceso — a proof-of-concept smart city middleware that provides location- and context-sensitive services with full support for HADR operations. Aceso provides a set of functions, ranging from resource discovery to secure information sharing, that can be quickly activated in case of unpredictable and adverse events to facilitate HADR operations. Furthermore, Aceso leverages the Value-of-Information (VoI) methodology to handle the processing and dissemination of mission-critical information. To validate the capabilities of Aceso, we devised a fictional HADR scenario, set in the city of Helsinki, Finland, that involves the collaboration of multiple responder teams with different roles. The validation results confirm Aceso’s usefulness in prioritizing the processing and dispatching of critical information and in realizing federation-wide sharing of this information among HADR teams.}
}
@article{LIU2023116308,
title = {A continuum and computational framework for viscoelastodynamics: II. Strain-driven and energy–momentum consistent schemes},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {417},
pages = {116308},
year = {2023},
note = {A Special Issue in Honor of the Lifetime Achievements of T. J. R. Hughes},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116308},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523004322},
author = {Ju Liu and Jiashen Guan},
keywords = {Continuum mechanics, Energy–momentum method, Viscoelasticity, Integration algorithm for constitutive equations, Isogeometric analysis, Nonlinear stability},
abstract = {We continue our investigation of finite deformation linear viscoelastodynamics by focusing on constructing accurate and reliable numerical schemes. The concrete thermomechanical foundation developed in the previous study paves the way for pursuing discrete formulations with critical physical and mathematical structures preserved. Energy stability, momentum conservation, and temporal accuracy constitute the primary factors in our algorithm design. For inelastic materials, the directionality condition, a property for the stress to be energy consistent, is extended with the dissipation effect taken into account. Moreover, the integration of the constitutive relations calls for an algorithm design of the internal state variables and their conjugate variables. A directionality condition for the conjugate variables is introduced as an indispensable ingredient for ensuring physically correct numerical dissipation. By leveraging the particular structure of the configurational free energy, a set of update formulas for the internal state variables is obtained. Detailed analysis reveals that the overall discrete schemes are energy–momentum consistent and achieve first- and second-order accuracy in time, respectively. Numerical examples are provided to justify the appealing features of the proposed methodology.}
}
@article{CAIAZZA2021108140,
title = {Measurement-driven design and runtime optimization in edge computing: Methodology and tools},
journal = {Computer Networks},
volume = {194},
pages = {108140},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108140},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621002085},
author = {Chiara Caiazza and Claudio Cicconetti and Valerio Luconi and Alessio Vecchio},
keywords = {Edge computing, ETSI MEC, Network measurements, GSMA platform operator},
abstract = {Edge computing is projected to become the dominant form of cloud computing in the future because of the significant advantages it brings to both users (less latency, higher throughput) and telecom operators (less Internet traffic, more local management). However, to fully unlock its potential at scale, system designers and automated optimization systems alike will have to monitor closely the dynamics of both processing and communication facilities. Especially the latter is often neglected in current systems since network performance in cloud computing plays only a minor role. In this paper, we propose the architecture of MECPerf, which is a solution to collect network measurements in a live edge computing domain, to be collected for offline provisioning analysis and simulations, or to be provided in real-time for on-line system optimization. MECPerf has been validated in a realistic testbed funded by the European Commission (Fed4Fire+), and we describe here a summary of the results, which are fully available as open data and through a Python library to expedite their utilization. This is demonstrated via a use case involving the optimization of a system parameter for migrating clients in a federated edge computing system adopting the GSMA platform operator concept.}
}
@article{GALLEGOMADRID2023556,
title = {The role of vehicular applications in the design of future 6G infrastructures},
journal = {ICT Express},
volume = {9},
number = {4},
pages = {556-570},
year = {2023},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2023.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S2405959523000383},
author = {Jorge Gallego-Madrid and Ramon Sanchez-Iborra and Jordi Ortiz and Jose Santa},
keywords = {6G, Vehicular communications, V2X, Applications},
abstract = {A great lack of 5G design is the traditional bottom-up development of network evolution, which has not effectively considered the requirements of applications and, particularly, vehicle to everything (V2X) applications. This paper provides a service-centric approach towards 6G V2X, with a concise overview of the upcoming hyper-connected vehicular ecosystem and its integration in the whole 6G fabric, analysing its particular infrastructure needs, as a way to reach key performance indicators (KPIs). We also present a 6G-oriented platform design able to manage the life-cycle of V2X applications across different domains by means of intelligent orchestration decisions.}
}
@article{MWASE2022292,
title = {Communication-efficient distributed AI strategies for the IoT edge},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {292-308},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000218},
author = {Christine Mwase and Yi Jin and Tomi Westerlund and Hannu Tenhunen and Zhuo Zou},
keywords = {Distributed AI/ML, Communication efficient AI/ML, Fog/edge computing, Edge ML, IIoT},
abstract = {The impact that artificial intelligence (AI) has made across several industries in today’s society is clearly seen in applications ranging from medical diagnosis to customer service chatbots, to financial trading. It is also evident that AI has a huge role to play in emerging and future applications and will be increasingly used in mission-critical and time-sensitive applications such as remote surgeries, cybersecurity and self-driving cars. To satisfy the latency, security and privacy requirements that such applications require, AI which has gained its merit by utilising resource-heavy cloud infrastructure, needs to perform well in resource-constrained environments at the network edge. To address this need, this paper characterises the cloud-to-thing continuum and provides an architecture for enabling AI in fully edge-based scenarios. In addition, the paper provides strategies to tackle the communication inefficiencies that arise from the distributed nature of fully edge-based scenarios. Performance improvements exhibited by these strategies in state-of-the art research is presented, as well as directions where further advancements can be made. The material is presented in a simple manner to catalyse the understanding and hence the participation of multidisciplinary researchers in addressing this challenge.}
}
@article{VOLPERT2023243,
title = {The view on systems monitoring and its requirements from future Cloud-to-Thing applications and infrastructures},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {243-257},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003843},
author = {Simon Volpert and Philipp Eichhammer and Florian Held and Thomas Huffert and Hans P. Reiser and Jörg Domaschka},
keywords = {Internet of Things, Monitoring, Edge/fog computing, Observability, Instrumentation, Cloud-to-Thing continuum},
abstract = {Monitoring infrastructures is a key cornerstone for both security and resilience mechanisms, as well as, infrastructure management helping to ensure availability and reliability. Its importance is increasing in the light of Internet-of-Things (IoT) infrastructures growing both in size and complexity, hence, widening the attack surface and decreasing the possibilities of manual management. There is a multitude of different monitoring concepts providing solutions to those problems. However, in the context of IoT those traditional concepts are limited in their adoption capabilities due to fundamental differences in architecture and structure. In this paper, we create a systematization of knowledge about the impact of IoT infrastructure characteristics on requirements for and architecture of monitoring. In particular, we first discuss definitions of monitoring resulting in an own definition, before elaborating on the terminology encompassing monitoring. We further discuss requirements imposed by IoT systems combined with an analysis of monitoring properties relevant for IoT. As part of this paper, we also focus on highlighting current and future directions of IoT architecture and infrastructure.}
}
@article{WU2021245,
title = {The inclusion-based boundary element method (iBEM) for virtual experiments of elastic composites},
journal = {Engineering Analysis with Boundary Elements},
volume = {124},
pages = {245-258},
year = {2021},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2020.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0955799720303349},
author = {Chunlin Wu and Huiming Yin},
keywords = {Boundary element method, Equivalent inclusion method, Harmonic potential integral, Eshelby’s tensor, Homogenization},
abstract = {This paper introduces the inclusion based boundary element method (iBEM) to calculate the elastic fields and effective modulus of a composite containing particles for both three dimensional (3D) and two dimensional (2D) cases. Considering a finite bounded domain containing many inclusions, the isotropic Green’s function has been used to obtain the elastic field caused by source fields on inclusion domains and applied loads on the boundary. Based on Eshelby’s equivalent inclusion method (EIM), the material mismatch between the particle and matrix phases is simulated with a continuously distributed source field, namely eigenstrain, on particles. Because explicit integrals can be obtained for ellipsoidal particles, no mesh is needed for those particles, which enables virtual experiments of a composite containing a large number of particles. The classic Eshelby’s tensor is extended from a constant eigenstrain for the single particle in the infinite domain to a form of a Taylor series for particle-boundary interaction and particle-particle interactions. Using the Hadamard regularization, the 2D formulation is derived from the 3D case by the integral of the elastic solution in the third direction together with an analytical circular harmonic potential integral scheme. The iBEM is particularly suitable to conduct virtual experiments for studying the local elastic field with the integrals of all sources and calculating the effective material properties by the volume average of local fields. A parametric study of accuracy on stress field for uniform, linear, quadratic eigenstrain fields was performed and case studies have been presented to demonstrate the capability of the iBEM for virtual experiments of composites. Some interesting discoveries of microstructure-dependent material behavior are reported with the aid of virtual experiments.}
}
@article{AGOSTA2022104679,
title = {Towards EXtreme scale technologies and accelerators for euROhpc hw/Sw supercomputing applications for exascale: The TEXTAROSSA approach},
journal = {Microprocessors and Microsystems},
volume = {95},
pages = {104679},
year = {2022},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2022.104679},
url = {https://www.sciencedirect.com/science/article/pii/S0141933122002095},
author = {Giovanni Agosta and Marco Aldinucci and Carlos Alvarez and Roberto Ammendola and Yasir Arfat and Olivier Beaumont and Massimo Bernaschi and Andrea Biagioni and Tommaso Boccali and Berenger Bramas and Carlo Brandolese and Barbara Cantalupo and Mauro Carrozzo and Daniele Cattaneo and Alessandro Celestini and Massimo Celino and Iacopo Colonnelli and Paolo Cretaro and Pasqua D’Ambra and Marco Danelutto and Roberto Esposito and Lionel Eyraud-Dubois and Antonio Filgueras and William Fornaciari and Ottorino Frezza and Andrea Galimberti and Francesco Giacomini and Brice Goglin and Daniele Gregori and Abdou Guermouche and Francesco Iannone and Michal Kulczewski and Francesca {Lo Cicero} and Alessandro Lonardo and Alberto R. Martinelli and Michele Martinelli and Xavier Martorell and Giuseppe Massari and Simone Montangero and Gianluca Mittone and Raymond Namyst and Ariel Oleksiak and Paolo Palazzari and Pier Stanislao Paolucci and Federico Reghenzani and Cristian Rossi and Sergio Saponara and Francesco Simula and Federico Terraneo and Samuel Thibault and Massimo Torquati and Matteo Turisini and Piero Vicini and Miquel Vidal and Davide Zoni and Giuseppe Zummo},
keywords = {High-performance computing},
abstract = {In the near future, Exascale systems will need to bridge three technology gaps to achieve high performance while remaining under tight power constraints: energy efficiency and thermal control; extreme computation efficiency via HW acceleration and new arithmetic; methods and tools for seamless integration of reconfigurable accelerators in heterogeneous HPC multi-node platforms. TEXTAROSSA addresses these gaps through a co-design approach to heterogeneous HPC solutions, supported by the integration and extension of HW and SW IPs, programming models, and tools derived from European research.}
}
@article{HEIDEN2021387,
title = {Framing Artificial Intelligence (AI) Additive Manufacturing (AM)},
journal = {Procedia Computer Science},
volume = {186},
pages = {387-394},
year = {2021},
note = {14th International Symposium "Intelligent Systems},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.161},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921009923},
author = {Bernhard Heiden and Volodymyr Alieksieiev and Matthias Volk and Bianca Tonino-Heiden},
keywords = {Artificial Intelligence, AI, Additive Manufacturing, AM, Production, Production Technology, Production Control, Production Engineering, System Theory, Graph Theory},
abstract = {Nowadays AM is a rapidly growing and emerging discipline in manufacturing, as well as AI is in informational applications. Both are related to logistical and self-referential/-copying concepts which make them scalable. What is in AM osmotic mass production spreading is in AI-related Cyber-Physical Systems (CPS) the osmotic computational approach. AI-AM self-propagatedly framed is itself an emerging field, which can be logically or systematically unified. The paper investigates firstly recent developments in the field of the AM process flow and how it is related to AI applications. The result is a list of logistical, organisational, and industrial process steps as well as modern and future AI-AM applications. The extended approach then gives prospect to a meta-perspectively embedded osmotic decentralized computing, as well as an osmotic manufacturing paradigm, which utilizes glocal functions, concerning local production as well as global distributed material and information transport nets and their connection graphs.}
}
@article{WANG2022103354,
title = {Context-aware distribution of fog applications using deep reinforcement learning},
journal = {Journal of Network and Computer Applications},
volume = {203},
pages = {103354},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103354},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000236},
author = {Nan Wang and Blesson Varghese},
keywords = {Fog computing, Decentralised cloud, Edge computing, Context-aware distribution},
abstract = {Fog computing is an emerging paradigm that aims to meet the increasing computation demands arising from the billions of devices connected to the Internet. Offloading services of an application from the Cloud to the edge of the network can improve the overall latency of the application since it can process data closer to user devices. Diverse Fog nodes ranging from Wi-Fi routers to mini-clouds with varying resource capabilities makes it challenging to determine which services of an application need to be offloaded. In this paper, a context-aware mechanism for distributing applications across the Cloud and the Fog is proposed. The mechanism dynamically generates (re)deployment plans for the application to maximise the performance efficiency of the application by taking operational conditions, such as hardware utilisation and network state, and running costs into account. The mechanism relies on deep Q-networks to generate a distribution plan without prior knowledge of the available resources on the Fog node, the network condition, and the application. The feasibility of the proposed context-aware distribution mechanism is demonstrated on two use-cases, namely a face detection application and a location-based mobile game. The benefits are increased utility of dynamic distribution by 50% and 20% for the two use-cases respectively when compared to a static distribution approach used in existing research.}
}
@article{ZAPPATORE20231,
title = {Semantic models for IoT sensing to infer environment–wellness relationships},
journal = {Future Generation Computer Systems},
volume = {140},
pages = {1-17},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003211},
author = {Marco Zappatore and Antonella Longo and Angelo Martella and Beniamino {Di Martino} and Antonio Esposito and Serena Angela Gracco},
keywords = {IoT interoperability, Semantic API, Environmental sensing, Mobile Crowd Sensing, Ontology patterns},
abstract = {Every time an Internet of Things (IoT) solution is deployed, every time a smartphone owner connects her/his wireless device to a wearable activity-tracker, every time groups of citizens use geo-mapping applications to move around the city, choosing the least crowded path, data are produced and information have to be exchanged appropriately via APIs. Even if novel added-value IoT-based applications appear on the market with increasing speed, true semantic interoperability is far from being achieved, thus limiting the large-scale exploitation, the scalability and the time-to-market of novel apps. Currently, connecting different data prosumers with multiple data sources is still hampered by the lack of standardized and sustainable solutions, especially due to the significant heterogeneity of IoT platforms. In such a landscape, ontologies come to the rescue, thanks to their formal semantics, knowledge representation formats, and shared vocabularies. In this paper we examine, from an ontological perspective, how to describe environmental sensing and wellness monitoring, two of the most popular application cases of Mobile Crowd Sensing (MCS) and IoT, respectively. To this purpose, an ontology of sensor-agnostic APIs is proposed, along with a set of MCS-dedicated ontology modules (and the supporting platform), leveraging on standard and reusable domain ontologies. Moreover, it will be shown how to properly combine the proposed ontologies in order to support complex functionalities based on inference rules addressing the environment–wellness relationships. Finally, specific semantic modeling patterns suitable for typical IoT and MCS scenarios will be discussed.}
}
@article{2021iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {179},
pages = {iii-ix},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(21)00447-6},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921004476}
}
@article{LIU2021114059,
title = {A continuum and computational framework for viscoelastodynamics: I. Finite deformation linear models},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {385},
pages = {114059},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.114059},
url = {https://www.sciencedirect.com/science/article/pii/S004578252100390X},
author = {Ju Liu and Marcos Latorre and Alison L. Marsden},
keywords = {Continuum mechanics, Gibbs free energy, Viscoelasticity, Incompressible solids, Isogeometric analysis, Nonlinear stability},
abstract = {This work concerns the continuum basis and numerical formulation for deformable materials with viscous dissipative mechanisms. We derive a viscohyperelastic modeling framework based on fundamental thermomechanical principles. Since most large deformation problems exhibit isochoric properties, our modeling work is constructed based on the Gibbs free energy in order to develop a continuum theory using pressure-primitive variables, which is known to be well-behaved in the incompressible limit. A set of general evolution equations for the internal state variables is derived. With that, we focus on a family of free energies that leads to the so-called finite deformation linear model. Our derivation elucidates the origin of the evolution equations of that model, which was originally proposed heuristically and thus lacked formal compatibility with the underlying thermodynamics. In our derivation, the thermodynamic inconsistency is clarified and rectified. A classical model based on the identical polymer chain assumption is revisited and is found to have non-vanishing viscous stresses in the equilibrium limit, which is counter-intuitive in the physical sense. Because of that, we then discuss the relaxation property of the non-equilibrium stress in the thermodynamic equilibrium limit and its implication on the form of free energy. A modified version of the identical polymer chain model is then proposed, with a special case being the model proposed by G. Holzapfel and J. Simo. Based on the consistent modeling framework, a provably energy stable numerical scheme is constructed for incompressible viscohyperelasticity using inf–sup stable elements. In particular, we adopt a suite of smooth generalization of the Taylor–Hood element based on Non-Uniform Rational B-Splines (NURBS) for spatial discretization. The temporal discretization is performed via the generalized-α scheme. We present a suite of numerical results to corroborate the proposed numerical properties, including the nonlinear stability, robustness under large deformation, and the stress accuracy resolved by the higher-order elements. Additionally, the pathological behavior of the original identical polymer chain model is numerically identified with an unbounded energy decaying. This again underlines the importance of demanding vanishing non-equilibrium stress in the equilibrium limit.}
}
@article{VILA2022101699,
title = {Edge-to-cloud sensing and actuation semantics in the industrial Internet of Things},
journal = {Pervasive and Mobile Computing},
volume = {87},
pages = {101699},
year = {2022},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2022.101699},
url = {https://www.sciencedirect.com/science/article/pii/S1574119222001122},
author = {Marc Vila and Víctor Casamayor and Schahram Dustdar and Ernest Teniente},
keywords = {Industrial Internet of Things, Interoperability, Computing Continuum, Context-awareness, Semantics, Autonomous cars},
abstract = {There are billions of devices worldwide deployed, connected, and communicating to other systems. Sensors and actuators, which can be stationary or movable devices. These Edge devices are considered part of the Internet of Things (IoT) devices, which can be referred to as a tier of the Computing Continuum paradigm. There are two main concerns at stake in the success of this ecosystem. The interoperability between devices and systems is the first. Mainly, because most of them communicate uniquely and differently from each other, leading to heterogeneous data. The second issue is the lack of decision-making capacity to conduct actuations, such as communicating through different computing tiers based on latency constraints due to a certain measured factor. In this article, we propose an ontology to improve device interoperability in the IoT. In addition, we also explain how to ease data communication between Computing Continuum devices, providing tools to enhance data management and decision-making. A use case is also presented, using the automotive industry, where quickness in maneuver determination is key to avoid accidents. It is exemplified using two Raspberry Pi devices, connected using different networks and choosing the appropriate one depending on context-aware conditions.}
}
@article{CASADEI2021104081,
title = {Engineering collective intelligence at the edge with aggregate processes},
journal = {Engineering Applications of Artificial Intelligence},
volume = {97},
pages = {104081},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.104081},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620303389},
author = {Roberto Casadei and Mirko Viroli and Giorgio Audrito and Danilo Pianini and Ferruccio Damiani},
keywords = {Computational collective intelligence, Self-organisation, Distributed computing, Aggregate programming, Computational fields, Multi-agent systems},
abstract = {Edge computing promotes the execution of complex computational processes without the cloud, i.e., on top of the heterogeneous, articulated, and possibly mobile systems composed of IoT and edge devices. Such a pervasive smart fabric augments our environment with computing and networking capabilities. This leads to a complex and dynamic ecosystem of devices that should not only exhibit individual intelligence but also collective intelligence—the ability to take group decisions or process knowledge among autonomous units of a distributed environment. Self-adaptation and self-organisation mechanisms are also typically required to ensure continuous and inherent toleration of changes of various kinds, to distribution of devices, energy available, computational load, as well as faults. To achieve this behaviour in a massively distributed setting like edge computing demands, we seek for identifying proper abstractions, and engineering tools therefore, to smoothly capture collective behaviour, adaptivity, and dynamic injection and execution of concurrent distributed activities. Accordingly, we elaborate on a notion of “aggregate process” as a concurrent collective computation whose execution and interactions are sustained by a dynamic team of devices, whose spatial region can opportunistically vary over time. We ground this notion by extending the aggregate computing model and toolchain with new constructs to instantiate aggregate processes and regulate key aspects of their lifecycle. By virtue of an open-source implementation in the ScaFi framework, we show basic programming examples as well as case studies of edge computing, evaluated by simulation in realistic settings.}
}
@article{GAGLIANESE202377,
title = {Assessing and enhancing a Cloud-IoT monitoring service over federated testbeds},
journal = {Future Generation Computer Systems},
volume = {147},
pages = {77-92},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001668},
author = {M. Gaglianese and S. Forti and F. Paganelli and A. Brogi},
keywords = {Cloud-IoT monitoring, Federated testbeds, Lightweight monitoring, Fog computing, Fault resilience, Self-organising systems},
abstract = {Monitoring resource availability along Cloud-IoT networks in a lightweight and fault-resilient manner is a challenging research problem due to scarce resource availability, infrastructure dynamics, and platform heterogeneity. In this article, we illustrate a thorough experimental assessment of a self-organising and fault-tolerant monitoring service, FogMon, especially targeting Cloud-IoT settings and capable of probing hardware resource, latency and bandwidth. The assessment is carried out over networks made up of 20 to 40 nodes across two testbeds within the Fed4Fire+ federation. As a result of the assessment, we agilely improved and refined FogMon into FogMon 2, which settles at TRL5 and improves on monitoring accuracy and fault-resiliency. Experimental results show how FogMon 2 can promptly and suitably handle different types of infrastructure failure, with an average relative error of 10% on measurements and limited footprint on hardware and network resources.}
}
@article{PULIAFITO2023101808,
title = {Balancing local vs. remote state allocation for micro-services in the cloud–edge continuum},
journal = {Pervasive and Mobile Computing},
volume = {93},
pages = {101808},
year = {2023},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2023.101808},
url = {https://www.sciencedirect.com/science/article/pii/S1574119223000664},
author = {Carlo Puliafito and Claudio Cicconetti and Marco Conti and Enzo Mingozzi and Andrea Passarella},
keywords = {FaaS, Function-as-a-service, Distributed computing, Edge computing, Micro-services, Stateful functions},
abstract = {In the world of cloud technologies, serverless computing has now settled as a stable and promising resident. This gives a cloud provider the flexibility to provide its users with both Platform-as-a-Service (PaaS), i.e., the back-end application runs in a dedicated container, or Function-as-a-Service (FaaS), i.e., the back-end logic is offered as elementary functions that are invoked by the client applications. In parallel, edge computing has attracted a significant interest, due its enticing promises of reducing the outbound traffic of telco operators, while at the same time cutting down the user latency. As a result, in the near future, PaaS and FaaS containers are going to cohabit in a versatile computation infrastructure spanning from the far edge up to the cloud. In this paper we propose a mathematical formulation of a resource allocation problem that optimizes the assignment of both types of containers and can be solved efficiently by an edge orchestrator. We evaluate the proposed solution via extensive simulation experiments, which show that our approach, which takes into account the characteristics of PaaS vs. FaaS, provides significant performance benefits compared to less sophisticated strategies, despite its relatively low run-time complexity.}
}
@incollection{KUMAR202181,
title = {Chapter 6 - IoT services in healthcare industry with fog/edge and cloud computing},
editor = {Sanjay Kumar Singh and Ravi Shankar Singh and Anil Kumar Pandey and Sandeep S. Udmale and Ankit Chaudhary},
booktitle = {IoT-Based Data Analytics for the Healthcare Industry},
publisher = {Academic Press},
pages = {81-103},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-821472-5},
doi = {https://doi.org/10.1016/B978-0-12-821472-5.00017-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821472500017X},
author = {Dinesh Kumar and Ashish Kumar Maurya and Gaurav Baranwal},
keywords = {Internet of Things, Fog computing, Edge computing, Cloud computing, Healthcare industry},
abstract = {A continuous growth in human population has made availability of healthcare services a challenge. In recent past, various online healthcare applications have been proposed, but they were not reachable to common people because of unavailability of suitable and cheap handheld devices. A few years back, when Internet of Things (IoT) was introduced with clear architecture and smartphones came into the daily routine of humans, tremendous growth is observed in the IoT technology. Though smart city, smart farming, and smart parking are various applications of IoT, healthcare domain has emerged as one of the most popular domains of IoT technology. IoT along with cloud computing, fog computing, and mobile edge computing are some promising technologies behind the buildup of an advanced, digital, and smart healthcare system. Automated patient monitoring, activity tracking, measuring heart rate, calculating calorie burn/intake, etc. are some of the tasks performed by IoT devices attached to sensors in healthcare systems. In this chapter the role of fog and cloud computing in an IoT-based healthcare system is presented along with detailed technical aspects of each of the technologies for the realization of a complete and efficient IoT-based healthcare system. Several aspects of these technologies in healthcare such as the IoT-Fog-Cloud continuum, the standard platform that facilitates the communications among these different layers and types of fog devices, have been discussed. Along with this a separate discussion is given on how IoT-based healthcare can be integrated and implemented using emerging computing technologies. At last, various research challenges and future directions are provided.}
}
@article{NURNOBY20231102,
title = {A Real-Time Deep Learning-based Smart Surveillance Using Fog Computing: A Complete Architecture},
journal = {Procedia Computer Science},
volume = {218},
pages = {1102-1111},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000893},
author = {M Fasial Nurnoby and Tarek Helmy},
keywords = {Fog-Computing, Deep Learning, Computer Vision, Smart Video Surveillance, Internet of Things},
abstract = {Fog computing offers low-latency and real-time big-data processing capabilities closer to the network edge. This particular benefit addresses the main bottleneck in a centralized cloud framework, which is, it cannot process latency-sensitive large video frames generated from the Internet of Things-based video surveillance cameras in real-time. Besides, the recent advancements in the computer vision field offer many state-of-the-art image processing capabilities that can be utilized for real-time surveillance data processing. Deploying those processing powers at several fog computing layers can bring novel solutions for computer vision-based real-time security solutions. This paper proposes a deep learning-based framework for smart video surveillance that can process the real-time frames on two consecutive fog layers, one for action recognition and the other for criminal threat-based response generation. The proposed architecture consists of three major modules. The first module is responsible for capturing surveillance videos by deploying RaspberryPi cameras in a distributed network. The second module is responsible for action recognition using a deep learning-based model installed inside NVIDIA Jetson Nano-devices placed on two fog layers. Finally, the security response is generated and broadcast to the law-enforcement agency. To evaluate the proposed model, experiments on semantic segmentation-based scene object recognition were run. The experimental results came up with a suitable recognition model that can be deployed in the fog layers of our proposed framework.}
}
@article{TUSA2023473,
title = {End-to-end slices to orchestrate resources and services in the cloud-to-edge continuum},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {473-488},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.11.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003971},
author = {Francesco Tusa and Stuart Clayman},
keywords = {Internet of things, End-to-end slice, Bare-metal cloud, Slice abstraction, Orchestration, Programmability, Multi-tenant},
abstract = {Fog computing, combined with traditional cloud computing, offers an inherently distributed infrastructure – referred to as the cloud-to-edge continuum – that can be used for the execution of low-latency and location-aware IoT services. The management of such an infrastructure is complex: resources in multiple domains need to be accessed by several tenants, while an adequate level of isolation and performance has to be guaranteed. This paper proposes the dynamic allocation of end-to-end slices to perform the orchestration of resources and services in such a scenario. These end-to-end slices require a unified resource management approach that encompasses both data centre and network resources. Currently, fog orchestration is mainly focused on the management of compute resources, likewise, the slicing domain is specifically centred solely on the creation of isolated network partitions. A unified resource orchestration strategy, able to integrate the selection, configuration and management of compute and network resources, as part of a single abstracted object, is missing. This work aims to minimise the silo-effect, and proposes end-to-end slices as the foundation for the comprehensive orchestration of compute resources, network resources, and services in the cloud-to-edge continuum, as well acting as the basis for a system implementation. The concept of the end-to-end slice is formally described via a graph-based model that allows for dynamic resource discovery, selection and mapping via different algorithms and optimisation goals; and a working system is presented as the way to build slices across multiple domains dynamically, based on that model. These are independently accessible objects that abstract resources of various providers – traded via a Marketplace – with compute slices, allocated using the bare-metal cloud approach, being interconnected to each other via the connectivity of network slices. Experiments, carried out on a real testbed, demonstrate three features of the end-to-end slices: resources can be selected, allocated and controlled in a softwarised fashion; tenants can instantiate distributed IoT services on those resources transparently; the performance of a service is absolutely not affected by the status of other slices that share the same resource infrastructure.}
}
@article{OSTROWSKI2023103724,
title = {Mobility-aware fog computing in dynamic networks with mobile nodes: A survey},
journal = {Journal of Network and Computer Applications},
volume = {219},
pages = {103724},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103724},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523001431},
author = {Krzysztof Ostrowski and Krzysztof Małecki and Piotr Dziurzański and Amit Kumar Singh},
keywords = {Mobile fog computing, Mobility-aware, Dynamic networks},
abstract = {Fog computing is an evolving paradigm that addresses the latency-oriented performance and spatio-temporal issues of the cloud services by providing an extension to the cloud computing and storage services in the vicinity of the service requester. In dynamic networks, where both the mobile fog nodes and the end users exhibit time-varying characteristics, including dynamic network topology changes, there is a need of mobility-aware fog computing, which is very challenging due to various dynamisms, and yet systematically uncovered. This paper presents a comprehensive survey on the fog computing compliant with the OpenFog (IEEE 1934) standardised concept, where the mobility of fog nodes constitutes an integral part. A review of the state-of-the-art research in fog computing implemented with mobile nodes is conducted. The review includes the identification of several models of fog computing concept established on the principles of opportunistic networking, social communities, temporal networks, and vehicular ad-hoc networks. Relevant to these models, the contributing research studies are critically examined to provide an insight into the open issues and future research directions in mobile fog computing research.}
}
@article{CASTANAR2022114438,
title = {Topological derivative-based topology optimization of incompressible structures using mixed formulations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {390},
pages = {114438},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.114438},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521006757},
author = {Inocencio Castañar and Joan Baiges and Ramon Codina and Henning Venghaus},
keywords = {Topology optimization, Topological derivative, Incompressible elasticity, Mixed interpolations, Stabilization methods},
abstract = {In this work an algorithm for topological optimization, based on the topological derivative concept, is proposed for both nearly and fully incompressible materials. In order to deal with such materials, a new decomposition of the Polarization tensor is proposed in terms of its deviatoric and volumetric components. Mixed formulations applied in the context of linear elasticity do not only allow to deal with incompressible material behavior but also to obtain a higher accuracy in the computation of stresses. The system is stabilized by means of the Variational Multiscale method based on the decomposition of the unknowns into resolvable and subgrid scales in order to prevent fluctuations. Several numerical examples are presented and discussed to assess the robustness of the proposed formulation and its applicability to Topology Optimization problems for incompressible elastic solids.}
}
@article{DAMSGAARD2023109872,
title = {Approximate computing in B5G and 6G wireless systems: A survey and future outlook},
journal = {Computer Networks},
volume = {233},
pages = {109872},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109872},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623003171},
author = {Hans Jakob Damsgaard and Aleksandr Ometov and Md Munjure Mowla and Adam Flizikowski and Jari Nurmi},
keywords = {Approximate computing, Beyond 5G, Mobile networks},
abstract = {As modern 5G systems are being deployed, researchers question whether they are sufficient for the oncoming decades of technological evolution. Growing numbers of interconnected intelligent devices put these networks under tremendous pressure, demanding their development. Paving the way for beyond 5G and 6G systems, commonly denoted by B5G herein, therefore means seeking enablers to increase efficiency from different perspectives. One novel look on this is the application of inexact computations where nine 9s reliability is not needed, for example, in non-critical mobile broadband traffic. The paradigm of Approximate Computing (AxC) focuses on such areas where constrained quality degradation results in savings that benefit the users and operators. This paper surveys the state-of-the-art publications on the intersection of AxC and B5G systems, identifying and emphasizing trends and tendencies in existing work and directions for future research. The work highlights resource allocation algorithms as particularly mesmerizing in the former, while research related to Intelligent Reflective Surfaces appears the most prominent in the latter. In both, problems are often NP-hard and, thus, only solvable using heuristics or approximations, Successive Convex Approximation and Reinforcement Learning are most frequently applied.}
}
@article{ISHII2022103048,
title = {Precise path computation based on functional block-based disaggregation for future heterogeneous access-metro networks},
journal = {Optical Fiber Technology},
volume = {73},
pages = {103048},
year = {2022},
issn = {1068-5200},
doi = {https://doi.org/10.1016/j.yofte.2022.103048},
url = {https://www.sciencedirect.com/science/article/pii/S1068520022002310},
author = {Kiyo Ishii and Shu Namiki},
keywords = {Disaggregation, Network resource management, FBD model},
abstract = {Emerging 5G/6G mobile services are intended to cover a broad range of use cases, including applications requiring high bandwidth, low latency, and/or high reliability. To meet these diverse requirements, various optical network technologies and architectures, including optical node structures, transmission technologies, and virtualization technologies, have been extensively investigated. These novel technologies will be integrated to form a platform of future optical access-metro networks that support various mobile services. Such an optical access-metro platform will comprise heterogeneous node structures based on various optical functional blocks (e.g., wavelength selective switches, optical splitters, or arrayed waveguide gratings). To realize a versatile optical access-metro platform, a network resource management system that can universally handle heterogeneous node structures is indispensable. This study investigates the applicability of a functional block-based disaggregation (FBD) approach to such a resource-management system. Here, the previously proposed FBD model is extended to incorporate latency aware path computation. The results demonstrated that the FBD model could be successfully used with heterogeneous network structures, including both passive and switchable optical nodes. The precise path computation capability of the model was also demonstrated, and the scalability of the computation time was quantitatively evaluated in a parallel processing environment. Precise path computation can effectively consider both intra- and inter-node fiber connection lengths, which are useful for handling latency requirements based on an accurate representation of the propagation delay.}
}
@article{JALALIKHALILABADI2023100550,
title = {Task scheduling in fog environment — Challenges, tools & methodologies: A review},
journal = {Computer Science Review},
volume = {48},
pages = {100550},
year = {2023},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2023.100550},
url = {https://www.sciencedirect.com/science/article/pii/S1574013723000175},
author = {Zahra {Jalali Khalil Abadi} and Najme Mansouri and Mahshid Khalouie},
keywords = {Fog computing, Cloud computing, Task scheduling, Literature review},
abstract = {Even though cloud computing offers many advantages, it can be a poor choice sometimes because of its slow response to existing requests, leading to the need for fog computing. Scheduling tasks in a fog environment is a major challenge. It is important that IoT clients execute their tasks in a timely manner and obtain lower-cost services; however, they are also looking for tasks to be executed in a secure manner. In this paper, we review the advantages, limitations, and issues associated with scheduling algorithms proposed by a number of different researchers for fog environments. For fog computing developers, we compare different simulation tools to help them choose the product that is most appropriate and flexible for simulating the application they are considering. Finally, open issues and promising research directions associated with task scheduling in fog computing are discussed.}
}
@article{SICARI2021107578,
title = {An Osmotic Computing Enabled Domain Naming System (OCE-DNS) for distributed service relocation between cloud and edge},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107578},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107578},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621005176},
author = {Christian Sicari and Antonino Galletta and Antonio Celesti and Maria Fazio and Massimo Villari},
keywords = {Cloud computing, Edge computing, Internet of Things, Osmotic computing, Service migration, Domain Name System},
abstract = {Osmotic computing has emerged as a solution enabling the Cloud–Edge–Internet of Things (IoT) continuum. It supports the transparent deployment of distributed services called Micro ELements (MELs) over these heterogeneous software layers guaranteeing application proximity to end-users. This paradigm is very useful for data mining processing of emerging future Internet applications because it allows optimizing both the response time and the usage of computational resources. In this paper, we focus on an approach for identifying MELs which serve a specific geographical area. In particular, we present the Osmotic Computing Enabled Domain Name System (OCE-DNS), an advanced DNS able to reference MELs using the Extended Plus Codes, a three dimensional geocode algorithm defined by us. Experiments show that OCE-DNS guarantees quick Resource Records (RR) readings and updates, thence supporting the management of transparent Osmotic MEL migrations.}
}
@article{HANNOUSSE2021100415,
title = {Securing microservices and microservice architectures: A systematic mapping study},
journal = {Computer Science Review},
volume = {41},
pages = {100415},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100415},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000551},
author = {Abdelhakim Hannousse and Salima Yahiouche},
keywords = {Microservices, Microservice architectures, Security, Systematic mapping},
abstract = {Microservice architectures (MSA) are becoming trending alternatives to existing software development paradigms notably for developing complex and distributed applications. Microservices emerged as an architectural design pattern aiming to address the scalability and ease the maintenance of online services. However, security breaches have increased threatening availability, integrity and confidentiality of microservice-based systems. A growing body of literature is found addressing security threats and security mechanisms to individual microservices and microservice architectures. The aim of this study is to provide a helpful guide to developers about already recognized threats on microservices and how they can be detected, mitigated or prevented; we also aim to identify potential research gaps on securing MSA. In this paper, we conduct a systematic mapping in order to categorize threats on MSA with their security proposals. Therefore, we extracted threats and details of proposed solutions reported in selected studies. Obtained results are used to design a lightweight ontology for security patterns of MSA. The ontology can be queried to identify source of threats, security mechanisms used to prevent each threat, applicability layer and validation techniques used for each mechanism. The systematic search yielded 1067 studies of which 46 are selected as primary studies. The results of the mapping revealed an unbalanced research focus in favor of external attacks; auditing and enforcing access control are the most investigated techniques compared with prevention and mitigation. Additionally, we found that most proposed solutions are soft-infrastructure applicable layer compared with other layers such as communication and deployment. We also found that performance analysis and case studies are the most used validation techniques of security proposals.}
}
@article{KHAN2023110008,
title = {Realistic assessment of transport protocols performance over LEO-based communications},
journal = {Computer Networks},
volume = {236},
pages = {110008},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110008},
url = {https://www.sciencedirect.com/science/article/pii/S138912862300453X},
author = {Fátima Khan and Cristina Hervella and Luis Diez and Fátima Fernández and Néstor J. Hernández Marcano and Rune Hylsberg Jacobsen and Ramón Agüero},
keywords = {QUIC, LEO, Scheduling, LMS, TCP},
abstract = {We study the performance exhibited by the transport protocols, Transport Control Protocol (TCP) and QUIC, over realistic satellite networks. We propose a novel methodology, which combines real implementation (exploiting virtualization techniques) and simulation, to carry out systematic and repetitive experiments. We modify the default operation of the ns-3 framework and we integrate the dynamism that characterizes satellite communication links, particularly Low Earth Orbit (LEO). We carry out a thorough assessment over different setups, changing the operating frequency band and packet buffer lengths. In addition, we ascertain the impact of using the multi-streaming feature that QUIC integrates. The results show that QUIC yields lower delays than TCP, although it might suffer from higher jitter in particular setups. In addition, the results evince that using multiple streams in QUIC does not yield a relevant gain for the default Round-Robin (RR) scheduler. We propose more appropriate scheduling strategies, which are able to yield better performances with unbalanced traffic. Even if the behavior of transport protocols over non-terrestrial-networks might not be always appropriate, the obtained results evince that QUIC can definitively bring benefits when compared to TCP. Furthermore, we have shown that optimal scheduling policies yields a fairer performance when using multiple flows, having unbalanced traffic loads.}
}
@article{HOSSAIN20231162,
title = {The role of microservice approach in edge computing: Opportunities, challenges, and research directions},
journal = {ICT Express},
volume = {9},
number = {6},
pages = {1162-1182},
year = {2023},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2023.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S2405959523000760},
author = {Md. Delowar Hossain and Tangina Sultana and Sharmen Akhter and Md Imtiaz Hossain and Ngo Thien Thu and Luan N.T. Huynh and Ga-Won Lee and Eui-Nam Huh},
keywords = {Edge computing, Microservices, Monolithic architectures, Microservice security, AI},
abstract = {Edge computing has emerged as a promising computing paradigm that enables real-time data processing and analysis closer to the data source and boosts decision-making applications in a safe manner. On the other hand, the microservice is a new type of architecture that can be dynamically deployed, migrating across edge clouds on demand. Therefore, the combination of these two technologies can provide numerous benefits, including improved performance, reduced latency, and better resource utilization. In this paper, we present a thorough analysis of state-of-the-art research on the use of microservices in edge computing environments. We take into consideration several distinct microservice research directions, including coordination, orchestration, repositories, scheduling, autoscaling, deployment, resource management, and different security issues. Furthermore, we explore the potential applications of microservices in edge computing across various domains. Finally, the unsolved research issues and future directions of emerging trends in this area are also discussed.}
}
@article{ISLAM202346,
title = {Optimal placement of applications in the fog environment: A systematic literature review},
journal = {Journal of Parallel and Distributed Computing},
volume = {174},
pages = {46-69},
year = {2023},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522002465},
author = {Mohammad Mainul Islam and Fahimeh Ramezani and Hai Yan Lu and Mohsen Naderpour},
keywords = {Fog computing, Application placement, Service placement, Resource management},
abstract = {The fog-computing paradigm complements cloud computing to support the deployment and execution of latency-sensitive applications at the network edge by offering enhanced computational power. Optimal placement of such applications over a fog network comprising geographically distributed, heterogeneous, and resource-constrained fog nodes is a core challenge in fog-computing paradigm research. This study systematically reviews existing research on optimal fog application placement over the cloud-to-thing continuum. Surveyed articles are analyzed in four aspects: i) layers of the cloud-to-thing continuum considered for placing an application; ii) application characteristics that are considered in making placement decisions; iii) application placement mechanism; iv) tools and technology for placing an application. This review also categorizes the research problems associated with fog application placement. Finally, based on this review, we suggest directions for future adaptive fog-application placement research.}
}
@article{HERNANDEZNIEVES2021104327,
title = {CEBRA: A CasE-Based Reasoning Application to recommend banking products},
journal = {Engineering Applications of Artificial Intelligence},
volume = {104},
pages = {104327},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104327},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621001755},
author = {Elena Hernández-Nieves and Guillermo Hernández and Ana B. Gil-González and Sara Rodríguez-González and Juan M. Corchado},
keywords = {Case-based reasoning, Fog Computing, Virtual agents, Artificial intelligence, Fintech, Commercial banking},
abstract = {Following data ethics and respecting the clients’ privacy, the banking environment can use the client data that is available to them to offer personalized services to its clients. Intelligent recommender systems can support this attempt through specialized technological architectures. This article proposes the inclusion of CEBRA (CasE-Based Reasoning Application), a case-based reasoning system oriented to commercial banking, in a Fog Computing architecture coordinated by virtual agents. Throughout this article, the model of this architecture is presented and its life cycle is described, and improvements are proposed through the incorporation of several techniques in the retrieve and reuse phases, including the extraction of interests expressed by users on their social network profiles and collaborative filtering systems. A comprehensive case study has been carried out and a dataset of 60,000 cases has been generated to evaluate CEBRA. As a result, the Recommender System is presented, by including, the recommendation algorithm and a REST interface for its use. The recommendations are based on the user’s profile, previous ratings and/or additional knowledge such as the user’s contextual information. The proposal takes advantage of contextual information to support the promotion of banking and financial products, improving user satisfaction.}
}
@article{SOUZA2023446,
title = {EdgeSimPy: Python-based modeling and simulation of edge computing resource management policies},
journal = {Future Generation Computer Systems},
volume = {148},
pages = {446-459},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23002340},
author = {Paulo S. Souza and Tiago Ferreto and Rodrigo N. Calheiros},
keywords = {Simulation, Modeling, Edge computing, Resource management, Containers, Python},
abstract = {The increasing popularity of applications with tight latency requirements has motivated research on Edge Computing, which positions computing resources near data sources at the Internet’s edge. Despite the emergence of simulation tools that make prototype validation less complex, time-consuming, and expensive, researchers and practitioners still face significant challenges when developing resource management strategies for the edge, as existing simulators fall short in providing a fine-grained model of edge applications provisioning. To overcome this challenge, we propose EdgeSimPy, a simulation framework written in Python for modeling and evaluating resource management policies in Edge Computing environments. EdgeSimPy features a modular architecture that incorporates several functional abstractions for edge servers, network devices, and applications with built-in models for user mobility, application composition, and power consumption that allow the simulation of various scenarios. Furthermore, we propose a novel conceptual model that accurately represents the entire lifecycle of edge applications and ensures seamless integration with real application traces. In addition to submitting EdgeSimPy to an in-depth verification that checks the simulator implementation, we discuss case studies that show EdgeSimPy in action in different large-scale scenarios.}
}
@article{ANISETTI202334,
title = {An assurance process for Big Data trustworthiness},
journal = {Future Generation Computer Systems},
volume = {146},
pages = {34-46},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001371},
author = {Marco Anisetti and Claudio A. Ardagna and Filippo Berto},
keywords = {Non-functional assurance, Big Data transparency, Trustworthiness, Security, Distributed systems, Monitoring},
abstract = {Modern (industrial) domains are based on large digital ecosystems where huge amounts of data and information need to be collected, shared, and analyzed by multiple actors working within and across organizational boundaries. This data-driven ecosystem poses strong requirements on data management and data analysis, as well as on data protection and system trustworthiness. However, although Big Data has reached its functional maturity and represents a key enabler for enterprises to compete in the global market, the assurance and trustworthiness of Big Data computations (e.g., security, privacy) are still in their infancy. While functionally appealing, Big Data does not provide a transparent environment with clear non-functional properties, impairing the users’ ability to evaluate its behavior and clashing with modern data-privacy regulations. In this paper, we present a novel assurance process for Big Data, which evaluates the Big Data pipelines, and the Big Data ecosystem underneath, to provide a comprehensive measure of their trustworthiness. To the best of our knowledge, this approach is the first attempt to address the general problem of Big Data trustworthiness in an holistic way. We experimentally evaluate our solution in a real Big Data Analytics-as-a-Service environment, first presenting a detailed walkthrough evaluation, and then showing its feasibility and negligible performance overhead (i.e., approx 1 min).}
}
@article{ZONI2021100450,
title = {An FPU design template to optimize the accuracy-efficiency-area trade-off},
journal = {Sustainable Computing: Informatics and Systems},
volume = {29},
pages = {100450},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100450},
url = {https://www.sciencedirect.com/science/article/pii/S2210537920301761},
author = {Davide Zoni and Andrea Galimberti and William Fornaciari},
keywords = {Floating Point Units (FPU), Accuracy-Cost-energy tradeoff, Run-time optimization},
abstract = {Modern embedded systems are in charge of an increasing number of tasks that extensively employ floating-point (FP) computations. The ever-increasing efficiency requirement, coupled with the additional computational effort to perform FP computations, motivates several microarchitectural optimizations of the FPU. This manuscript presents a novel modular FPU microarchitecture, which targets modern embedded systems and considers heterogeneous workloads including both best-effort and accuracy-sensitive applications. The design optimizes the EDP-accuracy-area figure of merit by allowing, at design-time, to independently configure the precision of each FP operation, while the FP dynamic range is kept common to the entire FPU to deliver a simpler microarchitecture. To ensure the correct execution of accuracy-sensitive applications, a novel compiler pass allows to substitute each FP operation for which a low-precision hardware support is offered with the corresponding soft-float function call. The assessment considers seven FPU variants encompassing three different state-of-the-art designs. The results on several representative use cases show that the binary32 FPU implementation offers an EDP gain of 15%, while, in case the FPU implements a mix of binary32 and bfloat16 operations, the EDP gain is 19%, the reduction in the resource utilization is 21% and the average accuracy loss is less than 2.5%. Moreover, the resource utilization of our FPU variants is aligned with the one of the FPU employing state-of-the-art, highly specialized FP hardware accelerators. Starting from the assessment, a set of guidelines is drawn to steer the design of the FP hardware support in modern embedded systems.}
}
@article{TIAN2023103725,
title = {MADDPG-empowered slice reconfiguration approach for 5G multi-tier system},
journal = {Journal of Network and Computer Applications},
volume = {219},
pages = {103725},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103725},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523001443},
author = {Chenjing Tian and Haotong Cao and Jun Xie and Sahil Garg and Joel J.P.C. Rodrigues and M. Shamim Hossain},
keywords = {Network slicing, Slice reconfiguration, Multi-agent deep reinforcement learning, Multi-tier edge-cloud system, Fifth generation (5G) network},
abstract = {With the advancement of fifth-generation (5G) communication technology, plenty of vertical applications have emerged. These applications usually center on highly different quality of service (QoS) requirements, posing challenges for the supporting communication network. To address these challenges, network slicing has been proposed to partition the underlying network into multiple tailored logical networks, called as network slices. Each slice corresponds to a specific application. However, fluctuations in user activities for applications may lead to workload variation across corresponding slices, which compromising the consistent QoS provisioning. In this context, slice reconfiguration becomes imperative to ensure satisfactory user experiences. In this work, we introduce the multi-agent deep reinforcement learning method to tackle the slice reconfiguration problem and propose the multi-agent deep deterministic policy gradient-empowered slice reconfiguration (MADDPG-SR) algorithm. In MADDPG-SR, each network function is abstracted as an agent that learns to optimize its own migration in the multi-layer network to minimize the preference-weighted processing resource cost while adhering QoS and resource constraints. The proposed method is compared with four other benchmarks, and the comparison results demonstrate that MADDPG-SR can significantly reduce both the preference-weighted resource cost, and the running time for acquiring slice reconfiguration decisions while ensuring the QoS performance of slices.}
}
@article{SICARI2022102822,
title = {Insights into security and privacy towards fog computing evolution},
journal = {Computers & Security},
volume = {120},
pages = {102822},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102822},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822002164},
author = {Sabrina Sicari and Alessandra Rizzardi and Alberto Coen-Porisini},
keywords = {Fog computing, Internet of things, Security, Privacy, Cloud computing, Fog networking},
abstract = {The incremental diffusion of the Internet of Things (IoT) technologies and applications represents the outcome of a world ever more connected by means of heterogeneous and mobile devices. IoT scenarios imply the presence of multiple data producers (e.g., sensors, actuators, RFID, NFC) and consumers (e.g., end-user devices, such as smartphones, tablets, and PCs). A variety of standards and protocols must cooperate to efficiently gather, process, and share the information. The fog computing paradigm, due to its distributed nature, represents a viable solution to cope with interoperability, scalability, security, and privacy issues, which naturally emerge, since it operates as an intermediate layer between data consumers/producers and traditional cloud systems. This paper analyzes the evolution in the modeling of new methodologies, related to fog computing and IoT, showing how moving security and privacy tasks toward the edge of the network provide both advantages and new challenges to be faced in this research field. The proposed discussion provides an overview of requirements for the realization of secure and privacy-aware IoT-based fog computing infrastructures.}
}
@incollection{RANGANATH202263,
title = {Chapter Three - Industry initiatives across edge computing},
editor = {Pethuru Raj and Kavita Saini and Chellammal Surianarayanan},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {127},
pages = {63-115},
year = {2022},
booktitle = {Edge/Fog Computing Paradigm: The Concept Platforms and Applications},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2022.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065245822000468},
author = {Sunku Ranganath},
keywords = {Edge computing, Linux foundation, ETSI MEC, 5GAA, O-RAN, CNCF, Small cell forum, Broadband forum, GSMA, Smart edge},
abstract = {To accelerate the evolution and adoption of Edge computing various standard bodies, open-source projects and industry consortia have come together in recent times to revolutionize Edge compute. This chapter goes through various initiatives across the world that have major traction in terms of collaboration, collateral produced and industry impact. Architecture, collateral produced and details of the projects involved are described.}
}
@article{VEIGA20234,
title = {Towards containerized, reuse-oriented AI deployment platforms for cognitive IoT applications},
journal = {Future Generation Computer Systems},
volume = {142},
pages = {4-13},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22004320},
author = {Tiago Veiga and Hafiz Areeb Asad and Frank Alexander Kraemer and Kerstin Bach},
keywords = {Cognitive IoT, Self-adaptive IoT, Cognitive architecture, Container-based deployment},
abstract = {IoT applications with their resource-constrained sensor devices can benefit from adjusting their operations to the phenomena they sense and the environments they operate in, leading to the paradigm of self-adaptive, autonomous, or cognitive IoT. On the other side, current AI deployment platforms focus on the provision and reuse of machine learning models through containers that can be wired together to build new applications. The challenge is that composition mechanisms of the AI platforms, albeit effective due to their simplicity, are in fact too simplistic to support cognitive IoT applications, in which sensor devices also benefit from the machine learning results. Our objective is to perform a gap analysis between the requirements of cognitive IoT applications on the one side and the current functionalities of AI deployment platforms on the other side. In this work, we provide an overview of the paradigms in AI deployment platforms and the requirements of cognitive IoT applications. We study a use case for person counting in a skiing area through camera sensors, and how this use case benefits from letting the IoT sensors have access to operational knowledge in the form of visual attention models. We describe the implementation of the IoT application using an AI deployment platform, analyze its shortcomings, and necessary workarounds. From the use case, we identify and generalize five gaps that limit the usage of deployment platforms: the transparent management of multiple instances of components, a more seamless integration with IoT devices, explicit definition of data flow triggers, and the availability of templates for cognitive IoT architectures and reuse below the top-level.}
}
@article{ROSENDO202271,
title = {Distributed intelligence on the Edge-to-Cloud Continuum: A systematic literature review},
journal = {Journal of Parallel and Distributed Computing},
volume = {166},
pages = {71-94},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522000843},
author = {Daniel Rosendo and Alexandru Costan and Patrick Valduriez and Gabriel Antoniu},
keywords = {Edge computing, Distributed intelligence, Big Data Analytics, Computing Continuum, Reproducibility},
abstract = {The explosion of data volumes generated by an increasing number of applications is strongly impacting the evolution of distributed digital infrastructures for data analytics and machine learning (ML). While data analytics used to be mainly performed on cloud infrastructures, the rapid development of IoT infrastructures and the requirements for low-latency, secure processing has motivated the development of edge analytics. Today, to balance various trade-offs, ML-based analytics tends to increasingly leverage an interconnected ecosystem that allows complex applications to be executed on hybrid infrastructures where IoT Edge devices are interconnected to Cloud/HPC systems in what is called the Computing Continuum, the Digital Continuum, or the Transcontinuum. Enabling learning-based analytics on such complex infrastructures is challenging. The large scale and optimized deployment of learning-based workflows across the Edge-to-Cloud Continuum requires extensive and reproducible experimental analysis of the application execution on representative testbeds. This is necessary to help understand the performance trade-offs that result from combining a variety of learning paradigms and supportive frameworks. A thorough experimental analysis requires the assessment of the impact of multiple factors, such as: model accuracy, training time, network overhead, energy consumption, processing latency, among others. This review aims at providing a comprehensive vision of the main state-of-the-art libraries and frameworks for machine learning and data analytics available today. It describes the main learning paradigms enabling learning-based analytics on the Edge-to-Cloud Continuum. The main simulation, emulation, deployment systems, and testbeds for experimental research on the Edge-to-Cloud Continuum available today are also surveyed. Furthermore, we analyze how the selected systems provide support for experiment reproducibility. We conclude our review with a detailed discussion of relevant open research challenges and of future directions in this domain such as: holistic understanding of performance; performance optimization of applications; efficient deployment of Artificial Intelligence (AI) workflows on highly heterogeneous infrastructures; and reproducible analysis of experiments on the Computing Continuum.}
}
@article{DELUCIA2023207,
title = {Unlocking the potential of edge computing for hyperspectral image classification: An efficient low-energy strategy},
journal = {Future Generation Computer Systems},
volume = {147},
pages = {207-218},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001802},
author = {Gianluca {De Lucia} and Marco Lapegna and Diego Romano},
keywords = {Hyperspectral classification, Edge Computing, Principal Component Analysis, GPU computing},
abstract = {Despite recent improvements, the computing capability of Edge Computing devices is still inferior to high-end servers, so special methodologies are required to consider the computing environment while developing algorithms. In the present work, we propose a hybrid technique to make the classification of Hyperspectral Images feasible and effective through a Convolutional Neural Network on low-power and high-performance sensor devices. More specifically, we combine two strategies: we initially use the Principal Component Analysis method to discard non-significant wavelengths and shrink the dataset; then, we apply a process acceleration method to boost performance by implementing a form of GPU-based parallelism. The experiments demonstrate the technique’s effectiveness in terms of performance and energy consumption: it enables correct classifications even with low-power devices often deployed on Unmanned Aerial Vehicles, where the network connection is unpredictable or erratic.}
}
@article{GILL2021100391,
title = {A comprehensive study of simulation frameworks and research directions in fog computing},
journal = {Computer Science Review},
volume = {40},
pages = {100391},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100391},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000319},
author = {Monika Gill and Dinesh Singh},
keywords = {Fog computing, Edge, Simulation},
abstract = {Context:
Fog computing paradigm consists of resource constrained devices that support data processing and service provisioning at the edge of the network. Simulation frameworks play a key role in the design, development and validation of novel approaches for fog environment. The existing fog simulators model one or more aspects of fog environment and hence it becomes a tedious task to analyse and choose them as per the research requirements.
Objective:
This paper reviews the literature of simulation tools for fog computing and aims to help the novice researchers to explore and assess fog related proposals.
Method:
The study has employed a systematic search procedure to identify relevant articles published in the duration of 2015-2020.
Results:
The relevant publications are evaluated to highlight their strengths and underline the limitations. A comparative analysis of studies based on eight characteristic and few non technical features is presented. The scope for improvement in fog simulators is reported. Lastly, the prevailing research challenges in fog that can be addressed with reviewed simulation frameworks are detailed out.
Conclusion:
The paper has identified an increased interest in the development of novel and extended fog simulators thus emphasizing their importance. Also, the need to develop more advanced fog simulators that can model a wider range of fog environments is recognized. Directions are given for future work.}
}
@article{MIMRAN2022102890,
title = {Security of Open Radio Access Networks},
journal = {Computers & Security},
volume = {122},
pages = {102890},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102890},
url = {https://www.sciencedirect.com/science/article/pii/S016740482200284X},
author = {Dudu Mimran and Ron Bitton and Yehonatan Kfir and Eitan Klevansky and Oleg Brodt and Heiko Lehmann and Yuval Elovici and Asaf Shabtai},
keywords = {Open radio access networks, 5G, Risk assessment, Threat analysis},
abstract = {The Open Radio Access Network (O-RAN) is a promising radio access network (RAN) architecture aimed at reshaping the RAN industry toward an open, adaptive, and intelligent RAN. In this paper, we perform a comprehensive security analysis of O-RANs. Specifically, we review the architectural blueprint designed by the O-RAN Alliance, leader in the cellular ecosystem. As part of the security analysis, we provide a detailed overview of the O-RAN architecture; present an ontology for evaluating the security of a system that is currently at an early development stage; identify O-RAN’s high-risk areas; enumerate O-RAN’s threat actors; and model potential threats to O-RAN. The significance of this work is in providing an updated attack surface to cellular network operators. Based on the attack surface, cellular network operators can carefully deploy the appropriate countermeasures to improve O-RAN’s security.}
}
@article{CAIAZZA2022213,
title = {Edge computing vs centralized cloud: Impact of communication latency on the energy consumption of LTE terminal nodes},
journal = {Computer Communications},
volume = {194},
pages = {213-225},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422002730},
author = {Chiara Caiazza and Silvia Giordano and Valerio Luconi and Alessio Vecchio},
keywords = {Edge computing, Energy saving, IoT communication},
abstract = {Edge computing brings several advantages, such as reduced latency, increased bandwidth, and improved locality of traffic. One aspect that is not sufficiently understood is to what extent the different communication latency experienced in the edge-cloud continuum impacts on the energy consumption of clients. We studied the energy consumption of a request–response communication scheme when an LTE node communicates with edge-based or cloud-based servers. Results show that the reduced latency of edge servers bring significant benefits in terms of energy consumption. Experiments also show how the energy savings brought by edge computing are influenced by the prevalent direction of data transfer (upload vs download), load of the server, and daytime/nighttime operation.}
}
@article{VELASQUEZ2022311,
title = {Resource orchestration in 5G and beyond: Challenges and opportunities},
journal = {Computer Communications},
volume = {192},
pages = {311-315},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422002213},
author = {Karima Velasquez and David {Perez Abreu} and Marilia Curado and Edmundo Monteiro},
keywords = {5G, 6G, Orchestration, Architecture, Network automation},
abstract = {5G networks have strict constraints regarding the services in terms of latency, reliability, and availability, which pose additional challenges to the traditional orchestration solutions, and 6G networks will increment the number of slices and services deployed over different technological domains, adding more difficulties for the orchestrator. Distributed and automated solutions will be essential for this context. This article identifies the main challenges in 5G/6G orchestration and then describes the utility of Artificial Intelligence-driven solutions, outlining an orchestrator architecture for 5G networks. The architecture is then explored as an orchestration solution for two ongoing research projects focused on the deployment of critical services over 5G networks.}
}
@article{KATSARAGAKIS2023102936,
title = {A memory footprint optimization framework for Python applications targeting edge devices},
journal = {Journal of Systems Architecture},
volume = {142},
pages = {102936},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2023.102936},
url = {https://www.sciencedirect.com/science/article/pii/S1383762123001157},
author = {Manolis Katsaragakis and Lazaros Papadopoulos and Mario Konijnenburg and Francky Catthoor and Dimitrios Soudris},
keywords = {Memory management, Edge computing, Resource management, Python},
abstract = {The advantages of processing data at the source motivate developers to offload computations at the edges of IoT networks. However, the computational resource constraints of edge computing devices limit the opportunities for deploying applications developed in high-level languages at the edge. In contrast to C/C++, applications developed in Python and other dynamic high-level languages, often require an increased amount of memory size, due to their inherent static memory management approach. Therefore, developers targeting the deployment of applications in dynamic high-level languages at the edge need support by methodologies and tools, which will allow these applications to exploit the limited memory resources efficiently. This work presents a memory optimization framework for applications developed in the Python programming language targeting edge devices. Aiming to avoid the inherent pitfalls of static memory management that Python’s integrated memory manager imposes, the framework targets the reduction of the required memory footprint, by integrating a set of static and dynamic optimizations. The evaluation results, based on a set of representative real-life benchmark suites and applications, show 64% average memory footprint reduction, over the CPython’s minimum baseline of 24 MB. Additionally, we investigate the impact of memory size reduction on execution time and energy consumption. The results show 51% lower execution time and 47.3% reduction in the energy consumed.}
}
@article{KUMARI2022109137,
title = {Task offloading in fog computing: A survey of algorithms and optimization techniques},
journal = {Computer Networks},
volume = {214},
pages = {109137},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109137},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622002547},
author = {Nidhi Kumari and Anirudh Yadav and Prasanta K. Jana},
keywords = {Fog computing, Task offloading, Delay & latency, Energy & power consumption, Cost, Machine learning},
abstract = {The exponential growth in Internet of Things (IoT) devices and the limitations of cloud computing in terms of latency and quality of service for time-sensitive applications have led to the unfolding of the efficient middleware technology called fog. Fog computing circumvents the limitations of the cloud by creating a seamless continuum between the things/IoT/end-user devices and the cloud and reducing the effective distance. However, fog computing faces challenges for offloading tasks for their remote computation at some level. Hence, the optimality of task offloading is the primary research area in fog computing. Several contemporary papers exist on this important subject. The research gap in reviewing all these task offloading algorithms has motivated us for their presentation in the form of a detailed survey in this paper. There exist some survey papers which deal with the task offloading. However, none of them has covered the basics of optimization techniques and their solution approaches. The primary objective of this paper is to provide the readers with a complete overview of the journey from a task offloading idea to its mathematical problem formulation and finally to its solution with all details of optimization techniques. We begin by introducing fog computing, and task offloading process followed by several task offloading factors governing decision-making process and their surveys. A section is fully dedicated to the survey of offloading objectives with examples. We also present several optimization approaches used in task offloading. Finally, the last section dedicates to the challenges and future direction in fog computing. The outcomes of the survey will benefit readers in learning the optimization used in task offloading, and it will also provide them a systematic design of offloading scheme with specific objectives.}
}
@article{MULFARI2023455,
title = {Toward a lightweight ASR solution for atypical speech on the edge},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {455-463},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003023},
author = {Davide Mulfari and Lorenzo Carnevale and Massimo Villari},
keywords = {Artificial intelligence, Machine learning, Dysarthria, Automatic speech recognition, Edge computing, Speech disorder},
abstract = {In this article, to the purpose of simplifying challenges in designing automatic speech recognition (ASR) systems working on disordered speech, we focus on an isolated word recognition solution based on a convolutional neural network architecture to predict the presence of precise voice commands within atypical utterances. Italian speech recognizers have trained from scratch on custom datasets by following a speaker dependent approach, then their performances (in terms of word error rate) have been investigated with the collaboration of 16 Italian speakers with motor and speech impairments. Our ASR system works on Mel-frequency Cepstral Coefficients features extraction method, and thanks to its inner structure, the trained ASR model can be deployed on edge computing nodes where the local inference task requires limited computational resources. We acknowledge that it may have significant implications in the field of assistive technology. Then, in the end of the paper, we present a prototype of an edge computing device aimed at supporting the voice interaction between its user with speech disorders and his/her surrounding smart environment through a custom interaction with virtual assistants’ services.}
}
@incollection{RANGANATH202235,
title = {Chapter Two - Edge computing: Types and attributes},
editor = {Pethuru Raj and Kavita Saini and Chellammal Surianarayanan},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {127},
pages = {35-62},
year = {2022},
booktitle = {Edge/Fog Computing Paradigm: The Concept Platforms and Applications},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2022.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065245822000456},
author = {Sunku Ranganath},
keywords = {Edge Computing, IoT, Access Edge, On-premise, Wireless access, Challenges, Latency, O-RAN},
abstract = {The chapter on “Edge computing types and Attributes” provides introduction to various types of Edge computing by broadly classifying Edge in to four types, based on round trip latency requirements, (1) IoT Edge (2) Wireless Access Edge (3) On Premise Edge (4) Network Edge. Requirements and attributes of each of these edge compute types are discussed. Chapter then details the practical challenges across these Edge deployments and further explores how ETSI MEC specifications helps address these challenges.}
}
@article{MAHBUB2023103726,
title = {Contemporary advances in multi-access edge computing: A survey of fundamentals, architecture, technologies, deployment cases, security, challenges, and directions},
journal = {Journal of Network and Computer Applications},
volume = {219},
pages = {103726},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103726},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523001455},
author = {Mobasshir Mahbub and Raed M. Shubair},
keywords = {MEC, IoT, Digital twin, Open-source MEC, Quantum computing, AI, Security},
abstract = {With advancements of cloud technologies Multi-Access Edge Computing (MEC) emerged as a remarkable edge-cloud technology to provide computing facilities to resource-restrained edge user devices. Utilizing the features of MEC user devices can obtain computational services from the network edge which drastically reduces the transmission latency of evolving low-latency applications such as video analytics, e-healthcare, etc. The objective of the work is to perform a thorough survey of the recent advances relative to the MEC paradigm. In this context, the work overviewed the fundamentals, architecture, state-of-the-art enabling technologies, evolving supporting/assistive technologies, deployment scenarios, security issues, and solutions relative to the MEC technology. The work, moreover, stated the relative challenges and future directions to further improve the features of MEC.}
}
@article{MANOLIS2021149,
title = {Mechanical models and numerical simulations in nanomechanics: A review across the scales},
journal = {Engineering Analysis with Boundary Elements},
volume = {128},
pages = {149-170},
year = {2021},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0955799721000898},
author = {George D. Manolis and Petia S. Dineva and Tsviatko Rangelov and Dimitris Sfyris},
keywords = {Nanomechanics, Atomic level, Multi-scale models, Continuum mechanics, Numerical methods},
abstract = {This work gives an overview of the theoretical background and of the numerical modelling framework used to describe the mechanical properties and the response of materials on scales ranging from the atomistic, through the microstructure and all the way up to the macroscale. In order to describe the dual nature of the structure of matter, which is continuous when viewed at large length scales and discrete when viewed at the atomic scale, plus the interdependence of these scales, multiscale modelling is required to complement the continuum and the atomistic models. More specifically, what we aim for in this review is to present and discuss the following basic conceptual models, as well as the methodologies that accompany them: (a) discrete models such as ab initio, atomistic / molecular, mesoscopic; (b) continuum mechanics models (CMM) comprising pure CMM, non-local elasticity CMM, higher-order strain gradient and higher-order nonlocal strain gradient elasticity CMM, and surface elasticity CMM; (c) multiscale material models (MMM). Since the field of nanomechanics is currently a rapidly expanding research area, the presented state-of-the art is by no means exhaustive. It simply outlines the research efforts that go behind formulating numerical models for the solution of problems in nanomechanics. Despite the advantages that boundary element methods (BEM) have in solving problems at the physical scale, either as stand-alone or in combination with finite element methods (FEM), their application to multiscale modelling is still limited, despite the promise they seem to hold.}
}
@article{AFZAL2023103581,
title = {A holistic survey of multipath wireless video streaming},
journal = {Journal of Network and Computer Applications},
volume = {212},
pages = {103581},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103581},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522002223},
author = {Samira Afzal and Vanessa Testoni and Christian Esteve Rothenberg and Prakash Kolan and Imed Bouazizi},
keywords = {Wireless video streaming, Multipath routing, Packet scheduling, Heterogeneous networks},
abstract = {Demand for wireless video streaming services increases with users expecting to access high-quality video streaming experiences. Ensuring Quality of Experience (QoE) is quite challenging due to varying bandwidth and time constraints. Since most of today’s mobile devices are equipped with multiple network interfaces, one promising approach is to benefit from multipath communications. Multipathing leads to higher aggregate bandwidth and distributing video traffic over multiple network paths improves stability, seamless connectivity, and QoE. However, most of current transport protocols do not match the requirements of video streaming applications or are not designed to address relevant issues, such as networks heterogeneity, head-of-line blocking, and delay constraints. In this comprehensive survey, we first review video streaming standards and technology developments. We then discuss the benefits and challenges of multipath video transmission over wireless. We provide a holistic literature review of multipath wireless video streaming, shedding light on the different alternatives from an end-to-end layered stack perspective, reviewing key multipath wireless scheduling functions, unveiling trade-offs of each approach, and presenting a suitable taxonomy to classify the state-of-the-art. Finally, we discuss open issues and avenues for future work.}
}
@article{ZHENG2023105,
title = {A package-aware scheduling strategy for edge serverless functions based on multi-stage optimization},
journal = {Future Generation Computer Systems},
volume = {144},
pages = {105-116},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23000547},
author = {Senjiong Zheng and Bo Liu and Weiwei Lin and Xiaoying Ye and Keqin Li},
keywords = {Serverless function offloading, Dependency package awareness, Package caching strategy},
abstract = {Serverless computing offers a promising deployment model for edge IoT applications. However, serverless functions that rely on large libraries suffer from severe library loading latency when containerized, which is unfriendly to edge latency-sensitive applications. Most function offload strategies in edge environments ignore the impact of this latency. We also found that the measures taken by serverless platforms to reduce loading latency may not work in edge environments. To remedy that, this paper proposes a function offloading strategy to minimize loading latency, a new way to deeply integrate placement optimization with cache optimization. In this way, we first design a package caching policy suitable for edge environments based on the consistency of execution topology. Then a Double Layers Dynamic Programming algorithm (DLDP) is proposed to solve the problem of function offloading considering the dependent packages using a multi-stage progressive optimization approach. The caching policy is embedded in the scheduling algorithm through a phased optimization approach to achieve joint optimization. Extensive experiments on the cluster trace from Alibaba show that DLDP reduces the loading latency of packages by more than 97.84% and significantly outperforms four baselines in the application completion time by more than 55.67%.}
}
@article{DONTA2022100642,
title = {Delay-aware data fusion in duty-cycled wireless sensor networks: A Q-learning approach},
journal = {Sustainable Computing: Informatics and Systems},
volume = {33},
pages = {100642},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100642},
url = {https://www.sciencedirect.com/science/article/pii/S2210537921001256},
author = {Praveen Kumar Donta and Tarachand Amgoth and Chandra Sekhara Rao Annavarapu},
keywords = {Data fusion, Delay-aware, Duty-cycle, Internet of Things, Network lifetime, Q-learning, Throughput, Wireless sensor networks},
abstract = {In wireless sensor networks (WSNs), the sensor nodes (SNs) are deployed to acquire the data from the area of interest and transmit it to the sink via multi-hop communications. Due to computation, buffer, and energy constraints, the SNs need efficient routing to forward the data in time to sink with limited energy drain, and it is a challenging task. It is more difficult in duty-cycled WSNs because the SNs are active for a limited time and inactive in the remaining time to minimize the energy drain. In this context, we propose a delay-aware data fusion (DADF) approach to the trade-off between the delay and energy while performs the data fusion. Initially, the DADF performing the data fusion operation to avoid duplicating and inconsistent data at each SNs using a simple statistical approach during its active state. Afterward, the sink uses reinforcement learning to identify the best forwarding node of each SN for data communications with minimum delay and energy in duty-cycled WSNs. Each forwarding node operates the data fusion once if it acquires the data from its child nodes. The proposed method using various performance metrics such as network lifetime, throughput, energy consumption, buffer utilization, and end-to-end delay are compared with recent and relevant existing techniques, and our methods outperform them in varying dynamic conditions.}
}
@article{AHMAD2023100568,
title = {Deep learning models for cloud, edge, fog, and IoT computing paradigms: Survey, recent advances, and future directions},
journal = {Computer Science Review},
volume = {49},
pages = {100568},
year = {2023},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2023.100568},
url = {https://www.sciencedirect.com/science/article/pii/S1574013723000357},
author = {Shahnawaz Ahmad and Iman Shakeel and Shabana Mehfuz and Javed Ahmad},
keywords = {Deep learning, Cloud computing, Edge computing, Fog computing, IoT, Models},
abstract = {In recent times, the machine learning (ML) community has recognized the deep learning (DL) computing model as the Gold Standard. DL has gradually become the most widely used computational approach in the field of machine learning, achieving remarkable results in various complex cognitive tasks that are comparable to, or even surpassing human performance. One of the key benefits of DL is its ability to learn from vast amounts of data. In recent years, the DL field has witnessed rapid expansion and has found successful applications in various conventional areas. Significantly, DL has outperformed established ML techniques in multiple domains, such as cloud computing, robotics, cybersecurity, and several others. Nowadays, cloud computing has become crucial owing to the constant growth of the IoT network. It remains the finest approach for putting sophisticated computational applications into use, stressing the huge data processing. Nevertheless, the cloud falls short because of the crucial limitations of cutting-edge IoT applications that produce enormous amounts of data and necessitate a quick reaction time with increased privacy. The latest trend is to adopt a decentralized distributed architecture and transfer processing and storage resources to the network edge. This eliminates the bottleneck of cloud computing as it places data processing and analytics closer to the consumer. Machine learning (ML) is being increasingly utilized at the network edge to strengthen computer programs, specifically by reducing latency and energy consumption while enhancing resource management and security. To achieve optimal outcomes in terms of efficiency, space, reliability, and safety with minimal power usage, intensive research is needed to develop and apply machine learning algorithms. This comprehensive examination of prevalent computing paradigms underscores recent advancements resulting from the integration of machine learning and emerging computing models, while also addressing the underlying open research issues along with potential future directions. Because it is thought to open up new opportunities for both interdisciplinary research and commercial applications, we present a thorough assessment of the most recent works involving the convergence of deep learning with various computing paradigms, including cloud, fog, edge, and IoT, in this contribution. We also draw attention to the main issues and possible future lines of research. We hope this survey will spur additional study and contributions in this exciting area.}
}
@article{SALIS202320,
title = {An Edge-Cloud based Reference Architecture to support cognitive solutions in Process Industry},
journal = {Procedia Computer Science},
volume = {217},
pages = {20-30},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.198},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022761},
author = {Antonio Salis and Angelo Marguglio and Gabriele {De Luca} and Silvia Razzetti and Walter Quadrini and Sergio Gusmeroli},
keywords = {Industry 4.0, process industry, smart manufacturing, reference architecture, cloud, edge computing, cognitive computing, artificial intelligence, big data analytics},
abstract = {Process Industry is one of the leading sectors of the world economy, characterized however by intense environmental impact, and very high-energy consumption. Despite a traditional low innovation pace in PI, in the recent years a strong push at worldwide level towards the dual objective of improving the efficiency of plants and the quality of products, significantly reducing the consumption of electricity and CO2 emissions has taken momentum. Digital Technologies (namely Smart Embedded Systems, IoT, Data, AI and Edge-to-Cloud Technologies) are enabling drivers for a Twin Digital-Green Transition, as well as foundations for human centric, safe, comfortable and inclusive workplaces. Currently, digital sensors in plants produce a large amount of data, which in most cases constitutes just a potential and not a real value for Process Industry, often locked-in in close proprietary systems and seldomly exploited. Digital technologies, with process modelling-simulation via digital twins, can build a bridge between the physical and the virtual worlds, bringing innovation with great efficiency and drastic reduction of waste. In accordance with the guidelines of Industrie 4.0 this work proposes a modular and scalable Reference Architecture, based on open source software, which can be implemented both in brownfield and greenfield scenarios. The ability to distribute processing between the edge, where the data have been created, and the cloud, where the greatest computational resources are available, facilitates the development of integrated digital solutions with cognitive capabilities. The reference architecture is being validated in the three pilot plants, paving the way to the development of integrated planning solutions, with scheduling and control of the plants, optimizing the efficiency and reliability of the supply chain, and balancing energy efficiency.}
}
@article{BUKHARI2022114,
title = {Fog node discovery and selection: A Systematic literature review},
journal = {Future Generation Computer Systems},
volume = {135},
pages = {114-128},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001625},
author = {Afnan Bukhari and Farookh Khadeer Hussain and Omar K. Hussain},
keywords = {Fog computing, Fog nodes, Fog service, Fog node discovery, Fog node selection, Trust-based selection},
abstract = {Fog computing is a new computing paradigm that extends cloud services by providing computing resources in the form of fog nodes closer to the edge devices. Fog computing supports mobility, location-awareness, low latency, and saves network bandwidth to efficiently process and store the data generated by edge devices. A prerequisite to achieve these aims is first the discovery of optimal fog nodes before selecting the most appropriate one/s. To achieve these aims, there are various selection criteria that need to be satisfied. The aim of this study is to provide a detailed overview of the state of the art in the area of fog node discovery and selection. To achieve this aim, we first define the different requirements that need to be met in identifying optimal fog nodes. We then identify the existing literature between 2014–2021 and compare them against the defined requirements in identifying optimal fog nodes. Based on the analysis, we present the gaps and open issues in the existing literature relating to optimal fog node discovery and selection.}
}
@article{MONTOYAMUNOZ2022107252,
title = {Reliability provisioning for Fog Nodes in Smart Farming IoT-Fog-Cloud continuum},
journal = {Computers and Electronics in Agriculture},
volume = {200},
pages = {107252},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107252},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005658},
author = {Ana Isabel Montoya-Munoz and Rodrigo A.C. da Silva and Oscar M. Caicedo Rendon and Nelson L.S. da Fonseca},
keywords = {Reliability, Smart Farming, Internet of Things, Fog Computing, Optimization, Redundancy},
abstract = {Reliability is essential in Smart Farming supported by the IoT-Fog-Cloud continuum. Smart Farms’ unprotection may cause significant economic losses and low yields of production. This paper introduces an optimization model for providing reliability and, consequently, service continuity to the IoT-Fog-Cloud continuum-based smart farms. The proposed model allows Smart Farming stakeholders to find the optimal number of Fog Nodes needed to deploy farming services considering the heterogeneity in the fog capabilities, resource demands, redundancy techniques, and reliability requirements. The model was solved using linear programming and evaluated with different demands and protection schemes. Results show that protection schemes guarantee high reliability and reveal that a shared redundancy scheme reduces deployment cost and yet provides reliability. Results also indicate that deployment costs and resources depend on the type of fog-based smart farm services to serve. Moreover, they show that deploying more low-resource hardware can be less expensive for low-reliability demands than deploying with a few high-resource hardware.}
}
@article{SAEIK2021108177,
title = {Task offloading in Edge and Cloud Computing: A survey on mathematical, artificial intelligence and control theory solutions},
journal = {Computer Networks},
volume = {195},
pages = {108177},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108177},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621002322},
author = {Firdose Saeik and Marios Avgeris and Dimitrios Spatharakis and Nina Santi and Dimitrios Dechouniotis and John Violos and Aris Leivadeas and Nikolaos Athanasopoulos and Nathalie Mitton and Symeon Papavassiliou},
keywords = {Edge Computing, Task offloading, Resource allocation, Control theory, Mathematical optimization, Artificial intelligence},
abstract = {Next generation communication networks are expected to accommodate a high number of new and resource-voracious applications that can be offered to a large range of end users. Even though end devices are becoming more powerful, the available local resources cannot cope with the requirements of these applications. This has created a new challenge called task offloading, where computation intensive tasks need to be offloaded to more resource powerful remote devices. Naturally, the Cloud Computing is a well-tested infrastructure that can facilitate the task offloading. However, Cloud Computing as a centralized and distant infrastructure creates significant communication delays that cannot satisfy the requirements of the emerging delay-sensitive applications. To this end, the concept of Edge Computing has been proposed, where the Cloud Computing capabilities are repositioned closer to the end devices at the edge of the network. This paper provides a detailed survey of how the Edge and/or Cloud can be combined together to facilitate the task offloading problem. Particular emphasis is given on the mathematical, artificial intelligence and control theory optimization approaches that can be used to satisfy the various objectives, constraints and dynamic conditions of this end-to-end application execution approach. The survey concludes with identifying open challenges and future directions of the problem at hand.}
}
@article{PEDRATSCHER202257,
title = {M2FaaS: Transparent and fault tolerant FaaSification of Node.js monolith code blocks},
journal = {Future Generation Computer Systems},
volume = {135},
pages = {57-71},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.04.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001509},
author = {Stefan Pedratscher and Sasko Ristov and Thomas Fahringer},
keywords = {Cloud, FaaS, FaaSification, Functions, Portability, Serverless},
abstract = {Porting existing monoliths to the Function-as-a-Service (FaaS) (FaaSification) can be very challenging for software developers due to different architectural styles. For a successful porting, developers need to resolve various dependencies, such as method invocations of external packages or user-defined codes, as well as global and local variables used in and after the code block that should be faasified. To bridge the gap and automatize FaaSification, this paper introduces M2FaaS, a FaaSifier that automatically converts a Node.js monolith into a hybrid by faasifying annotated code blocks as serverless functions on multiple FaaS providers. M2FaaS is a novel FaaSifier that resolves many challenges for the resulting monolith to work properly after the FaaSification. Developers may annotate all dependencies that need to be resolved for the generated functions to run properly and specify variables that should be returned by the function to the monolith because they are used later in the monolith. Moreover, M2FaaS is the first FaaSifier that faasifies arbitrary code blocks. The current M2FaaS prototype supports FaaSification of individual functions on two FaaS providers, AWS Lambda and IBM Cloud Functions. Finally, M2FaaS introduces an optional annotation for alternative functions to be invoked in case the primary faasified function fails. The resulting hybrid application invokes the automatically deployed serverless functions, while the original code remains executable. Experiments with four complementary monoliths demonstrate that M2FaaS outperforms state-of-the-art FaaSifiers in terms of development effort by up to 73.3%. Moreover, with the fault tolerance support, M2FaaS finishes all submitted functions, thereby achieving by 18.5% higher throughput than the other FaaSifiers.}
}
@article{SHINDE2021108598,
title = {A network operator-biased approach for multi-service network function placement in a 5G network slicing architecture},
journal = {Computer Networks},
volume = {201},
pages = {108598},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108598},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621004989},
author = {Swapnil Sadashiv Shinde and Dania Marabissi and Daniele Tarchi},
keywords = {5G, Network slicing, Edge Computing, Network function placement, Genetic Algorithms},
abstract = {The 5G communication standard is characterized by an increased softwarization, allowing a higher flexibility able to cope with different requirements and services. In particular, Network Function Virtualization (NFV) is a recently introduced technology that enables a software implementation of different network functions exploiting virtualization techniques, hence, enabling their flexible deployment upon system requirements. Boosted by NFV, the concept of network slicing is gaining great attention in 5G networks. The idea is that physical communication and computing resources are sliced in multiple end-to-end logical networks, each one tailored to best support a specific service. The advantages of NFV, in the network slicing context, are even more evident in distributed computing environments, such as the edge-to-cloud continuum, recently introduced for enabling a flexible deployment of multiple functions. In particular, thanks to the introduction of cloud-native technologies, based on the usage of containerization and microservice technologies, the virtual network functions (VNFs) deployment and their orchestration is an easy operation, allowing the on-the-fly network configuration. Gaining from the NFV, Network Slicing and Edge-to-Cloud continuum paradigms, we propose a new network function allocation problem for multi-service 5G networks, able to deploy network functions on a distributed computing environment depending on the service requests. The proposed approach jointly considers Radio Access Network (RAN) and Core Network (CN) functions and, differently from other approaches, introduces an option able to bias the function placement depending on the service requirements, allowing a fast-and-easy operator-side deployment of the network functions. We propose to solve the problem through a Genetic Algorithm able to approach the optimal solution but with reduced complexity and execution time. The performance is compared with two other heuristic algorithms and with an exhaustive search algorithm, introduced as benchmarks, showing the benefits of the selected solution in terms of performance, flexibility and complexity.}
}
@article{RODRIGUEZCONDE2023126835,
title = {Cloud-assisted collaborative inference of convolutional neural networks for vision tasks on resource-constrained devices},
journal = {Neurocomputing},
volume = {560},
pages = {126835},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126835},
url = {https://www.sciencedirect.com/science/article/pii/S092523122300958X},
author = {Ivan Rodriguez-Conde and Celso Campos and Florentino Fdez-Riverola},
keywords = {Collaborative inference, Convolutional neural networks, Mobile cloud computing, DNN splitting, Task offloading, Distributed computing},
abstract = {This work analyses the most relevant research conducted under the mobile cloud computing paradigm to bring vision tasks supported by state-of-the-art deep convolutional neural networks closer to the end-user through collaborative intelligence. In particular, this review aims to comprehensively address collaborative inference on convolutional networks, offering the reader a detailed explanation of the main methods and technologies used to partition and deploy such models on the UE-edge-cloud continuum, which have made it possible to leverage the capabilities of resource-constrained devices to alleviate, and ideally eliminate, the traditional dependence on the cloud for high-performance computing, thereby enabling a more rational exploitation of the supporting hardware infrastructure. The paper details the technical aspects of the different frameworks designed to support these tasks, examining and comparing the mechanisms and techniques used to conceive pertinent configurations. Moreover, the study outlines the various algorithmic solutions developed for synthesizing optimal co-inference schemes, also covering the design conventions adopted and the tweaks implemented to optimize the overall performance delivered, concluding with a discussion of the specific challenges that have arisen thus far, as well as the following steps to be taken in this field.}
}
@article{VESCOVI2022100606,
title = {Linking scientific instruments and computation: Patterns, technologies, and experiences},
journal = {Patterns},
volume = {3},
number = {10},
pages = {100606},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100606},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002318},
author = {Rafael Vescovi and Ryan Chard and Nickolaus D. Saint and Ben Blaiszik and Jim Pruyne and Tekin Bicer and Alex Lavens and Zhengchun Liu and Michael E. Papka and Suresh Narayanan and Nicholas Schwarz and Kyle Chard and Ian T. Foster},
keywords = {Experiment automation, workflow, Globus, synchrotron light source, big data, machine learning, data fabric, computing fabric, trust fabric, scientific facility},
abstract = {Summary
Powerful detectors at modern experimental facilities routinely collect data at multiple GB/s. Online analysis methods are needed to enable the collection of only interesting subsets of such massive data streams, such as by explicitly discarding some data elements or by directing instruments to relevant areas of experimental space. Thus, methods are required for configuring and running distributed computing pipelines—what we call flows—that link instruments, computers (e.g., for analysis, simulation, artificial intelligence [AI] model training), edge computing (e.g., for analysis), data stores, metadata catalogs, and high-speed networks. We review common patterns associated with such flows and describe methods for instantiating these patterns. We present experiences with the application of these methods to the processing of data from five different scientific instruments, each of which engages powerful computers for data inversion,model training, or other purposes. We also discuss implications of such methods for operators and users of scientific facilities.}
}
@article{SARTI2023519,
title = {Anticipate, Ensemble and Prune: Improving Convolutional Neural Networks via Aggregated Early Exits},
journal = {Procedia Computer Science},
volume = {222},
pages = {519-528},
year = {2023},
note = {International Neural Network Society Workshop on Deep Learning Innovations and Applications (INNS DLIA 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923009559},
author = {Simone Sarti and Eugenio Lomurno and Matteo Matteucci},
keywords = {Early Exits, Ensemble, Pruning, AEP, Image classification, Convolutional Neural Networks},
abstract = {Today, artificial neural networks are the state of the art for solving a variety of complex tasks, especially in image classification. Such architectures consist of a sequence of stacked layers with the aim of extracting useful information and having it processed by a classifier to make accurate predictions. However, intermediate information within such models is often left unused. In other cases, such as in edge computing contexts, these architectures are divided into multiple partitions that are made functional by including early exits, i.e., intermediate classifiers, with the goal of reducing the computational and temporal load without extremely compromising the accuracy of the classifications. In this paper, we present Anticipate, Ensemble and Prune (AEP), a new training technique based on a weighted ensemble of early exits, which aims at exploiting the information in the structure of networks to maximise their performance. Through a comprehensive set of experiments, we show how the use of this approach can yield average accuracy improvements of up to 15% over traditional training. AEP's internal pruning operation also allows reducing the number of parameters by up to 41%, lowering the number of multiplications and additions by 18% and the latency time to make inference by 16%. By using AEP, it is also possible to learn weights that allow early exits to achieve better accuracy values than those obtained from single-output reference models. The code will be available on GitHub after acceptance of the paper.}
}
@article{XUE2023307,
title = {Integration of blockchain and edge computing in internet of things: A survey},
journal = {Future Generation Computer Systems},
volume = {144},
pages = {307-326},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003521},
author = {He Xue and Dajiang Chen and Ning Zhang and Hong-Ning Dai and Keping Yu},
keywords = {Blockchain, Edge computing, Internet of things, Resource management, Security and privacy, Data management},
abstract = {As an important technology to ensure data security, consistency, traceability, etc., blockchain has been increasingly used in Internet of Things (IoT) applications. The integration of blockchain and edge computing (IBEC) can further improve the resource utilization in terms of network, computing, storage, and security. This paper aims to present a survey on the IBEC. In particular, we first give an overview of blockchain and edge computing respectively. We then present a general architecture of an IBEC system. We next study the various applications of the IBEC in IoT. We also discuss the optimizations of the IBEC system and solutions from perspectives of resource management and performance improvement. Finally, we analyze and summarize the existing challenges posed by the IBEC system and the potential solutions in the future.}
}
@article{ALSHAMRANI20224687,
title = {IoT and artificial intelligence implementations for remote healthcare monitoring systems: A survey},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part A},
pages = {4687-4701},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001385},
author = {Mazin Alshamrani},
keywords = {RHM, H-IoT, Healthcare, Machine learning, Monitoring systems},
abstract = {The Internet of Things (IoT) and artificial intelligence (AI) are two of the fastest-growing technologies in the world. With more people moving to cities, the concept of a smart city is not foreign. The idea of a smart city is based on transforming the healthcare sector by increasing its efficiency, lowering costs, and putting the focus back on a better patient care system. Implementing IoT and AI for remote healthcare monitoring (RHM) systems requires a deep understanding of different frameworks in smart cities. These frameworks occur in the form of underlying technologies, devices, systems, models, designs, use cases, and applications. The IoT-based RHM system mainly employs both AI and machine learning (ML) by gathering different records and datasets. On the other hand, ML methods are broadly used to create analytic representations and are incorporated into clinical decision support systems and diverse healthcare service forms. After carefully examining each factor in clinical decision support systems, a unique treatment, lifestyle advice, and care strategy are proposed to patients. The technology used helps to support healthcare applications and analyze activities, body temperature, heart rate, blood glucose, etcetera. Keeping this in mind, this paper provides a survey that focuses on the identification of the most relevant health Internet of things (H-IoT) applications supported by smart city infrastructure. This study also evaluates related technologies and systems for RHM services by understanding the most pertinent monitoring applications based on several models with different corresponding IoT-based sensors. Finally, this research contributes to scientific knowledge by highlighting the main limitations of the topic and recommending possible opportunities in this research area.}
}
@article{SAVAGLIO2021107562,
title = {Introduction to the Special Section on Research challenges and directions of Data Mining in Edge Computing systems (VSI-dmec)},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107562},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107562},
url = {https://www.sciencedirect.com/science/article/pii/S004579062100505X},
author = {Claudio Savaglio and Teemu Leppänen and Giuseppe {Di Fatta}}
}
@article{SABBIONI2022108993,
title = {DIFFUSE: A DIstributed and decentralized platForm enabling Function composition in Serverless Environments},
journal = {Computer Networks},
volume = {210},
pages = {108993},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.108993},
url = {https://www.sciencedirect.com/science/article/pii/S138912862200161X},
author = {Andrea Sabbioni and Lorenzo Rosa and Armir Bujari and Luca Foschini and Antonio Corradi},
keywords = {Serverless computing, Function composition, Shared memory, Middleware, Latency, Distributed system},
abstract = {Serverless computing is an emerging proposition in the cloud offering landscape that promotes a higher level of abstraction, further decoupling software operations from the underlying hardware. Often recognized as an economically driven computational approach, the serverless model relies on the execution of short-lived stateless functions, enabling a fine-grained accounting and control of resources. In this context, function composition represents an appealing feature, allowing the composition of two or more functions to create tailored processing pipelines, incentivizing modularity and reusability of functions, while paving the way to application-specific run-time optimizations. This work presents DIFFUSE: a DIstributed and decentralized platForm enabling Function composition in Serverless Environments. DIFFUSE embodies an innovative infrastructural support, enabling the efficient and transparent composition of functions by relying on pluggable middleware support, serving as a conveyor of messages among the platform components. Broadening the deployment spectrum of our proposal, we assess different middleware solutions, each presenting distinct delivery profiles, evidencing the tradeoffs that emerge.}
}
@article{RZEPKA20221,
title = {SDN-based fog and cloud interplay for stream processing},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {1-17},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000140},
author = {Michał Rzepka and Piotr Boryło and Marcos D. Assunção and Artur Lasoń and Laurent Lefèvre},
keywords = {Stream processing, Fog computing, Edge computing, SDN},
abstract = {This paper focuses on SDN-based approaches for deploying stream processing workloads on heterogeneous environments comprising wide-area networks, cloud and fog resources. Stream processing applications impose strict latency requirements to operate appropriately. Deploying workloads in the fog reduces unnecessary delays, but its computational resources may not handle all the tasks. On the other hand, offloading the tasks to the cloud is constrained by limited network resources and involves additional transmission delays that exceed latency thresholds. Adaptive workload deployment may solve these issues by ensuring that resource and latency requirements are satisfied for all the data streams processed by an application. This paper’s main contribution consists of dynamic workload placement algorithms operating on stream processing requests with latency constraints. Provisioning of computing infrastructure exploits the interplay between fog and cloud under limited network capacity. The algorithms aim to maximize the ratio of successfully handled requests by effectively utilizing available resources while meeting application latency constraints. Experiments demonstrate that the goal can be achieved by detailed analysis of requests and ensuring balanced computing and network resources utilization. As a result, up to 30% improvement over the reference algorithms in success rate is observed.}
}
@article{BADIDI2023958,
title = {On workflow scheduling for latency-sensitive edge computing applications},
journal = {Procedia Computer Science},
volume = {220},
pages = {958-963},
year = {2023},
note = {The 14th International Conference on Ambient Systems, Networks and Technologies Networks (ANT) and The 6th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.03.132},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923006671},
author = {Elarbi Badidi},
keywords = {Edge computing, cloud computing, latency, scheduling},
abstract = {With the rapid proliferation of edge computing-based solutions, many edge computing applications use the cloud for data processing and analysis. However, latency-sensitive applications have low latency requirements and can be very bandwidth hungry, so processing their collected data through cloud servers is not efficient and cost-effective. For example, traffic management and health condition monitoring applications, require real-time data processing at the network edge to respond immediately to unexpected events. These applications are typically executed as workflows with dependent tasks that require careful scheduling to allocate the appropriate resources to the tasks so that the execution is done in a way that satisfies the users’ target functions. In this work, we evaluate some traditional scheduling heuristics for the execution of workflow tasks in an edge-computing scenario. Our goal is to compare their performance in terms of execution time and cost and show that these heuristics, previously used in scheduling in the context of cloud environments, can also be used in edge computing scenarios. The results show that the MinMin and PSO scheduling algorithms offer the best results with regard to execution time and cost.}
}
@article{DELTOROLLORENS2021106636,
title = {An isogeometric finite element-boundary element approach for the vibration analysis of submerged thin-walled structures},
journal = {Computers & Structures},
volume = {256},
pages = {106636},
year = {2021},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2021.106636},
url = {https://www.sciencedirect.com/science/article/pii/S0045794921001589},
author = {Alvaro {del Toro Llorens} and Josef Kiendl},
keywords = {Isogeometric analysis, Finite element method, Boundary element method, Modal analysis, Generalized added mass matrix},
abstract = {In this paper, the isogeometric formulations of the finite element and boundary element methods are applied to the dynamic analysis of thin-walled structures submerged in an infinite, inviscid, and incompressible fluid medium. This fluid–structure interaction problem is decoupled using the modal analysis technique, and the fluid effect on the structure is taken into account through the generalized added mass matrix. The structure is modeled with NURBS-based Kirchhoff–Love shell elements. The fluid response is computed using a regularized boundary integral equation. We take advantage of the geometry preserving property of the NURBS refinement techniques to reduce the computational cost without the need for a projection scheme. The implementation is benchmarked with three test cases, and good accuracy is obtained for a relatively low number of degrees of freedom.}
}
@article{COSTA2022109189,
title = {Monitoring fog computing: A review, taxonomy and open challenges},
journal = {Computer Networks},
volume = {215},
pages = {109189},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109189},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622002845},
author = {Breno Costa and João Bachiega and Leonardo Rebouças Carvalho and Michel Rosa and Aleteia Araujo},
keywords = {Monitoring, Orchestration, Fog computing, Taxonomy, Fog monitoring},
abstract = {Fog computing is a distributed paradigm that provides computational resources in the users’ vicinity. Fog orchestration is a set of functionalities that coordinate the dynamic infrastructure and manage the services to guarantee the Service Level Agreements. Monitoring is an orchestration functionality of prime importance. It is the basis for resource management actions, collecting status of resource and service and delivering updated data to the orchestrator. There are several cloud monitoring solutions and tools, but none of them comply with fog characteristics and challenges. Fog monitoring solutions are scarce, and they may not be prepared to compose an orchestration service. This paper updates the knowledge base about fog monitoring, assessing recent subjects in this context like observability, data standardization and instrumentation domains. We propose a novel taxonomy of fog monitoring solutions, supported by a systematic review of the literature. Fog monitoring proposals are analyzed and categorized by this new taxonomy, offering researchers a comprehensive overview. This work also highlights the main challenges and open research questions.}
}
@article{CAVIGLIONE2021108010,
title = {Kernel-level tracing for detecting stegomalware and covert channels in Linux environments},
journal = {Computer Networks},
volume = {191},
pages = {108010},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108010},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621001249},
author = {Luca Caviglione and Wojciech Mazurczyk and Matteo Repetto and Andreas Schaffhauser and Marco Zuppelli},
keywords = {eBPF, Syscall and network tracing, Stegomalware, Covert channels, Detection},
abstract = {Modern malware is becoming hard to spot since attackers are increasingly adopting new techniques to elude signature- and rule-based detection mechanisms. Among the others, steganography and information hiding can be used to bypass security frameworks searching for suspicious communications between processes or exfiltration attempts through covert channels. Since the array of potential carriers is very large (e.g., information can be hidden in hardware resources, various multimedia files or network flows), detecting this class of threats is a scarcely generalizable process and gathering multiple behavioral information is time-consuming, lacks scalability, and could lead to performance degradation. In this paper, we leverage the extended Berkeley Packet Filter (eBPF), which is a recent code augmentation feature provided by the Linux kernel, for programmatically tracing and monitoring the behavior of software processes in a very efficient way. To prove the flexibility of the approach, we investigate two realistic use cases implementing different attack mechanisms, i.e., two processes colluding via the alteration of the file system and hidden network communication attempts nested within IPv6 traffic flows. Our results show that even simple eBPF programs can provide useful data for the detection of anomalies, with a minimal overhead. Furthermore, the flexibility to develop and run such programs allows to extract relevant features that could be used for the creation of datasets for feeding security frameworks exploiting AI.}
}
@article{HUANG2022114811,
title = {Adaptive stochastic morphology simulation and mesh generation of high-quality 3D particulate composite microstructures with complex surface texture},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {393},
pages = {114811},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.114811},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522001372},
author = {Junjie Huang and Fangqian Deng and Lingfei Liu and Jianqiao Ye},
keywords = {Particulate composite materials, Microstructure, Surface texture, Heat kernel smoothing},
abstract = {Particulate composite materials have a broad range of potential applications in engineering and other disciplines. Accurate modeling of their microstructures and fast generation of the finite element meshes play a vital role in investigating many micromechanical phenomena and improving understanding of the underlying failure mechanisms. Due to the exceedingly intricate multiscale internal structures that they possess, the modeling and meshing of their microstructures still remain difficult in general. In this work, we present a computational framework and methodology for the representation, simulation, and mesh generation of 3D stochastic microstructures of particulate composites. Towards this goal, we propose a multi-level multiscale scheme that allows for capturing the multiscale structures of particulate composite materials at both the coarse and fine scales. A briging scale approach based on heat kernel smoothing is also presented to seamlessly link the coarse and fine scales. In addition to the microstructural modeling of particulate composite materials, we also develop an adaptive curvature-based surface and volume mesh generation algorithm for particulate composite microstructures with complex surface texture. Following the implementation of the morphology and mesh generation algorithm, a series of numerical examples are presented to demonstrate the capability and potential of the proposed method.}
}
@article{CAPODIECI2021481,
title = {Improving emergency response in the era of ADAS vehicles in the Smart City},
journal = {ICT Express},
volume = {7},
number = {4},
pages = {481-486},
year = {2021},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2021.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405959521000382},
author = {Nicola Capodieci and Roberto Cavicchioli and Filippo Muzzini and Leonard Montagna},
keywords = {Smart City, Multi-agent systems, Simulation},
abstract = {Management of emergency vehicles can be fostered within a Smart City, i.e. an urban environment in which many IoT devices are orchestrated by a distributed intelligence able to suggest to road users the best course of action in different traffic situations. By extending MATSim (Multi-Agent Transport Simulation Software), we design and test appropriate mitigation strategies when traffic accidents occur within an existing urban area augmented with V2V (Vehicle-to-Vehicle), V2I (Vehicle-to-Infrastructure) capabilities and Advanced Driving Assisted cars (ADAS). Further, we propose traffic congestion models and related mechanisms for improving the necessary time for emergency vehicles to respond to accidents.}
}
@article{SHARMA2020723,
title = {Osmotic computing-based service migration and resource scheduling in Mobile Augmented Reality Networks (MARN)},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {723-737},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19310738},
author = {Vishal Sharma and Dushantha Nalin K. Jayakody and Marwa Qaraqe},
keywords = {Osmotic computing, Edge-computing, Augmented reality, Resource scheduling, Service migrations},
abstract = {Resources and services between the servers in Mobile Augmented Reality Networks (MARN) are tedious to manage. These networks comprise users possessing Augmented Reality (AR)-Virtual Reality (VR) applications. Low latency, robustness, and tolerance are the key requirements of these networks, which can be attained by using near-user solutions such as edge computing. However, management of services and scheduling them to near-user servers in an integrated environment of edge and public/private infrastructure are complex tasks. These require an optimal solution, which can be obtained by using “Osmotic Computing”, that has been recently proposed as a paradigm for the integration of edge and public/private cloud. This paper uses osmotic computing for effectively migrating and scheduling the services between the servers of the different layers. The paper also presents the details on various components that are used for applying osmotic computing to a network followed by core applications, types, service classification, migration, and scheduling through the rules of osmotic game formulated for its operations. The evaluations are conducted on 100,000 requests and the proposed approach shows significant performance with the probability of the error being 0.1 at 55.72% conservation of the energy and memory resources for the entire network despite the increasing number of users. The proposed approach also satisfies the conditions of the joint optimization functions presented in the system model and demonstrates that the system holds true even with varying users, thus, proving its robustness and tolerance against the number of users.}
}
@article{20067300,
title = {Contents of Volume 195},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {195},
number = {52},
pages = {7300-7314},
year = {2006},
note = {Computational Modelling of Concrete},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(06)00279-9},
url = {https://www.sciencedirect.com/science/article/pii/S0045782506002799}
}
@article{KOJIC2018156,
title = {Mass release curves as the constitutive curves for modeling diffusive transport within biological tissue},
journal = {Computers in Biology and Medicine},
volume = {92},
pages = {156-167},
year = {2018},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2016.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0010482516301706},
author = {M. Kojic and M. Milosevic and N. Kojic and E.J. Koay and J.B. Fleming and M. Ferrari and A. Ziemys},
keywords = {Diffusion, Porous material, Biological tissue, Mass release curve, Equivalent diffusion coefficient, Multiscale model, Numerical homogenization},
abstract = {In diffusion governed by Fick’s law, the diffusion coefficient represents the phenomenological material parameter and is, in general, a constant. In certain cases of diffusion through porous media, the diffusion coefficient can be variable (i.e. non-constant) due to the complex process of solute displacements within microstructure, since these displacements depend on porosity, internal microstructural geometry, size of the transported particles, chemical nature, and physical interactions between the diffusing substance and the microstructural surroundings. In order to provide a simple and general approach of determining the diffusion coefficient for diffusion through porous media, we have introduced mass release curves as the constitutive curves of diffusion. The mass release curve for a selected direction represents cumulative mass (per surface area) passed in that direction through a small reference volume, in terms of time. We have developed a methodology, based on numerical Finite Element (FE) and Molecular Dynamics (MD) methods, to determine simple mass release curves of solutes through complex media from which we calculate the diffusion coefficient. The diffusion models take into account interactions between solute particles and microstructural surfaces, as well as hydrophobicity (partitioning). We illustrate the effectiveness of our approach on several examples of complex composite media, including an imaging-based analysis of diffusion through pancreatic cancer tissue. The presented work offers an insight into the role of mass release curves in describing diffusion through porous media in general, and further in case of complex composite media such as biological tissue.}
}
@article{ARDAGNA2021661,
title = {Editorial: Special issue on trusted Cloud-Edges computations},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {661-664},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.08.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20326819},
author = {Claudio A. Ardagna and Mauro Conti and Ernesto Damiani and Chia-Mu Yu}
}
@article{MERCER2015268,
title = {Novel formulations of microscopic boundary-value problems in continuous multiscale finite element methods},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {286},
pages = {268-292},
year = {2015},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2014.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0045782514005003},
author = {Brian S. Mercer and Kranthi K. Mandadapu and Panayiotis Papadopoulos},
keywords = {Multiscale, Homogenization, Finite element method, Irving–Kirkwood procedure, Hill–Mandel condition, RVE boundary conditions},
abstract = {This article explores the use of a homogenization method inspired by the classical Irving–Kirkwood procedure in the continuous multiscale modeling of elastic solids within the context of the finite element method. This homogenization method gives rise to a broad range of allowable boundary conditions for the RVE, which, in turn, yield a rich spectrum of estimates of the macroscopic stress response for two representative problems involving heterogeneous elastic bodies.}
}
@article{YAN2012103,
title = {A mesh-free computational framework for predicting buckling behaviors of single-walled carbon nanocones under axial compression based on the moving Kriging interpolation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {247-248},
pages = {103-112},
year = {2012},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2012.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045782512002629},
author = {J.W. Yan and K.M. Liew and L.H. He},
keywords = {Buckling, Higher-order gradient theory, Mechanical property, Moving Kriging interpolation, Single-walled carbon nanocones},
abstract = {This paper employs a higher-order gradient continuum theory for studying mechanical properties of single-walled carbon nanocones (SWCNCs). The SWCNC is constructed by rolling up a fan-shaped graphite sheet and connecting their two sides to form a conical structure. A mesh-free computational framework based on the moving Kriging (MK) interpolation is developed to study the buckling behaviors of SWCNCs under axial compression. Mechanical behaviors of SWCNCs with five different apex angles, i.e. 19.2°, 38.9°, 60°, 84.6° and 112.9°, are studied. Critical strains are predicted with the SWCNC subjects to uniform axial compression on the two ends. Computational results demonstrate that the apex angle has an increasing effect on the critical strain but a decreasing effect on the elastic properties (such as axial Young’s modulus) of SWCNCs. The corresponding buckling patterns reveal that a larger apex angle developed more fins on the side surface of the CNC at critical strain. For some of the CNCs, it is found that the elastic property slightly increases as the cutting tip’s length increases. Besides, a sharp-decrease of the critical strain with an increase number of fins, which approach to ripples, indicates that the CNC becomes unstable.}
}
@article{LIU201496,
title = {Unified gas-kinetic scheme for diatomic molecular simulations in all flow regimes},
journal = {Journal of Computational Physics},
volume = {259},
pages = {96-113},
year = {2014},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113007924},
author = {Sha Liu and Pubing Yu and Kun Xu and Chengwen Zhong},
keywords = {Unified gas-kinetic scheme, Diatomic molecules, Rarefied flows, Hypersonic non-equilibrium flows},
abstract = {A unified gas-kinetic scheme (UGKS) is constructed for both continuum and rarefied flow computations. The underlying principle for the development of UGKS is the direct modeling for the gas evolution process from the kinetic to the hydrodynamic scale, which is used in the flux construction across a cell interface. More specifically, the physical process from the kinetic particle free transport to the hydrodynamic pressure wave propagation is recovered in the flux function. In the previous study, the UGKS has been developed mainly for monatomic gas with particle translational motion only. The construction of time evolution solution is based on the BGK, Shakhov, and ES–BGK models. The UGKS has been validated through extensive numerical tests. In this paper, a UGKS for diatomic gas will be constructed, where the gas-kinetic Rykov model with a Landau–Teller–Jeans-type rotational energy relaxation is used in the numerical scheme. The new scheme will be tested in many cases, such as homogeneous flow relaxation, shock structure calculations, hypersonic flow passing a flat plate, and the flow around a blunt circular cylinder. The analytic, DSMC, and experimental measurements will be used for validating the solutions of UGKS.}
}
@article{WANG2020598,
title = {DYVERSE: DYnamic VERtical Scaling in multi-tenant Edge environments},
journal = {Future Generation Computer Systems},
volume = {108},
pages = {598-612},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.02.043},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19312403},
author = {Nan Wang and Michail Matthaiou and Dimitrios S. Nikolopoulos and Blesson Varghese},
keywords = {Edge computing, Multi-tenancy, Vertical scaling, Dynamic priority, Dynamic scaling},
abstract = {Multi-tenancy in resource-constrained environments is a key challenge in Edge computing. In this paper, we develop ‘DYVERSE: DYnamic VERtical Scaling in Edge’ environments, which is the first light-weight and dynamic vertical scaling mechanism for managing resources allocated to applications for facilitating multi-tenancy in Edge environments. To enable dynamic vertical scaling, one static and three dynamic priority management approaches that are workload-aware, community-aware and system-aware, respectively are proposed. This research advocates that dynamic vertical scaling and priority management approaches reduce Service Level Objective (SLO) violation rates. An online-game and a face detection workload in a Cloud-Edge test-bed are used to validate the research. The merit of DYVERSE is that there is only a sub-second overhead per Edge server when 32 Edge servers are deployed on a single Edge node. When compared to executing applications on the Edge servers without dynamic vertical scaling, static priorities and dynamic priorities reduce SLO violation rates of requests by up to 4% and 12% for the online game, respectively, and in both cases 6% for the face detection workload. Moreover, for both workloads, the system-aware dynamic vertical scaling method effectively reduces the latency of non-violated requests, when compared to other methods.}
}
@article{CELESTI20191,
title = {An approach for the secure management of hybrid cloud–edge environments},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {1-19},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.06.043},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18300682},
author = {Antonio Celesti and Maria Fazio and Antonino Galletta and Lorenzo Carnevale and Jiafu Wan and Massimo Villari},
keywords = {Cloud computing, Edge computing, Management, Communication, Security},
abstract = {The Cloud-of-Things (CoT) paradigm is a challenging approach to manage IoT applications exploiting Cloud resources and services. In order to avoid latency in Cloud–IoT communications, the management of time-sensitive services has to be moved to the edge of the CoT. To this aim, a secure Cloud-to-Edge environment for seamless management of IoT applications is necessary. The realization of a performing and secure Cloud-to-Edge middleware solution is a very strategic goal for future business CoT services. Thus, it needs to be deeply investigated, as highlighted by the Cloud Security Alliance (CSA). A valuable approach to develop an efficient Cloud-to-Edge system is based on an instant-message communication solution. In current Cloud environments, a Message Oriented Middleware (MOM) based on an Instant Message Protocol (IMP) provides good performance, but overlook security requirements. In this paper, we aim at overcoming such a gap following the CSA guidelines. In particular, we discuss the involved issues for improving such a kind of Cloud-to-Edge system in order to achieve data confidentiality, integrity, authenticity and non-repudiation. Moreover, we analyze a real case of study considering a MOM architectural model. Experimental results performed on a real testbed show how the introduced secure capabilities do not affect the overall performances of the whole middleware.}
}
@article{VAQUERO201920,
title = {Research challenges in nextgen service orchestration},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {20-38},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18303157},
author = {Luis M. Vaquero and Felix Cuadrado and Yehia Elkhatib and Jorge Bernal-Bernabe and Satish N. Srirama and Mohamed Faten Zhani},
keywords = {NVM, SDN, NFV, Orchestration, Large scale, Serverless, FaaS, Churn, Edge, Fog},
abstract = {Fog/edge computing, function as a service, and programmable infrastructures, like software-defined networking or network function virtualisation, are becoming ubiquitously used in modern Information Technology infrastructures. These technologies change the characteristics and capabilities of the underlying computational substrate where services run (e.g. higher volatility, scarcer computational power, or programmability). As a consequence, the nature of the services that can be run on them changes too (smaller codebases, more fragmented state, etc.). These changes bring new requirements for service orchestrators, which need to evolve so as to support new scenarios where a close interaction between service and infrastructure becomes essential to deliver a seamless user experience. Here, we present the challenges brought forward by this new breed of technologies and where current orchestration techniques stand with regards to the new challenges. We also present a set of promising technologies that can help tame this brave new world.}
}
@article{RASTELLINI2008879,
title = {Composite materials non-linear modelling for long fibre-reinforced laminates: Continuum basis, computational aspects and validations},
journal = {Computers & Structures},
volume = {86},
number = {9},
pages = {879-896},
year = {2008},
note = {Composites},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2007.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0045794907001642},
author = {Fernando Rastellini and Sergio Oller and Omar Salomón and Eugenio Oñate},
keywords = {FRP, Long fibre laminates, Composite failure, Non-linear modelling, FEM},
abstract = {An innovative computational methodology is proposed for modelling the material non-linear mechanical behaviour of FRP structures. To model a single unidirectional composite lamina, a serial–parallel (SP) continuum approach has been developed assuming that components behave as parallel materials in the fibres alignment direction and as serial materials in orthogonal directions. The model is based on the appropriate management of the constitutive models of the component materials, by making use of suitable ‘closure equations’ that characterize the composite micro-mechanics [Rastellini F. Modelización numérica de la no-linealidad constitutiva de laminados compuestos. PhD thesis. ETSECCPB, Politechnical University of Catalonia, Barcelona, March, 2006. [in Spanish]]. Classical lamination theory is combined with the SP model to describe multidirectional laminates. The methodology is validated through several numerical analyses, which are contrasted against benchmark tests and experimental data taken from the world-wide failure exercise [Hinton MJ, Soden PD. Predicting failure in composite laminates: The background to the exercise. Comp Sci Technol 1998; 58:1001–10].}
}
@article{SANZ20191,
title = {Sheet metal forming analysis using a large strain anisotropic multiplicative plasticity formulation, based on elastic correctors, which preserves the structure of the infinitesimal theory},
journal = {Finite Elements in Analysis and Design},
volume = {164},
pages = {1-17},
year = {2019},
issn = {0168-874X},
doi = {https://doi.org/10.1016/j.finel.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0168874X19300885},
author = {Miguel A. Sanz and K. Nguyen and Marcos Latorre and Manuel Rodríguez and Francisco J. Montáns},
keywords = {Large strains, Anisotropic plasticity, Sheet metal forming, Logarithmic strains, Multiplicative decomposition, Hill plasticity},
abstract = {Sheet metal forming is a very important process in industry to create a wide variety of goods. The analysis of local ductility and residual stresses is important both to assess the viability of the manufacturing process and the reliability of the resulting elements in service. An example is crash-worthiness, where remaining ductility and residual stresses govern the safety of the overall structure during the impact. A main ingredient of finite element simulations for sheet metal forming in industry is a robust continuum-based computational algorithm for large strain elastoplasticity which includes both elastic and plastic anisotropy, as well as mixed hardening. The theory should use exactly-integrable (conservative) elastic and hardening behaviors based on physically motivated proper state variables and, if possible, result in a simple integration algorithm. In this work we implement a novel large strain formulation for anisotropic hyperelasto-plasticity in a user subroutine of the commercial program ADINA to perform sheet metal forming simulations, testing the robustness and suitability of the model for industry, as well as its accuracy. The formulation is based on a new approach to the treatment of large strain kinematics, using logarithmic elastic corrector rates instead of plastic rates. Furthermore, kinematic hardening is formulated without an explicit backstress. We compare and discuss the results with those in the literature which use alternative frameworks.}
}
@article{GIL2013178,
title = {An enhanced Immersed Structural Potential Method for fluid–structure interaction},
journal = {Journal of Computational Physics},
volume = {250},
pages = {178-205},
year = {2013},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113003422},
author = {A.J. Gil and A. {Arranz Carreño} and J. Bonet and O. Hassan},
keywords = {Fluid–structure interaction, Immersed Boundary Method, Immersed Structural Potential Method},
abstract = {Within the group of immersed boundary methods employed for the numerical simulation of fluid–structure interaction problems, the Immersed Structural Potential Method (ISPM) was recently introduced (Gil et al., 2010) [1] in order to overcome some of the shortcomings of existing immersed methodologies. In the ISPM, an incompressible immersed solid is modelled as a deviatoric strain energy functional whose spatial gradient defines a fluid–structure interaction force field in the Navier–Stokes equations used to resolve the underlying incompressible Newtonian viscous fluid. In this paper, two enhancements of the methodology are presented. First, the introduction of a new family of spline-based kernel functions for the transfer of information between both physics. In contrast to classical IBM kernels, these new kernels are shown not to introduce spurious oscillations in the solution. Second, the use of tensorised Gaussian quadrature rules that allow for accurate and efficient numerical integration of the immersed structural potential. A series of numerical examples will be presented in order to demonstrate the capabilities of the enhanced methodology and to draw some key comparisons against other existing immersed methodologies in terms of accuracy, preservation of the incompressibility constraint and computational speed.}
}
@article{LONGO2020899,
title = {Apollon: Towards a citizen science methodology for urban environmental monitoring},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {899-912},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.06.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20303940},
author = {Antonella Longo and Marco Zappatore and Mario A. Bochicchio},
keywords = {Mobile crowd sensing, Citizen science, Smart cities, Internet of people},
abstract = {The collaborative power of ICT systems is a key enabler of social and technological advances providing multiple opportunities for public involvement in participatory activities, thanks to novel paradigms like citizen science and mobile crowd sensing. These paradigms, if applied according to specific methodologies, promise to increase the pervasive observation of urban environmental pollution either directly by human observers, or by means of crowd-sourcing data measurement tasks using sensors in smart phones or other mobile devices. We propose a platform, named Apollon, to enable scientists and others to take part in citizen science projects based on the exploitation of mobile devices. The platform has been implemented and validated in an educational context, in which students participate in urban environmental monitoring activities. In the paper, we describe the platform and the approach developed to produce successful experiments.}
}
@article{KIM2014358,
title = {High-accuracy calculations of sixteen collision integrals for Lennard-Jones (12–6) gases and their interpolation to parameterize neon, argon, and krypton},
journal = {Journal of Computational Physics},
volume = {273},
pages = {358-373},
year = {2014},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2014.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0021999114003647},
author = {Sun Ung Kim and Charles W. Monroe},
keywords = {Kinetic theory, Transport properties, Dilute noble gases, Collision integrals},
abstract = {The inverse problem of parameterizing intermolecular potentials given macroscopic transport and thermodynamic data is addressed. Procedures are developed to create arbitrary-precision algorithms for transport collision integrals, using the Lennard-Jones (12–6) potential as an example. Interpolation formulas are produced that compute these collision integrals to four-digit accuracy over the reduced-temperature range 0.3≤T⁎≤400, allowing very fast computation. Lennard-Jones parameters for neon, argon, and krypton are determined by simultaneously fitting the observed temperature dependences of their viscosities and second virial coefficients—one of the first times that a thermodynamic and a dynamic property have been used simultaneously for Lennard-Jones parameterization. In addition to matching viscosities and second virial coefficients within the bounds of experimental error, the determined Lennard-Jones parameters are also found to predict the thermal conductivity and self-diffusion coefficient accurately, supporting the value of the Lennard-Jones (12–6) potential for noble-gas transport-property correlation.}
}
@article{MARTINS2015334,
title = {The Programmable City},
journal = {Procedia Computer Science},
volume = {52},
pages = {334-341},
year = {2015},
note = {The 6th International Conference on Ambient Systems, Networks and Technologies (ANT-2015), the 5th International Conference on Sustainable Energy Information Technology (SEIT-2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.104},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915009047},
author = {Pedro M.N. Martins and Julie A. McCann},
keywords = {Ubiquitous Computing, Programming Languages, Macroprogramming, Internet of Things},
abstract = {The worldwide proliferation of mobile connected devices has brought about a revolution in the way we live, and will inevitably guide the way in which we design the cities of the future. However, designing city-wide systems poses a new set of challenges in terms of scale, manageability and citizen involvement. Solving these challenges is crucial to making sure that the vision of a programmable Internet of Things (IoT) becomes reality. In this article we will analyse these issues and present a novel programming approach to designing scalable systems for the Internet of Things, with an emphasis on smart city applications, that addresses these issues.}
}
@article{OLEKSIAK2017117,
title = {M2DC – Modular Microserver DataCentre with heterogeneous hardware},
journal = {Microprocessors and Microsystems},
volume = {52},
pages = {117-130},
year = {2017},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2017.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0141933116304574},
author = {Ariel Oleksiak and Michal Kierzynka and Wojciech Piatek and Giovanni Agosta and Alessandro Barenghi and Carlo Brandolese and William Fornaciari and Gerardo Pelosi and Mariano Cecowski and Robert Plestenjak and Justin Činkelj and Mario Porrmann and Jens Hagemeyer and René Griessl and Jan Lachmair and Meysam Peykanu and Lennart Tigges and Micha vor dem Berge and Wolfgang Christmann and Stefan Krupop and Alexandre Carbon and Loïc Cudennec and Thierry Goubier and Jean-Marc Philippe and Sven Rosinger and Daniel Schlitt and Christian Pieper and Chris Adeniyi-Jones and Javier Setoain and Luca Ceva and Udo Janssen},
keywords = {Microservers, Data centres, Heterogeneous architectures},
abstract = {The Modular Microserver DataCentre (M2DC) project investigates, develops and demonstrates a modular, highly-efficient, cost-optimized server architecture composed of heterogeneous microserver computing resources. The resulting server architecture will be able to be tailored to meet requirements from a wide range of application domains. M2DC is built on three main pillars: a flexible server architecture that can be easily customised, maintained and updated; advanced management strategies and system efficiency enhancements (SEE); well-defined interfaces to the surrounding software data centre ecosystem. In this paper, we focus in particular on the thermal management strategies and on the initial benchmarking of the Aarch64 ARM architecture.}
}
@article{VALLERO20151204,
title = {Cross-layer reliability evaluation, moving from the hardware architecture to the system level: A CLERECO EU project overview},
journal = {Microprocessors and Microsystems},
volume = {39},
number = {8},
pages = {1204-1214},
year = {2015},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2015.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0141933115000824},
author = {A. Vallero and S. Tselonis and N. Foutris and M. Kaliorakis and M. Kooli and A. Savino and G. Politano and A. Bosio and G. {Di Natale} and D. Gizopoulos and S. {Di Carlo}},
keywords = {Reliability evaluation, Fault injection, Statistical models},
abstract = {Advanced computing systems realized in forthcoming technologies hold the promise of a significant increase of computational capabilities. However, the same path that is leading technologies toward these remarkable achievements is also making electronic devices increasingly unreliable. Developing new methods to evaluate the reliability of these systems in an early design stage has the potential to save costs, produce optimized designs and have a positive impact on the product time-to-market. CLERECO European FP7 research project addresses early reliability evaluation with a cross-layer approach across different computing disciplines, across computing system layers and across computing market segments. The fundamental objective of the project is to investigate in depth a methodology to assess system reliability early in the design cycle of the future systems of the emerging computing continuum. This paper presents a general overview of the CLERECO project focusing on the main tools and models that are being developed that could be of interest for the research community and engineering practice.}
}
@article{FERNANDES202025,
title = {A 2D BEM formulation considering dissipative phenomena and a full coupled multiscale modelling},
journal = {Engineering Analysis with Boundary Elements},
volume = {119},
pages = {25-43},
year = {2020},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2020.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0955799720301739},
author = {G.R. Fernandes and G.B.S. Pontes and V.N. Oliveira},
keywords = {Multi-scale modelling, Homogenization, RVE, Boundary elements, 2D problem},
abstract = {A full coupled multi-scale modelling using the Boundary Element Method for analysing the 2D problem of stretched plates composed of heterogeneous materials, where dissipative phenomena can be considered, is presented. Both the macro-scale and the micro-scale are modelled by BEM formulations where the consistent tangent operator (CTO) is used to achieve the equilibrium of the iterative procedures. The equilibrium equation of the plate (macro-continuum) is written in terms of in-plane strains while the equilibrium problem of the microstructure, which is defined by the RVE (Representative Volume Element), is solved in terms of displacements fluctuations. In this kind of modelling, the mechanical behaviour of the material is governed by the homogenized response of the RVE, obtained after solving its equilibrium problem. As this kind of modelling is expensive computationally, it is important to investigate other numerical methods to have faster formulations, but which are still accurate. To validate the presented model, the numerical results are compared to the ones where the material microstructure (RVE) is modelled by the FEM.}
}
@article{ARDAGNA20201180,
title = {Special issue on Trusted Cloud-Edges Computations},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {1180-1183},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.060},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20324183},
author = {Claudio A. Ardagna and Mauro Conti and Chia-Mu Yu}
}
@article{GULTEKIN2016542,
title = {A phase-field approach to model fracture of arterial walls: Theory and finite element analysis},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {312},
pages = {542-566},
year = {2016},
note = {Phase Field Approaches to Fracture},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2016.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0045782516301645},
author = {Osman Gültekin and Hüsnü Dal and Gerhard A. Holzapfel},
keywords = {Biomechanics, Arterial wall, Fracture, Multi-field modeling, Crack phase-field, Anisotropic failure criterion},
abstract = {This study uses a recently developed phase-field approach to model fracture of arterial walls with an emphasis on aortic tissues. We start by deriving the regularized crack surface to overcome complexities inherent in sharp crack discontinuities, thereby relaxing the acute crack surface topology into a diffusive one. In fact, the regularized crack surface possesses the property of Gamma-Convergence, i.e. the sharp crack topology is restored with a vanishing length-scale parameter. Next, we deal with the continuous formulation of the variational principle for the multi-field problem manifested through the deformation map and the crack phase-field at finite strains which leads to the Euler–Lagrange equations of the coupled problem. In particular, the coupled balance equations derived render the evolution of the crack phase-field and the balance of linear momentum. As an important aspect of the continuum formulation we consider an invariant-based anisotropic constitutive model which is additively decomposed into an isotropic part for the ground matrix and an exponential anisotropic part for the two families of collagen fibers embedded in the ground matrix. In addition we propose a novel energy-based anisotropic failure criterion which regulates the evolution of the crack phase-field. The coupled problem is solved using a one-pass operator-splitting algorithm composed of a mechanical predictor step (solved for the frozen crack phase-field parameter) and a crack evolution step (solved for the frozen deformation map); a history field governed by the failure criterion is successively updated. Subsequently, a conventional Galerkin procedure leads to the weak forms of the governing differential equations for the physical problem. Accordingly, we provide the discrete residual vectors and a corresponding linearization yields the element matrices for the two sub-problems. Finally, we demonstrate the numerical performance of the crack phase-field model by simulating uniaxial extension and simple shear fracture tests performed on specimens obtained from a human aneurysmatic thoracic aorta. Model parameters are obtained by fitting the set of novel experimental data to the predicted model response; the finite element results agree favorably with the experimental findings.}
}
@article{BENITEZ201775,
title = {The mechanical behavior of skin: Structures and models for the finite element analysis},
journal = {Computers & Structures},
volume = {190},
pages = {75-107},
year = {2017},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2017.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045794916314183},
author = {José María Benítez and Francisco Javier Montáns},
keywords = {Biological tissues, Skin, Hyperelasticity, Viscoelasticity, Damage, Anisotropy},
abstract = {Soft biological tissues are complex materials with a large structural variety, with differences in behavior, but with some common characteristics. Skin is an archetypal soft tissue which presents many common characteristics to other soft biological tissues, like being a multilayer collagen-reinforced structure, with nonlinear behavior, anisotropy, viscosity, preconditioning effects, internal stresses and tissue growth and adaptation. Departing from a detailed description of the structures of the skin and the experimental evidence, we herein analyze the different modeling approaches in the literature for the distinct aspects of the skin behavior, with attention to the implementation in finite element codes.}
}
@article{YVONNET2011614,
title = {Finite element model of ionic nanowires with size-dependent mechanical properties determined by ab initio calculations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {200},
number = {5},
pages = {614-625},
year = {2011},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2010.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0045782510002665},
author = {J. Yvonnet and A. Mitrushchenkov and G. Chambaud and Q.-C. He},
keywords = {Surface stress, Size effects, Nanowires,  calculations},
abstract = {A finite element procedure for modeling crystalline nanostructures such as nanowires is proposed. The size effects exhibited by nanoobjects are captured by taking into account a surface energy, following the classical Gurtin–Murdoch surface elasticity theory. An appropriate variational form and a finite element approach are provided to model and solve relevant problems numerically. We describe a simplified technique based on projection operators for constructing the surface elements. The methodology is completed with a computational procedure based on ab initio calculations to extract elastic coefficients of general anisotropic surfaces. The FEM continuum model is validated by comparisons with complete ab initio models of nanowires with different diameters where size-dependent mechanical properties are observed. The FEM continuum model can then be used to model similar nanostructures in ranges of sizes or geometries where analytical or atomistic model is limited. The validated model is applied to the analysis of size effects in the bending of an AlN nanowire.}
}
@article{BOUSSEJRA20191,
title = {aflak: Visual programming environment enabling end-to-end provenance management for the analysis of astronomical datasets},
journal = {Visual Informatics},
volume = {3},
number = {1},
pages = {1-8},
year = {2019},
note = {Proceedings of PacificVAST 2019},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X19300154},
author = {Malik Olivier Boussejra and Rikuo Uchiki and Yuriko Takeshima and Kazuya Matsubayashi and Shunya Takekawa and Makoto Uemura and Issei Fujishiro},
keywords = {Astronomy, Provenance, Visual programming, Visualization},
abstract = {This paper describes an extendable graphical framework, aflak, which provides a visualization and provenance management environment for the analysis of multi-spectral astronomical datasets. Via its node editor interface, aflak allows the astronomer to compose transforms on input datasets queryable from public astronomical data repositories, then to export the results of the analysis as Flexible Image Transport System (FITS) files, in a manner such that the full provenance of the output data be preserved and reviewable, and that the exported file be usable by other common astronomical analysis software. FITS is the standard of data interchange in astronomy. By embedding aflak’s provenance data into FITS files, we both achieve interoperability with existing software and full reproducibility of the process by which astronomers make discoveries.}
}
@article{DAVYDOV2014260,
title = {Comparison of several staggered atomistic-to-continuum concurrent coupling strategies},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {277},
pages = {260-280},
year = {2014},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2014.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0045782514001388},
author = {D. Davydov and J-P. Pelteret and P. Steinmann},
keywords = {Concurrent multiscale methods, Atomic-to-continuum coupling methods, Molecular mechanics, Irving–Kirkwood–Noll procedure, Finite elements, Large strain},
abstract = {In this contribution several staggered schemes used to couple continuum mechanics (CM) and molecular mechanics (MM) are proposed. The described approaches are based on the atomistic-to-continuum correspondence, obtained by spatial averaging in the spirit of Irving and Kirkwood, and Noll. Similarities between this and other concurrent coupling schemes are indicated, thus providing a broad overview of different approaches in the field. The schemes considered here are decomposed into the surface-type (displacement or traction boundary conditions) and the volume-type. The latter restricts the continuum displacement field (and possibly its gradient) in some sense to the atomistic (discrete) displacements using Lagrange multipliers. A large-strain CM formulation incorporating Lagrange multipliers and a strategy to solve the resulting coupled linear system using an iterative solver is presented. Finally, the described coupling methods are numerically examined using two examples: uniaxial deformation and a plate with a hole relaxed under surface tension. Accuracy and convergence rates of each method are reported. It was found that the displacement (surface) coupling scheme and the Lagrangian (volume) scheme based on either discrete displacements or the H1 norm derived from continuous displacement fields provide the best performance.}
}
@article{SOUZA20181,
title = {Towards a proper service placement in combined Fog-to-Cloud (F2C) architectures},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {1-15},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.04.042},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17323051},
author = {V.B. Souza and X. Masip-Bruin and E. Marín-Tordera and S. Sànchez-López and J. Garcia and G.J. Ren and A. Jukan and A. {Juan Ferrer}},
keywords = {Service placement and execution, Resource allocation, Cloud & fog computing, Distributed systems, Quality of service},
abstract = {The Internet of Things (IoT) has empowered the development of a plethora of new services, fueled by the deployment of devices located at the edge, providing multiple capabilities in terms of connectivity as well as in data collection and processing. With the inception of the Fog Computing paradigm, aimed at diminishing the distance between edge-devices and the IT premises running IoT services, the perceived service latency and even the security risks can be reduced, while simultaneously optimizing the network usage. When put together, Fog and Cloud computing (recently coined as fog-to-cloud, F2C) can be used to maximize the advantages of future computer systems, with the whole greater than the sum of individual parts. However, the specifics associated with cloud and fog resource models require new strategies to manage the mapping of novel IoT services into the suitable resources. Despite few proposals for service offloading between fog and cloud systems are slowly gaining momentum in the research community, many issues in service placement, both when the service is ready to be executed admitted as well as when the service is offloaded from Cloud to Fog, and vice-versa, are new and largely unsolved. In this paper, we provide some insights into the relevant features about service placement in F2C scenarios, highlighting main challenges in current systems towards the deployment of the next-generation IoT services.}
}
@incollection{ANDREA2009875,
title = {Paradoxes, Self-Reference and Truth in the 20th Century},
editor = {Dov M. Gabbay and John Woods},
series = {Handbook of the History of Logic},
publisher = {North-Holland},
volume = {5},
pages = {875-1013},
year = {2009},
booktitle = {Logic from Russell to Church},
issn = {1874-5857},
doi = {https://doi.org/10.1016/S1874-5857(09)70020-2},
url = {https://www.sciencedirect.com/science/article/pii/S1874585709700202},
author = {Cantini Andrea}
}
@article{GIANNONE2020107402,
title = {Orchestrating heterogeneous MEC-based applications for connected vehicles},
journal = {Computer Networks},
volume = {180},
pages = {107402},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107402},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620301997},
author = {Francesco Giannone and Pantelis A. Frangoudis and Adlen Ksentini and Luca Valcarenghi},
keywords = {Multi-access edge computing, 4G/5G systems, Connected vehicles, Video streaming, Quality of experience},
abstract = {In the near future, 5G-connected vehicles will be able to exchange messages with each other, with the roadside infrastructure, with back-end servers, and with the Internet. They will do so with reduced latency, increased reliability, and large throughput under high mobility and user density. Different services with different requirements, such as Advanced Driving Assistance (ADA) and High Definition (HD) Video Streaming, will share the same physical resources, such as the wireless channel. Thus, a rigid orchestration among them becomes necessary to prioritize network resource allocation. This study proposes a Connected Vehicle Service Orchestrator (CVSO) which optimizes the Quality of Experience (QoE) of an in-vehicle infotainment video delivery service, while taking into account the required bandwidth for coexisting high priority services, such as ADA. To this end, we provide an Integer Linear Programming (ILP) formulation for the problem of optimally assigning a video streaming bitrate/quality per user to maximize the overall QoE, considering information from the video service and the Radio Access Network (RAN) levels. Our system takes advantage of recent developments in the area of Multi-access Edge Computing (MEC). In particular, we have implemented the CVSO and other service-level components and have deployed them on top of a standards-compliant MEC platform that we have developed. We exploit MEC-native services such as the Radio Network Information Service (RNIS) to offer the CVSO the necessary level of RAN awareness. Experiments on a full LTE network testbed featuring our MEC platform demonstrate the performance improvements our system brings in terms of video QoE. Furthermore, we propose and evaluate different algorithms to solve the ILP, which exhibit different trade-offs between solution quality and execution time.}
}
@article{LATYPOV2019180,
title = {Materials knowledge system for nonlinear composites},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {346},
pages = {180-196},
year = {2019},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2018.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0045782518305930},
author = {Marat I. Latypov and Laszlo S. Toth and Surya R. Kalidindi},
keywords = {Micromechanics, Homogenization theories, Multiscale modeling, Materials knowledge systems, Reduced-order models},
abstract = {In this contribution, we present a new Materials Knowledge System framework for microstructure-sensitive predictions of effective stress–strain responses in composite materials. The model is developed for composites with a wide range of combinations of strain hardening laws and topologies of the constituents. The theoretical foundation of the model is inspired by statistical continuum theories, leveraged by mean-field approximation of self-consistent models, and calibrated to data obtained from micromechanical finite element simulations. The model also relies on newly formulated data-driven linkages between micromechanical responses (phase-average strain rates and effective strength) and microstructure as well as strength contrast of the constituents. The paper describes in detail the theoretical development of the model, its implementation into an efficient computational plasticity framework, calibration of the linkages, and demonstration of the model predictions on two-phase composites with isotropic constituents exhibiting linear and power-law strain hardening laws. It is shown that the model reproduces finite element results reasonably well with significant savings of the computational cost.}
}
@article{ALLA2019455,
title = {Gamification in IoT Application: A Systematic Mapping Study},
journal = {Procedia Computer Science},
volume = {151},
pages = {455-462},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.062},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305241},
author = {Abdelhadi Alla and Khalid Nafil},
keywords = {Internet of Things, IoT, Gamification, Game, Systematic Mapping Study},
abstract = {We are entering an era where everything is almost connected. According to Gartner, "we expect to see 20 billion internet-connected things by 2020". This evolution has engendered a colossal amount of data that traditional computer applications could no longer handle. We penetrate the epoch of smart applications: intelligent, contextual and proactive. On the other side, a trend that makes the hype is Gamification. It is now being applied to several domains especially those that targets human engagement, performance and sustainability. In this paper, through a systematic mapping study, we will present a breadth-first review of search papers that integrate gamification in Internet of Things (IoT) applications. The main aim is to provide a historical and geographic classification of research that combines IoT with gamification, the most challenging areas, and the artifacts that come out.}
}
@article{CASTANAR2020113164,
title = {A stabilized mixed finite element approximation for incompressible finite strain solid dynamics using a total Lagrangian formulation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {368},
pages = {113164},
year = {2020},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2020.113164},
url = {https://www.sciencedirect.com/science/article/pii/S0045782520303492},
author = {Inocencio Castañar and Joan Baiges and Ramon Codina},
keywords = {Incompressible hyperelasticity, Solid dynamics, Mixed interpolations, Stabilization methods, Orthogonal subgrid scales},
abstract = {In this work a new methodology for both the nearly and fully incompressible transient finite strain solid mechanics problem is presented. To this end, the momentum equation is complemented with a constitutive law for the pressure which emerges from the deviatoric/volumetric decomposition of the strain energy function for any hyperelastic material model. The incompressible limit is attained automatically depending on the material bulk modulus. The system is stabilized by means of the Variational Multiscale-Orthogonal Subgrid Scale method based on the decomposition of the unknowns into resolvable and subgrid scales in order to prevent pressure fluctuations. Several numerical examples are presented to assess the robustness and applicability of the proposed formulation.}
}
@article{SCHWARTZENTRUBER2006402,
title = {A hybrid particle-continuum method applied to shock waves},
journal = {Journal of Computational Physics},
volume = {215},
number = {2},
pages = {402-416},
year = {2006},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2005.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S0021999105004936},
author = {T.E. Schwartzentruber and I.D. Boyd},
keywords = {Direct simulation Monte Carlo, Hybrid particle-continuum, Hypersonics, Non-equilibrium, Re-entry vehicles},
abstract = {A hybrid numerical scheme designed for hypersonic non-equilibrium flows is presented which solves the Navier–Stokes equations in regions of near-equilibrium and uses the direct simulation Monte Carlo method where the flow is in non-equilibrium. Detailed analysis of each stage of the hybrid cycle illustrates the difficulty in defining physically correct DSMC boundary conditions in regards to both macroscopic state and velocity distribution. However, results also show that DSMC boundary conditions have little effect on a previously initialized interior particle domain. A sub-relaxation technique capable of determining macroscopic, hydrodynamic properties in a DSMC simulation is used to determine low-scatter boundary conditions for the NS domain. Particle and continuum domains adapt during the hybrid simulation through application of a continuum breakdown parameter based on the gradient-length Knudsen number. The hybrid code reproduces experimental results and full DSMC simulations in half the time for a large range of 1D shock waves in argon and diatomic nitrogen gas.}
}
@article{PAPADAKISVLACHOPAPADOPOULOS2019498,
title = {Collaborative SLA and reputation-based trust management in cloud federations},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {498-512},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18329248},
author = {Konstantinos Papadakis-Vlachopapadopoulos and Román Sosa González and Ioannis Dimolitsas and Dimitrios Dechouniotis and Ana Juan Ferrer and Symeon Papavassiliou},
keywords = {Cloud applications, SLA, Trust management, Federation},
abstract = {Industry and academia shift from the single cloud provider paradigm to cloud federations and alternative models, which orchestrate heterogeneous resources, such as Mobile Edge Computing and Fog Computing. In such complex environments, incorrect selection of deployment platform can lead to underwhelming application performance. This dictates the necessity of Service Level Agreements (SLA) and trust management services in order to enforce performance guarantees and enable customers to simultaneously evaluate their application’s performance and give performance indicators for future provider selection. In this paper, we propose a collaborative SLA and Reputation-based Trust Management (RTM) solution for federated cloud environment. The SLA service defines clearly the performance metrics and measures the actual performance of the deployed cloud applications. Based on the SLA, the RTM service of the collaborative solution leverages several technical and user’s experience metrics to compute the reliability of the cloud providers and the credibility of the customers. A proof of concept of the collaborative solution in a realistic federated environment is provided and validated. The corresponding experimental results demonstrate that it objectively computes the cloud providers’ reputation values under various scenarios.}
}
@article{LIU2016305,
title = {A unified gas-kinetic scheme for continuum and rarefied flows IV: Full Boltzmann and model equations},
journal = {Journal of Computational Physics},
volume = {314},
pages = {305-340},
year = {2016},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2016.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0021999116001662},
author = {Chang Liu and Kun Xu and Quanhua Sun and Qingdong Cai},
keywords = {Direct modeling, Unified gas kinetic scheme, Boltzmann equation, Kinetic collision model, Asymptotic preserving},
abstract = {Fluid dynamic equations are valid in their respective modeling scales, such as the particle mean free path scale of the Boltzmann equation and the hydrodynamic scale of the Navier–Stokes (NS) equations. With a variation of the modeling scales, theoretically there should have a continuous spectrum of fluid dynamic equations. Even though the Boltzmann equation is claimed to be valid in all scales, many Boltzmann solvers, including direct simulation Monte Carlo method, require the cell resolution to the order of particle mean free path scale. Therefore, they are still single scale methods. In order to study multiscale flow evolution efficiently, the dynamics in the computational fluid has to be changed with the scales. A direct modeling of flow physics with a changeable scale may become an appropriate approach. The unified gas-kinetic scheme (UGKS) is a direct modeling method in the mesh size scale, and its underlying flow physics depends on the resolution of the cell size relative to the particle mean free path. The cell size of UGKS is not limited by the particle mean free path. With the variation of the ratio between the numerical cell size and local particle mean free path, the UGKS recovers the flow dynamics from the particle transport and collision in the kinetic scale to the wave propagation in the hydrodynamic scale. The previous UGKS is mostly constructed from the evolution solution of kinetic model equations. Even though the UGKS is very accurate and effective in the low transition and continuum flow regimes with the time step being much larger than the particle mean free time, it still has space to develop more accurate flow solver in the region, where the time step is comparable with the local particle mean free time. In such a scale, there is dynamic difference from the full Boltzmann collision term and the model equations. This work is about the further development of the UGKS with the implementation of the full Boltzmann collision term in the region where it is needed. The central ingredient of the UGKS is the coupled treatment of particle transport and collision in the flux evaluation across a cell interface, where a continuous flow dynamics from kinetic to hydrodynamic scales is modeled. The newly developed UGKS has the asymptotic preserving (AP) property of recovering the NS solutions in the continuum flow regime, and the full Boltzmann solution in the rarefied regime. In the mostly unexplored transition regime, the UGKS itself provides a valuable tool for the non-equilibrium flow study. The mathematical properties of the scheme, such as stability, accuracy, and the asymptotic preserving, will be analyzed in this paper as well.}
}
@article{BRYCE2011848,
title = {State agnostic planning graphs: deterministic, non-deterministic, and probabilistic planning},
journal = {Artificial Intelligence},
volume = {175},
number = {3},
pages = {848-889},
year = {2011},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210002195},
author = {Daniel Bryce and William Cushing and Subbarao Kambhampati},
keywords = {Planning, Heuristics},
abstract = {Planning graphs have been shown to be a rich source of heuristic information for many kinds of planners. In many cases, planners must compute a planning graph for each element of a set of states, and the naive technique enumerates the graphs individually. This is equivalent to solving a multiple-source shortest path problem by iterating a single-source algorithm over each source. We introduce a data-structure, the state agnostic planning graph, that directly solves the multiple-source problem for the relaxation introduced by planning graphs. The technique can also be characterized as exploiting the overlap present in sets of planning graphs. For the purpose of exposition, we first present the technique in deterministic (classical) planning to capture a set of planning graphs used in forward chaining search. A more prominent application of this technique is in conformant and conditional planning (i.e., search in belief state space), where each search node utilizes a set of planning graphs; an optimization to exploit state overlap between belief states collapses the set of sets of planning graphs to a single set. We describe another extension in conformant probabilistic planning that reuses planning graph samples of probabilistic action outcomes across search nodes to otherwise curb the inherent prediction cost associated with handling probabilistic actions. Finally, we show how to extract a state agnostic relaxed plan that implicitly solves the relaxed planning problem in each of the planning graphs represented by the state agnostic planning graph and reduces each heuristic evaluation to counting the relevant actions in the state agnostic relaxed plan. Our experimental evaluation (using many existing International Planning Competition problems from classical and non-deterministic conformant tracks) quantifies each of these performance boosts, and demonstrates that heuristic belief state space progression planning using our technique is competitive with the state of the art.}
}
@article{JENKINS2015427,
title = {Coupling extended magnetohydrodynamic fluid codes with radiofrequency ray tracing codes for fusion modeling},
journal = {Journal of Computational Physics},
volume = {297},
pages = {427-441},
year = {2015},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2015.05.035},
url = {https://www.sciencedirect.com/science/article/pii/S0021999115003691},
author = {Thomas G. Jenkins and Eric D. Held},
keywords = {Computational geometry, Ray tracing, Finite element methods, Pseudospectral methods, Magnetohydrodynamics, Radiofrequency waves},
abstract = {Neoclassical tearing modes are macroscopic (L∼1 m) instabilities in magnetic fusion experiments; if unchecked, these modes degrade plasma performance and may catastrophically destroy plasma confinement by inducing a disruption. Fortunately, the use of properly tuned and directed radiofrequency waves (λ∼1 mm) can eliminate these modes. Numerical modeling of this difficult multiscale problem requires the integration of separate mathematical models for each length and time scale (Jenkins and Kruger, 2012 [21]); the extended MHD model captures macroscopic plasma evolution while the RF model tracks the flow and deposition of injected RF power through the evolving plasma profiles. The scale separation enables use of the eikonal (ray-tracing) approximation to model the RF wave propagation. In this work we demonstrate a technique, based on methods of computational geometry, for mapping the ensuing RF data (associated with discrete ray trajectories) onto the finite-element/pseudospectral grid that is used to model the extended MHD physics. In the new representation, the RF data can then be used to construct source terms in the equations of the extended MHD model, enabling quantitative modeling of RF-induced tearing mode stabilization. Though our specific implementation uses the NIMROD extended MHD (Sovinec et al., 2004 [22]) and GENRAY RF (Smirnov et al., 1994 [23]) codes, the approach presented can be applied more generally to any code coupling requiring the mapping of ray tracing data onto Eulerian grids.}
}
@article{MARKOVIC20065077,
title = {Complementary energy based FE modelling of coupled elasto-plastic and damage behavior for continuum microstructure computations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {195},
number = {37},
pages = {5077-5093},
year = {2006},
note = {John H. Argyris Memorial Issue. Part I},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2005.05.058},
url = {https://www.sciencedirect.com/science/article/pii/S0045782505005384},
author = {Damijan Markovic and Adnan Ibrahimbegovic},
keywords = {Continuume microstructure computations, Electro-plastic behavior, Damage behavior},
abstract = {We introduce a coupled elasto-plastic and damage constitutive model with the stress field as the independent variable. The damage part is described in an analogous way to the classical plasticity models, which enables an efficient use of the complementary variational formulation. We work out the complete numerical implementation and show its efficiency with respect to the more classical displacement based formulation. We demonstrate the particular advantage of using the proposed approach within the framework of structured representation of a heterogeneous material.}
}
@article{RAUSCH2021259,
title = {Optimized container scheduling for data-intensive serverless edge computing},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {259-271},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2030399X},
author = {Thomas Rausch and Alexander Rashed and Schahram Dustdar},
keywords = {Edge computing, Serverless, Container scheduling, Machine learning},
abstract = {Operating data-intensive applications on edge systems is challenging, due to the extreme workload and device heterogeneity, as well as the geographic dispersion of compute and storage infrastructure. Serverless computing has emerged as a compelling model to manage the complexity of such systems, by decoupling the underlying infrastructure and scaling mechanisms from applications. Although serverless platforms have reached a high level of maturity, we have found several limiting factors that inhibit their use in an edge setting. This paper presents a container scheduling system that enables such platforms to make efficient use of edge infrastructures. Our scheduler makes heuristic trade-offs between data and computation movement, and considers workload-specific compute requirements such as GPU acceleration. Furthermore, we present a method to automatically fine-tune the weights of scheduling constraints to optimize high-level operational objectives such as minimizing task execution time, uplink usage, or cloud execution cost. We implement a prototype that targets the container orchestration system Kubernetes, and deploy it on an edge testbed we have built. We evaluate our system with trace-driven simulations in different infrastructure scenarios, using traces generated from running representative workloads on our testbed. Our results show that (a) our scheduler significantly improves the quality of task placement compared to the state-of-the-art scheduler of Kubernetes, and (b) our method for fine-tuning scheduling parameters helps significantly in meeting operational goals.}
}
@article{SHARMA2018120,
title = {Resource-based mobility management for video users in 5G using catalytic computing},
journal = {Computer Communications},
volume = {118},
pages = {120-139},
year = {2018},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0140366417302396},
author = {Vishal Sharma and Ilsun You and Ravinder Kumar},
keywords = {5G, DMM, Mobility management, Catalytic computing, Video users},
abstract = {The upcoming 5G era emphasizes on a dramatic increase in the transmission rate of smartphone traffic. With more users operating at high rates, the type of data shared over the network is going to be complex and a majority of it will include video traffic. Such complex structure of traffic and heavy load over the components of the network are difficult to control. Further, the mobility of users adds up to this issue and makes it difficult to manage and operate the network without any breakdown. Thus, it important to control traffic as well as manage the mobility of users to provide efficient communication, which can support video traffic at high delivery rates. This paper proposes a novel resource-based mobility management approach for 5G networks comprising video users. A novel resource sharing paradigm, termed as “Catalytic Computing”, provides efficient management of user mobility as well as network resources. The proposed approach relies on Homogeneous discrete Markov model for user mobility patterns and a novel n-step algorithm for congestion prediction and selection of optimal routes between the serving terminals. An activation energy based handover mechanism is also presented in this paper, which reduces the handover latency in comparison with the existing solutions. The evaluation presented in the paper suggests that the proposed approach provides a minimum of 5.9 ms, maximum of 9.1 ms and an average of 6.5 ms latency during handoffs.}
}
@article{JASON20067077,
title = {An elastic plastic damage formulation for concrete: Application to elementary tests and comparison with an isotropic damage model},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {195},
number = {52},
pages = {7077-7092},
year = {2006},
note = {Computational Modelling of Concrete},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2005.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S0045782505003919},
author = {Ludovic Jason and Antonio Huerta and Gilles Pijaudier-Cabot and Shahrokh Ghavamian},
keywords = {Concrete, Damage, Plasticity},
abstract = {Pure elastic damage models or pure elastic plastic constitutive laws are not totally satisfactory to describe the behaviour of concrete. They indeed fail to reproduce the unloading slopes during cyclic loading which define experimentally the value of the damage in the material. When coupled effects are considered, in particular in hydro-mechanical problems, the capability of numerical models to reproduce the unloading behaviour is essential, because an accurate value of the damage, which controls the material permeability, is needed. In the context of very large size calculations that are needed for 3D massive structures heavily reinforced and pre-stressed (such as containment vessels), constitutive relations ought also to be as simple as possible. Here an elastic plastic damage formulation is proposed to circumvent the disadvantages of pure plastic and pure damage approaches. It is based on an isotropic damage model combined with a hardening yield plastic surface in order to reach a compromise as far as simplicity is concerned. Three elementary tests are first considered for validation. A tension test, a cyclic compression test and triaxial tests illustrate the improvements achieved by the coupled law compared to a simple damage model (plastic strains, change of volumetric behaviour, decrease in the elastic slope under hydrostatic pressures). Finally, one structural application is also considered: a concrete column wrapped in a steel tube.}
}
@article{RASTIELLO2019431,
title = {Discontinuity-scale path-following methods for the embedded discontinuity finite element modeling of failure in solids},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {349},
pages = {431-457},
year = {2019},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2019.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S004578251930101X},
author = {Giuseppe Rastiello and Francesco Riccardi and Benjamin Richard},
keywords = {Path-following methods, Strong discontinuities, Embedded finite element method, Dissipative path-following constraints, Operator-splitting method},
abstract = {The initiation and propagation of cracks in solids often leads to unstable structural responses characterized by snap-backs. Path-following procedures allow finding a solution to the algebraic system of equations resulting from the numerical formulation of the considered problem. Accordingly, the boundary value problem is supplemented by a novel global unknown, namely, the loading factor, which should comply with a dedicated equation, the so-called path-following constraint equation. In this contribution, path-following methods are discussed within the framework of the Embedded Finite Element Method (E-FEM). Thanks to the enhanced kinematic description provided by the E-FEM, we show that it is possible to formulate constraint equations where the prescribed quantities are directly related to the dissipative process occurring at the strong discontinuity level. After introducing the augmented E-FEM formulation, three discontinuity-scale path-following constraints and their numerical implementation (using an operator-splitting method) are described. Simple quasi-static strain localization problems characterized by unstable structural responses exhibiting multiple snap-backs are numerically simulated. A comparison with several well-known constraint equations (commonly used in non-linear finite element computations) is finally established. This allows for illustrating the main features of the proposed methods as well as their efficiency in controlling highly unstable embedded discontinuity finite element simulations.}
}
@article{ZOHDI2012206,
title = {Modeling and simulation of electrification delivery in functionalized textiles in electromagnetic fields},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {245-246},
pages = {206-216},
year = {2012},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2012.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S004578251200223X},
author = {T.I. Zohdi},
keywords = {Textiles, Electromagnetism, Multiphysics, Dynamics},
abstract = {This work investigates the deformation of electrified textiles in the presence of an externally supplied magnetic field (Bext). The electrification is delivered by running current (J) through the fibers from an external power source. Of primary interest is to ascertain the resulting electromagnetic forces imposed on the fabric, and the subsequent deformation, due to the terms J×Bext and PE, where P is the charge density, E is the electric field and the current given by J=σ(E+v×Bext), where σ is the fabric conductivity, and v is the fabric velocity. As the fabric deforms, the current changes direction and magnitude, due to the fact that it flows through the fabric. The charge density is dictated by Gauss’ law, ∇·D=P, where D=ϵE,ϵ is the electrical permittivity and D is the electric field flux. In order to simulate such a system, one must solve a set of coupled equations governing the charge distribution, current flow and system dynamics. The deformation of the fabric, as well as the charge distribution and current flow, are dictated by solving the coupled system of differential equations for the motion of lumped masses, which are coupled through the fiber-segments under the action of electromagnetically-induced forces acting on a reduced order network model. In the work, reduced order models are developed for (a) Gauss’ law (∇·D=P), (b) the conservation of current/charge, ∇·J+∂P∂t=0, and (c) the system dynamics, ∇·T+f=ρdvdt, where T is the Cauchy stress and f represents the induced body forces, which are proportional to PE+J×Bext. A temporally-adaptive, recursive, staggering scheme is developed to solve this strongly coupled system of equations. We also consider the effects of progressive fiber damage/rupture during the deformation process, which leads to changes (reduction) in the electrical conductivity and permittivity throughout the network. Numerical examples are given, as well as extensions to thermal effects, which are induced by the current-induced Joule-heating.}
}
@article{SIMAO2019674,
title = {GC-Wise: A Self-adaptive approach for memory-performance efficiency in Java VMs},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {674-688},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18304898},
author = {J. Simão and S. Esteves and André Pires and L. Veiga},
keywords = {Memory management, Machine learning, Java virtual machine},
abstract = {High-level language runtimes are ubiquitous in every cloud deployment. From the geo-distributed heavy resources cloud provider to the new Fog and Edge deployment paradigms, all rely on these runtimes for portability, isolation and resource management. Across these clouds, an efficient resource management of several managed runtimes involves limiting the heap size of some VMs so that extra memory can be assigned to higher priority workloads. The challenges in this approach rely on the potential scale of systems and the need to make decisions in an application-driven way, because performance degradation can be severe, and therefore it should be minimized. Also, each tenant tends to repeat the execution of applications with similar memory-usage patterns, giving opportunity to reuse parameters known to work well for a given workload. This paper presents GC-Wise, a system to determine, at run-time, the best values for critical heap management parameters of the OpenJDK JVM, aiming to maximize memory-performance efficiency. GC-Wise comprises two main phases: 1) a training phase where it collects, with different heap resizing policies, representative execution metrics during the lifespan of a workload; and 2) an execution phase where an oracle matches the execution parameters of new workloads against those of already seen workloads, and enforces the best heap resizing policy. Distinctly from other works, the oracle can also decide upon unknown workloads. Using representative applications and different hardware setting (a resourceful server and a fog-like device), we show that our approach can lead to significant memory savings with low-impact on the throughput of applications. Furthermore, we show that we can predict with high accuracy the best heap resizing configuration in a relatively short period of time.}
}
@article{FERRER2016140,
title = {Inter-cloud Research: Vision for 2020},
journal = {Procedia Computer Science},
volume = {97},
pages = {140-143},
year = {2016},
note = {2nd International Conference on Cloud Forward: From Distributed to Complete Computing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.08.292},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916321081},
author = {Ana Juan Ferrer},
keywords = {Inter-Cloud, Multi-Cloud, Hybrid Cloud},
abstract = {Inter-cloud Challenges, Expectations and Issues Cluster objective is to enable collaboration among European Research projects addressing topics of multi-cloud and inter-cloud. Today these projects analyze the question from diverse perspectives and focusing on specific parts of the problem. This position paper provides the work done in collaboration by all these projects to define research areas and challenges for 2020. It identifies a Cluster's vision of Inter-Cloud topics development by 2020, as well as, research areas in order to realize the provided vision. An extended version of this work is available on the Inter-Cloud Cluster position paper 1,2.}
}
@article{HAN20072029,
title = {A two-scale deformation model for polycrystalline solids using a strongly-coupled finite element methodology},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {196},
number = {13},
pages = {2029-2043},
year = {2007},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2006.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0045782506003604},
author = {Tong-Seok Han and Paul R. Dawson},
keywords = {Elasticity, Plasticity, Polycrystals, Multi-scale, Finite element method},
abstract = {A two-scale, finite element framework for analysis of polycrystalline solids at the continuum and crystal scales is demonstrated. The framework is strongly coupled in the sense that the response at the continuum scale directly bears on the response at the crystal scale and vice versa. Data needed at one scale from computations at the other scale are generated on-the-fly. Two important issues are addressed: the projection of continuum scale motion onto crystal scale aggregates and the averaging crystal scale stresses for use at the continuum scale. The framework is implemented in a scalable parallel computing environment and applied to two examples with different geometries and loading modes. It is shown that combining formulations for the crystal and continuum scales can provide more detailed and accurate results in comparison to a single-scale finite element approach that invokes a simpler scale-linking methodology.}
}
@incollection{TOBIS2006171,
title = { - PyNSol: A Framework for Interactive Development of High performance Continuum Models},
editor = {Anil Deane and Akin Ecer and James McDonough and Nobuyuki Satofuka and Gunther Brenner and David R. Emerson and Jacques Periaux and Damien Tromeur-Dervout},
booktitle = {Parallel Computational Fluid Dynamics 2005},
publisher = {Elsevier},
address = {Amsterdam},
pages = {171-178},
year = {2006},
isbn = {978-0-444-52206-1},
doi = {https://doi.org/10.1016/B978-044452206-1/50020-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780444522061500203},
author = {Michael Tobis},
abstract = {Publisher Summary
The chapter discusses objectives of a python numerical solver (PyNSol). PyNSol is a framework architecture for the incremental development of a high-productivity environment for developing high-performance codes. Its target domain is computational fluid dynamics (CFD) problems with simple geometries and schemes of modest order but with multiple physical processes, multiple time scales, and complex forcing. From the end-user's point of view, PyNSol is a competitor to Parallel Matlab. Numerous efforts to provide distributed Matlab are underway. These tools provide a scientist with an interactive development environment while offloading the complexities of targeting the parallel platform to an infrastructure layer that is not explicitly presented to the user. PyNSol intends to provide higher performance as well as comparable ease of use by approaching a less general set of problems. The PyNSol end user, a physical scientist, investigates physical problems of various sorts and is presented with extensions to Python to facilitate this process.}
}
@incollection{MAHMOUD2019601,
title = {Thermal conductivity prediction of molten salt-based nanofluids for energy storage applications},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {601-606},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50101-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343501016},
author = {B.H. Mahmoud and L.F. Mortimer and M. Fairweather and H.P. Rice and J. Peakall and D. Harbottle},
keywords = {Nanofluids, oscillatory structural force, thermal conductivity, energy storage},
abstract = {Molten salt-based nanofluids (nano-salts) form an important class of thermal fluids that can act as both heat transfer and thermal energy storage media for high temperature applications, including solar thermal systems. Among these, the class of binary nitrate salts (60:40 wt. % NaNO3:KNO3) and their mixtures with metal oxide nanoparticles are the most prominent. This study evaluates the stability of nano-salts using a computational technique based on Lagrangian particle tracking, with the model considering the motion of solid nanoparticles suspended in a molten fluid. The technique enables various multiscale forces, with different characteristics, to be established. The system considered consists of 25-71 nm Al2O3 ceramic nanoparticles at volume fractions ranging from 1.0 to 5.0% suspended in fluids of different density ratios, with homogeneous temperature distributions from 250-600 °C. The simulation results demonstrate the effectiveness of the technique, with predictions elucidating the role of oscillatory structural, Brownian motion and particle collision forces, and their influence on the enhancement of thermal conductivity. The liquid structuring of salt melts around the embedded nanoparticles is found to play a key role in the nano-salts’ thermal behaviour, with predictions in agreement with previous theoretical and experimental studies. The outcome of this research forms the basis for the potential use of nano-salts in solar thermal systems.}
}
@article{RISTOV2021368,
title = {AFCL: An Abstract Function Choreography Language for serverless workflow specification},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {368-382},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20302648},
author = {Sasko Ristov and Stefan Pedratscher and Thomas Fahringer},
keywords = {AWS Step Functions, Cost, FaaS, IBM Composer, Performance},
abstract = {Serverless workflow applications or function choreographies (FCs), which connect serverless functions by data- and control-flow, have gained considerable momentum recently to create more sophisticated applications as part of Function-as-a-Service (FaaS) platforms. Initial experimental analysis of the current support for FCs uncovered important weaknesses, including provider lock-in, and limited support for important data-flow and control-flow constructs. To overcome some of these weaknesses, we introduce the Abstract Function Choreography Language (AFCL) for describing FCs at a high-level of abstraction, which abstracts the function implementations from the developer. AFCL is a YAML-based language that supports a rich set of constructs to express advanced control-flow (e.g. parallelFor loops, parallel sections, dynamic loop iterations counts) and data-flow (e.g multiple input and output parameters of functions, DAG-based data-flow). We introduce data collections which can be distributed to loop iterations and parallel sections that may substantially reduce the delays for function invocations due to reduced data transfers between functions. We also support asynchronous functions to avoid delays due to blocking functions. AFCL supports properties (e.g. expected size of function input data) and constraints (e.g. minimize execution time) for the user to optionally provide hints about the behavior of functions and FCs and to control the optimization by the underlying execution environment. We implemented a prototype AFCL environment that supports AFCL as input language with multiple backends (AWS Lambda and IBM Cloud Functions) thus avoiding provider lock-in which is a common problem in serverless computing. We created two realistic FCs from two different domains and encoded them with AWS Step Functions, IBM Composer and AFCL. Experimental results demonstrate that our current implementation of the AFCL environment substantially outperforms AWS Step Functions and IBM Composer in terms of development effort, economic costs, and makespan.}
}
@article{LI20101000,
title = {A seepage computational model of face slab cracks based on equi-width joint constant flow},
journal = {Advances in Engineering Software},
volume = {41},
number = {7},
pages = {1000-1004},
year = {2010},
note = {Advances in Structural Optimization},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2010.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0965997810000499},
author = {Shou-yi Li and Yan-long Li and Zheng Si and Xiao-fei Zhang},
keywords = {Concrete face rock-fill dam, Face slab crack, Seepage, Computation model},
abstract = {Based on the law of equi-width joint steady flow, a seepage computational model was established. This model permits use of finite element method in seepage simulation and analysis. In addition, rock-fill dam seepage characteristics under an unusual condition with massive face slab cracks was obtained via a computational example.}
}
@incollection{2017473,
title = {Subject Index},
editor = {Houbing Song and Danda B. Rawat and Sabina Jeschke and Christian Brecher},
booktitle = {Cyber-Physical Systems},
publisher = {Academic Press},
address = {Boston},
pages = {473-484},
year = {2017},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-803801-7},
doi = {https://doi.org/10.1016/B978-0-12-803801-7.09986-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128038017099867}
}
@article{ELHALABI2013183,
title = {FE2 multiscale in linear elasticity based on parametrized microscale models using proper generalized decomposition},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {257},
pages = {183-202},
year = {2013},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2013.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0045782513000224},
author = {F. {El Halabi} and D. González and A. Chico and M. Doblaré},
keywords = {Proper generalized decomposition (PGD), Multiscale, FE, Variational multiscale},
abstract = {In recent years, a tremendous growth of activity in multiscale modelling has been produced to get the mechanical response of highly heterogenous materials, where the complexity of solving numerically all microscale details is not feasible. In this paper we present a multiscale framework based on the decomposition of the displacement field into coarse (macro) and fine (micro) scales, earlier proposed in the Variational Multiscale approach or the hp-d method. The novelty of this work lies in solving the microscale step, where a multidimensional parametrized model of a generic RVE is solved, by means of the multidimensional model reduction technique, named as proper generalized decomposition (PGD). As result of this previous and off-line step, the displacement field over the RVE is obtained by simple algebraic operations for any combination of parameters (boundary condition, material properties, loads, etc.), allowing a significant reduction in computational cost when solving the macro scale problem. For problems where the detailed structure of the localized displacement, strain and stress fields are of interest, recovery of the fine scale components can be performed immediately. The basic concepts and several 1-D and 2-D linear elastic problems are presented to demonstrate the robustness of the formulation and computer time saving.}
}
@article{YU201317,
title = {Soil–pipe interaction due to tunnelling: Assessment of Winkler modulus for underground pipelines},
journal = {Computers and Geotechnics},
volume = {50},
pages = {17-28},
year = {2013},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2012.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X12002297},
author = {Jian Yu and Chenrong Zhang and Maosong Huang},
keywords = {Winkler subgrade modulus, Tunnel excavation, Underground pipeline, Soil movement},
abstract = {One of the key problems in the implementation of Winkler models to analyze tunnelling effects on existing pipelines lies in the assessment of subgrade modulus under external soil displacement. In this paper, an expression of the Winkler subgrade modulus for a pipeline buried at arbitrary depth and subjected to free soil displacement with arbitrary curve shape is given. Using superposition principle and the Fourier integral, the subgrade modulus of an infinite beam resting on the surface of an elastic half space and buried infinitely are obtained respectively. Then the influence of embedment depth is estimated based on Mindlin and Kelvin solution. The validity of the proposed subgrade modulus is verified by comparison with the results from an elastic continuum solution and two centrifuge model tests for the responses of buried pipeline due to nearby tunnelling. Thereafter, parametric studies are shown to assess the accuracy of the proposed subgrade modulus by comparing with an elastic continuum solution in homogeneous and non-homogeneous soil stratum and the amount of error is estimated.}
}
@article{PAYRE20071933,
title = {Influence graphs and the generalized finite difference method},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {196},
number = {13},
pages = {1933-1945},
year = {2007},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2006.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045782506003574},
author = {G.M.J. Payre},
keywords = {Discrete mechanics, Generalized finite differences, Meshless methods},
abstract = {This paper introduces directed graphs on which the evolution of a physical quantity depends only on local neighborhoods. These graphs are then used to model transfer phenomena occurring under a convective mode or a diffusive mode. The conditions under which the state associated to such graphs approaches the solution of a diffusion-convection partial differential model are established. An algorithm permitting to determine consistent neighborhoods is described and recognized as a generalization of the finite difference method. Examples of application are presented in order to illustrate the practical applicability of these concepts.}
}
@article{BLAIS2016201,
title = {Development of an unresolved CFD–DEM model for the flow of viscous suspensions and its application to solid–liquid mixing},
journal = {Journal of Computational Physics},
volume = {318},
pages = {201-221},
year = {2016},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2016.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0021999116301358},
author = {Bruno Blais and Manon Lassaigne and Christoph Goniva and Louis Fradette and François Bertrand},
keywords = {Solid–liquid mixing, Multiphase flows, Computational fluid dynamics, Discrete element method, CFD–DEM},
abstract = {Although viscous solid–liquid mixing plays a key role in the industry, the vast majority of the literature on the mixing of suspensions is centered around the turbulent regime of operation. However, the laminar and transitional regimes face considerable challenges. In particular, it is important to know the minimum impeller speed (Njs) that guarantees the suspension of all particles. In addition, local information on the flow patterns is necessary to evaluate the quality of mixing and identify the presence of dead zones. Multiphase computational fluid dynamics (CFD) is a powerful tool that can be used to gain insight into local and macroscopic properties of mixing processes. Among the variety of numerical models available in the literature, which are reviewed in this work, unresolved CFD–DEM, which combines CFD for the fluid phase with the discrete element method (DEM) for the solid particles, is an interesting approach due to its accurate prediction of the granular dynamics and its capability to simulate large amounts of particles. In this work, the unresolved CFD–DEM method is extended to viscous solid–liquid flows. Different solid–liquid momentum coupling strategies, along with their stability criteria, are investigated and their accuracies are compared. Furthermore, it is shown that an additional sub-grid viscosity model is necessary to ensure the correct rheology of the suspensions. The proposed model is used to study solid–liquid mixing in a stirred tank equipped with a pitched blade turbine. It is validated qualitatively by comparing the particle distribution against experimental observations, and quantitatively by compairing the fraction of suspended solids with results obtained via the pressure gauge technique.}
}
@article{TALEBI201582,
title = {Concurrent multiscale modeling of three dimensional crack and dislocation propagation},
journal = {Advances in Engineering Software},
volume = {80},
pages = {82-92},
year = {2015},
note = {Civil-Comp},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2014.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0965997814001653},
author = {Hossein Talebi and Mohammad Silani and Timon Rabczuk},
keywords = {Extended finite element method (XFEM), Molecular dynamics, Bridging domain method, Multiscale methods, LAMMPS, Fracture},
abstract = {In this manuscript a concurrent coupling scheme is presented to model three dimensional cracks and dislocations at the atomistic level. The scheme couples molecular dynamics to extended finite element method (XFEM) via the Bridging Domain Method (BDM). This method is based on linear weighting of the strain energy over a region (the bridging domain) which conserves the energy in the entire system. To compute the material behavior in the continuum scale, the Cauchy–Born method is used. Many improvements have been made in the implementation to make the method work for the general case of materials and presence of multi-million degrees of freedom. To show the applicability and productivity of the proposed method, two three dimensional crack examples were modeled. The results show that the method and the corresponding implementation are capable of handling dislocation and crack propagation in the three dimensional space.}
}
@article{KOMPIS201247,
title = {Parallel computational models for composites reinforced by CNT-fibres},
journal = {Engineering Analysis with Boundary Elements},
volume = {36},
number = {1},
pages = {47-52},
year = {2012},
note = {Special Issue on Trefftz Method},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2011.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0955799711001585},
author = {Vladimír Kompiš and Qing-Hua Qin and Zhuo-Jia Fu and C.S. Chen and Peter Droppa and Miroslav Kelemen and Wen Chen},
keywords = {Composite materials, CNT-fibres, 1D continuous Trefftz functions, LS collocation, Heat conduction},
abstract = {The aspect ratio of CNT-fibres reinforcing composite material is often with the value of 103:1–106:1, or even larger. Thus, the method of Continuous Source Functions (MCSF) developed by authors using 1D continuous source functions distributed along the fibre axis enables to simulate the interaction of each fibre with the matrix and also with other fibres. 1D source functions can serve as Trefftz (T-) functions, which satisfy the governing equations inside the domain (matrix), and boundary conditions on the fibre–matrix interfaces are satisfied in collocation points in the least square (LS) sense along the fibre boundaries. The source functions are defined by Non-Uniform Rational B-Spline (NURBS). Because of large gradients of stress fields, large number of collocation points and many NURBS shape functions are necessary to simulate the interaction. Moreover, the matrices used for solving the problem are numerically full. In our model, only the interactions of each two fibres is solved by elimination and then the complex interaction of all fibres in a patch of fibres and the matrix is completed by iteration steps in order to increase efficiency of computations. Such procedure enables to use parallel algorithm for solving all interactions of the pairs of fibres parallel. For heat conduction problem, fibres are supposed to be super-conductive at the first stage. The energy balance condition in each fibre enables to find temperature change of each fibre by the interaction with the other fibre in the first iteration step. The next iteration step enables to correct the temperature changes of the fibres by complex interaction of all fibres and the matrix and distribution of the source functions inside the fibres are obtained. Temperatures and heat flows in the control volume enable then to define homogenised material properties for corresponding patch of the composite material.}
}
@article{REESE20101276,
title = {Finite element-based multi-phase modelling of shape memory polymer stents},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {199},
number = {21},
pages = {1276-1286},
year = {2010},
note = {Multiscale Models and Mathematical Aspects in Solid and Fluid Mechanics},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2009.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S004578250900262X},
author = {S. Reese and M. Böl and D. Christ},
keywords = {Large deformations, Finite element method, Rubber–glass transition, Phase transformation},
abstract = {In the western hemisphere almost the half of all events of death are caused by cardiovascular diseases, e.g. strokes and heart attacks. The latter are consequences of arteriosclerosis leading to abnormal deposits (plaque) in blood vessels. In order to avoid the serious symptoms discussed in the above or to hold affected blood vessels open, tubular structures made of metallic or polymeric materials (stents) are implanted. In the paper we discuss the modelling of a new kind of stents, so-called shape memory polymer (SMP) stents. The first part of the paper is devoted to the thermo-mechanical modelling of these materials. Aspects as the transition from entropy to energy elasticity are included. The constitutive equations are derived in the framework of large strains. We follow both, a purely macroscopic as well as a micromechanically motivated approach. In the second part of the work representative examples based on realistic stent structures are used to validate the model.}
}
@article{MITRAN2013193,
title = {Continuum-kinetic-microscopic model of lung clearance due to core-annular fluid entrainment},
journal = {Journal of Computational Physics},
volume = {244},
pages = {193-211},
year = {2013},
note = {Multi-scale Modeling and Simulation of Biological Systems},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.01.037},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113000909},
author = {Sorin Mitran},
keywords = {Multiscale, Lattice Boltzmann, Lattice Fokker–Plank, Core-annular, Lung clearance},
abstract = {The human lung is protected against aspirated infectious and toxic agents by a thin liquid layer lining the interior of the airways. This airway surface liquid is a bilayer composed of a viscoelastic mucus layer supported by a fluid film known as the periciliary liquid. The viscoelastic behavior of the mucus layer is principally due to long-chain polymers known as mucins. The airway surface liquid is cleared from the lung by ciliary transport, surface tension gradients, and airflow shear forces. This work presents a multiscale model of the effect of airflow shear forces, as exerted by tidal breathing and cough, upon clearance. The composition of the mucus layer is complex and variable in time. To avoid the restrictions imposed by adopting a viscoelastic flow model of limited validity, a multiscale computational model is introduced in which the continuum-level properties of the airway surface liquid are determined by microscopic simulation of long-chain polymers. A bridge between microscopic and continuum levels is constructed through a kinetic-level probability density function describing polymer chain configurations. The overall multiscale framework is especially suited to biological problems due to the flexibility afforded in specifying microscopic constituents, and examining the effects of various constituents upon overall mucus transport at the continuum scale.}
}
@article{YAN2016208,
title = {A multiscale computational framework for the analysis of graphene involving geometrical and material nonlinearities},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {310},
pages = {208-232},
year = {2016},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2016.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0045782516306958},
author = {J.W. Yan and L.W. Zhang and K.M. Liew},
keywords = {Atomistic–continuum approach, Graphene, Bending deflection, Material nonlinearity, Geometrical nonlinearity},
abstract = {An atomistic–continuum approach, in which the constitutive model is derived from the lattice structure of graphene, is developed to simulate the mechanical behaviors of graphene. The chirality of graphene can be reflected by introducing a representative cell and calculated results reveal that the chirality of graphene has little effect on structural parameters and elastic properties. Since the constitutive model has incorporated the information in connection with atomistic structure, the material nonlinearity can be exactly reflected by iteratively updating the constitutive relationship in the present approach. Moreover, geometrical nonlinearity has also been considered under the higher-order gradient continuum theory. The bending deflections of rectangular and circular graphene, with both geometrical and material nonlinearities, having simply supported and clamped constraints are investigated. Based on the constitutive model, the definition of graphene thickness in building the stiffness matrix can be avoided by using the current atomistic–continuum approach. Computational results reveal that the atomistic–continuum approach can accurately capture geometrical and material nonlinearities of graphene and provide a good prediction of the full atomistic simulation even with a small number of nodes.}
}
@article{MTHUNZI2020620,
title = {Cloud computing security taxonomy: From an atomistic to a holistic view},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {620-644},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19300871},
author = {Siyakha N. Mthunzi and Elhadj Benkhelifa and Tomasz Bosakowski and Chirine Ghedira Guegan and Mahmoud Barhamgi},
abstract = {Countless discussions around security challenges affecting cloud computing are often large textual accounts, which can be cumbersome to read and prone to misinterpretation. The growing reliance on cloud computing means that not only should we focus on evaluating its security challenges but devote greater attention towards how challenges are viewed and communicated. With many cloud computing implementations in use and a growing evolution of the cloud paradigm (including fog, edge and cloudlets), comprehending, correlating and classifying diverse perspectives to security challenges increasingly becomes critical. Current classifications are only suited for limited use; both as effective tools for research and countermeasures design. The taxonomic approach has been used as a modeling technique towards classifying concepts across many domains. This paper surveys multiple perspectives of cloud security challenges and systematically develops corresponding graphical taxonomy based upon meta-synthesis of important cloud security concepts in literature. The contributions and significance of this work are as follows: (1) a holistic view simplifies visualization for the reader by providing illustrative graphics of existing textual perspectives, highlighting entity relationships among cloud entities/players thereby exposing security areas at every layer of the cloud. (2) a holistic taxonomy that facilitates the design of enforcement or corrective countermeasures based upon the source or origin of a security incident. (3) a holistic taxonomy highlights security boundary and identifies apt areas to implement security countermeasures.}
}
@article{XIANG2016104,
title = {Meshfree simulation of temperature effects on the mechanical behaviors of microtubules},
journal = {Engineering Analysis with Boundary Elements},
volume = {69},
pages = {104-118},
year = {2016},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2016.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0955799716300893},
author = {Ping Xiang and L.W. Zhang and K.M. Liew},
keywords = {Microtubules, Meshfree method, Temperature, Mechanical behaviors},
abstract = {The temperature-related mechanical behaviors of microtubules are investigated by way of the developed meshfree computational framework. An atomistic-continuum constitutive relationship is formulated for bridging-scale simulations of microtubules from polyatomic structure to continuum meshfree modeling. The establishment of a specific meshfree theory is based on high-order gradient continuity, by incorporating a higher-order Cauchy–Born rule. The influence of temperature on the critical buckling force and free vibration frequencies of microtubules is intensively studied. It is realized from the simulation results that temperature significantly affects the mechanical behaviors of microtubules. The critical buckling force and natural vibration frequencies of microtubules decrease with increases in temperature. A lower temperature will always result in a higher flexural rigidity, thus benefiting the mechanical strength of microtubules. In contrast, an elevated temperature will have negative impacts on microtubule stiffness. Microtubules with typical boundary restrictions subjected to different temperatures are included in the analysis. A series of simulation results on the critical buckling force and natural vibration frequencies of microtubules covering a wide range of microtubule lengths is presented for the purpose of the provision of engineering references.}
}
@article{SITTONCANDANEDO2019278,
title = {A review of edge computing reference architectures and a new global edge proposal},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {278-294},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1930264X},
author = {Inés Sittón-Candanedo and Ricardo S. Alonso and Juan M. Corchado and Sara Rodríguez-González and Roberto Casado-Vara},
keywords = {Edge computing, Internet of Things, Blockchain, Reference architecture, Industry 4.0, Agroindustry},
abstract = {Edge Computing represents the activities of IoT (Internet of Things) devices at the border or limit of the network connected to the remote Cloud. The latest research in this field has intended to demonstrate that Edge Computing architectures are the optimal solution to minimising latency, improving privacy and reducing bandwidth costs in IoT-based scenarios. This article is a review of the Edge Computing technology and its reference architectures proposed by the Edge Computing Consortium, Intel-SAP, the FAR-Edge Project and the Industrial Internet Consortium for Industry 4.0. Moreover, this article presents a proposal for a tiered architecture with a modular approach that allows to manage the complexity of solutions not only for Industry 4.0 environments but also for other scenarios such as smart cities, smart energy, healthcare or precision agrotechnology. The main contributions of the proposed architecture reside in the security and privacy provided by blockchain technologies. Finally, the proposed reference architecture is tested by building an IoT platform in a smart agroindustry scenario to reduce bandwidth costs between the Edge and the Cloud.}
}
@article{ALWASEL202017,
title = {IoTSim-SDWAN: A simulation framework for interconnecting distributed datacenters over Software-Defined Wide Area Network (SD-WAN)},
journal = {Journal of Parallel and Distributed Computing},
volume = {143},
pages = {17-35},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S074373151930886X},
author = {Khaled Alwasel and Devki Nandan Jha and Eduardo Hernandez and Deepak Puthal and Mutaz Barika and Blesson Varghese and Saurabh Kumar Garg and Philip James and Albert Zomaya and Graham Morgan and Rajiv Ranjan},
keywords = {Software-Defined Wide Area Network (SD-WAN), Software-Defined Network (SDN), Classical WAN, Internet of Things (IoT)},
abstract = {Software-defined networking (SDN) has evolved as an approach that allows network administrators to program and initialize, control, change and manage networking components (mostly at L2-L3 layers) of the OSI model. SDN is designed to address the programmability shortcomings of traditional networking architectures commonly used in cloud datacenters (CDC). Deployment of SDN solutions have demonstrated significant improvements in areas such as flow optimization and bandwidth allocation in a CDC. However, the benefits are significantly less explored when considering Software-Defined Wide Area Networks (SD-WAN) architectures in the context of delivering solutions by networking multiple CDCs. To support the testing and bench-marking of data-driven applications that rely on data ingestion and processing (e.g., Smart Energy Cloud, Content Delivery Networks) across multiple cloud datacenters, this paper presents the simulator, IoTSim-SDWAN. To the best of our knowledge, IoTSim-SDWAN is the first simulator that facilitates the modeling, simulating, and evaluating of new algorithms, policies, and designs in the context of SD-WAN ecosystems and SDN-enabled multiple cloud datacenters. Finally, IoTSim-SDWAN simulator is evaluated for network performance and energy to illustrate the difference between classical WAN and SD-WAN environments. The obtained results show that SD-WAN surpasses the classical WAN in terms of accelerating traffic flows and reducing power consumption.}
}
@article{FORMICA2018126,
title = {Computational efficiency and accuracy of sequential nonlinear cyclic analysis of carbon nanotube nanocomposites},
journal = {Advances in Engineering Software},
volume = {125},
pages = {126-135},
year = {2018},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2018.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818302928},
author = {Giovanni Formica and Franco Milicchio and Walter Lacarbonara},
keywords = {Three-dimensional finite element modeling, Time integration schemes, Differential evolution optimization, Hysteretic damping, CNT Nanocomposites},
abstract = {The accuracy and efficiency of a numerical strategy for sequential nonlinear cyclic analyses of carbon nanotube nanocomposites are investigated. The computational approach resorts to a nonlinear 3D finite element implementation that seeks to solve the cyclic hysteretic response of the nanocomposite. A variant of the Newton-Raphson method within a time integration scheme is proposed whereby the elastic tangent matrix is chosen as iteration matrix without paying the price of its iterative update. This is especially rewarding in the context of the employed mechanical model which exhibits hysteresis manifested through a discontinuous change in the stiffness at the reversal points where the loading direction is reversed. Key implementation aspects – such as the integration of the nonlinear 3D equations of motion, the numerical accuracy/efficiency as a function of the time step or the mesh size – are discussed. In particular, efficiency is regarded as performing fast computations especially when the number of cyclic analyses becomes large. By making use of laptop CPU cores, a good speed of computations is achieved not only through parallelization but also employing a caching procedure for the iteration matrix.}
}
@article{ZHU201616,
title = {Implicit unified gas-kinetic scheme for steady state solutions in all flow regimes},
journal = {Journal of Computational Physics},
volume = {315},
pages = {16-38},
year = {2016},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2016.03.038},
url = {https://www.sciencedirect.com/science/article/pii/S002199911600190X},
author = {Yajun Zhu and Chengwen Zhong and Kun Xu},
keywords = {Implicit scheme, Unified gas kinetic scheme, LU-SGS, Steady state solution, Rarefied and continuum flows},
abstract = {This paper presents an implicit unified gas-kinetic scheme (UGKS) for non-equilibrium steady state flow computation. The UGKS is a direct modeling method for flow simulation in all regimes with the updates of both macroscopic flow variables and microscopic gas distribution function. By solving the macroscopic equations implicitly, a predicted equilibrium state can be obtained first through iterations. With the newly predicted equilibrium state, the evolution equation of the gas distribution function and the corresponding collision term can be discretized in a fully implicit way for fast convergence through iterations as well. The lower–upper symmetric Gauss–Seidel (LU-SGS) factorization method is implemented to solve both macroscopic and microscopic equations, which improves the efficiency of the scheme. Since the UGKS is a direct modeling method and its physical solution depends on the mesh resolution and the local time step, a physical time step needs to be fixed before using an implicit iterative technique with a pseudo-time marching step. Therefore, the physical time step in the current implicit scheme is determined by the same way as that in the explicit UGKS for capturing the physical solution in all flow regimes, but the convergence to a steady state speeds up through the adoption of a numerical time step with large CFL number. Many numerical test cases in different flow regimes from low speed to hypersonic ones, such as the Couette flow, cavity flow, and the flow passing over a cylinder, are computed to validate the current implicit method. The overall efficiency of the implicit UGKS can be improved by one or two orders of magnitude in comparison with the explicit one.}
}
@article{PATRA2014744,
title = {A spectral multiscale method for wave propagation analysis: Atomistic–continuum coupled simulation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {278},
pages = {744-764},
year = {2014},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2014.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0045782514002035},
author = {Amit K. Patra and S. Gopalakrishnan and Ranjan Ganguli},
keywords = {Spectral multiscale method, Spectral coarse–fine decomposition, Molecular dynamics, Spectral finite element, Numerical Laplace transform, Atomistic–continuum coupled analysis},
abstract = {In this paper, we present a new multiscale method which is capable of coupling atomistic and continuum domains for high frequency wave propagation analysis. The problem of non-physical wave reflection, which occurs due to the change in system description across the interface between two scales, can be satisfactorily overcome by the proposed method. We propose an efficient spectral domain decomposition of the total fine scale displacement along with a potent macroscale equation in the Laplace domain to eliminate the spurious interfacial reflection. We use Laplace transform based spectral finite element method to model the macroscale, which provides the optimum approximations for required dynamic responses of the outer atoms of the simulated microscale region very accurately. This new method shows excellent agreement between the proposed multiscale model and the full molecular dynamics (MD) results. Numerical experiments of wave propagation in a 1D harmonic lattice, a 1D lattice with Lennard-Jones potential, a 2D square Bravais lattice, and a 2D triangular lattice with microcrack demonstrate the accuracy and the robustness of the method. In addition, under certain conditions, this method can simulate complex dynamics of crystalline solids involving different spatial and/or temporal scales with sufficient accuracy and efficiency.}
}
@article{LOCKERBY2013344,
title = {Time-step coupling for hybrid simulations of multiscale flows},
journal = {Journal of Computational Physics},
volume = {237},
pages = {344-365},
year = {2013},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2012.11.032},
url = {https://www.sciencedirect.com/science/article/pii/S0021999112007127},
author = {Duncan A. Lockerby and Carlos A. Duque-Daza and Matthew K. Borg and Jason M. Reese},
keywords = {Multiscale simulations, Unsteady micro/nano flows, Hybrid methods, Scale separation},
abstract = {A new method is presented for the exploitation of time-scale separation in hybrid continuum-molecular models of multiscale flows. Our method is a generalisation of existing approaches, and is evaluated in terms of computational efficiency and physical/numerical error. Comparison with existing schemes demonstrates comparable, or much improved, physical accuracy, at comparable, or far greater, efficiency (in terms of the number of time-step operations required to cover the same physical time). A leapfrog coupling is proposed between the ‘macro’ and ‘micro’ components of the hybrid model and demonstrates potential for improved numerical accuracy over a standard simultaneous approach. A general algorithm for a coupled time step is presented. Three test cases are considered where the degree of time-scale separation naturally varies during the course of the simulation. First, the step response of a second-order system composed of two linearly-coupled ODEs. Second, a micro-jet actuator combining a kinetic treatment in a small flow region where rarefaction is important with a simple ODE enforcing mass conservation in a much larger spatial region. Finally, the transient start-up flow of a journal bearing with a cylindrical rarefied gas layer. Our new time-stepping method consistently demonstrates as good as or better performance than existing schemes. This superior overall performance is due to an adaptability inherent in the method, which allows the most-desirable aspects of existing schemes to be applied only in the appropriate conditions.}
}
@article{GARCIAGRAJALES2017147,
title = {Continuum mechanical modeling of axonal growth},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {314},
pages = {147-163},
year = {2017},
note = {Special Issue on Biological Systems Dedicated to William S. Klug},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2016.07.032},
url = {https://www.sciencedirect.com/science/article/pii/S0045782516308064},
author = {Julián Andrés García-Grajales and Antoine Jérusalem and Alain Goriely},
keywords = {Axonal guidance, Axonal growth, Continuum modeling, Axonal elongation},
abstract = {Axonal growth is a complex phenomenon in which many intra- and extra-cellular signals collaborate simultaneously. Two different compartments can be identified in the growing axon: the growth cone, the leading tip that guides and steers the axon, and the axonal shaft, connecting the soma to the growth cone. The complex relations between both compartments and how their interaction leads the axon to its final synaptic target remain a topic of intense scrutiny. Here, we present a continuum and computational model for the development of the axonal shaft. Two different regions are considered: the axoplasm, filled with microtubules, and the surrounding cortical membrane, consisting mainly of F-actin, Myosin II motor proteins and the membrane. Based on the theory of morphoelasticity, the deformation gradient is decomposed into anelastic and viscoelastic parts. The former corresponds to either a growth tensor for the axoplasm, or a composition of growth and contractile tensors for the cortical membrane. The biophysical evolution for the anelastic parts is obtained at the constitutive level, in which the polymerization and depolymerization of microtubules and F-actin drive the growth, while the contractility is due to the pulling exerted by the Myosin II on the F-actin and depends on the stress. The coupling between cytoskeletal dynamics and mechanics is naturally derived from the equilibrium equations. The framework is exploited in two representative scenarios in which an external force is applied to the axonal shaft either along the axis or off the axis. In the first case three states are found: growth, collapse and stall. In the second case, axonal turning is observed. This framework is suitable to investigate the complex relationship between the local mechanical state, the cytoskeletal polymerization/depolymerization rates, and the contractility of the cortical membrane in axonal guidance.}
}
@article{ZHU2019190,
title = {An implicit unified gas-kinetic scheme for unsteady flow in all Knudsen regimes},
journal = {Journal of Computational Physics},
volume = {386},
pages = {190-217},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119300841},
author = {Yajun Zhu and Chengwen Zhong and Kun Xu},
keywords = {Unified gas-kinetic scheme, Implicit method, Unsteady flow, Multiscale transport},
abstract = {The unified gas-kinetic scheme (UGKS) is a direct modeling method for multiple scale transport. Based on the ratio of time step to the particle collision time, the local evolution solution on the mesh size and time step scales is used in the construction of the multiscale method. For a flow problem covering multiple flow regimes, such as the hypersonic flow around a flying vehicle in near space, the UGKS is able to capture the highly compressed Navier-Stokes solution in one region and fully expanded free molecular flow in another region, with significant variations of the ratio between the time step and the local particle collision time around the vehicle. For an explicit UGKS, the time step in the whole computational domain is determined by the CFL condition. With implicit and multigrid techniques, the efficiency of the UGKS [1], [2] has been improved by two orders of magnitude for steady state computation. However, for unsteady flow computation, due to the CFL condition the global time step used in the explicit UGKS may be limited by the smallest cell size in the computational domain. As a result, for a largely stretched non-uniform mesh the global time step becomes very small and the ratio of the time step to the local particle collision time may get a very small value. Under such a circumstance, even though the UGKS is a multiscale method, the real physics represented in the explicit UGKS may be constrained to the kinetic scale transport only, and the advantage of the multiscale nature in UGKS has not been fully utilized. In order to solve the multiscale unsteady flow problem efficiently, the time step restriction from a global CFL condition has to be released. In this paper, we will develop an implicit UGKS (IUGKS) for unsteady flows by alternatively solving the macroscopic and microscopic governing equations within a time step iteratively. With a pre-defined uniform large evolution time step, the local CFL number varies greatly in different region, such as on the order 1 in the large numerical cell size region, and 100 in the small cell size region. In order to preserve coherent flow evolution and keep the multiscale nature, the time averaged numerical flux across a cell interface is still evaluated by the explicit UGKS under the local CFL condition. Therefore, the multiscale property of the UGKS modeling has been kept over non-uniform meshes. With improved temporal discretization, the current IUGKS can automatically go back to the explicit UGKS and obtain identical solutions when the time step of the implicit scheme gets to that of an explicit one. Many numerical examples are included to validate the scheme for both continuum and rarefied flows with a large variation of artificially generated mesh size. The IUGKS has a second order accuracy and presents reasonably good results for unsteady flow computation, and its efficiency has been improved by dozens of times in comparison with the explicit UGKS.}
}
@incollection{ZIEMYS2018209,
title = {Chapter 7 - Multiscale models for transport and biodistribution of therapeutics in cancer},
editor = {Davide Manca},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {42},
pages = {209-237},
year = {2018},
booktitle = {Quantitative Systems Pharmacology},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63964-6.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444639646000076},
author = {Arturas Ziemys and Milos Kojic and Miljan Milosevic and Bernhard Schrefler and Mauro Ferrari},
keywords = {Diffusion, Convection, Finite element method, Molecular dynamics, Pharmacokinetics, Doxorubicin, Tumor microenvironment},
abstract = {The chapter discusses the computational framework dedicated to simulate time-dependent biodistribution with focus on cancer. While systemic pharmacokinetic and biodistribution problems are extensively studied, the factors considered here are drug transport from drug vectors, drug transport and biodistribution in the tumor microenvironment, whole organ, and including the biodistribution effect on tumor growth. Continuum-based computational strategies are presented. Selected example solutions demonstrate the applicability of this methodology to in vitro and in vivo conditions.}
}
@incollection{MARTINS2017103,
title = {Chapter 7 - Network-Wide Programming Challenges in Cyber-Physical Systems},
editor = {Houbing Song and Danda B. Rawat and Sabina Jeschke and Christian Brecher},
booktitle = {Cyber-Physical Systems},
publisher = {Academic Press},
address = {Boston},
pages = {103-113},
year = {2017},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-803801-7},
doi = {https://doi.org/10.1016/B978-0-12-803801-7.00007-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128038017000079},
author = {P.M.N. Martins and J.A. McCann},
keywords = {Cyber-physical systems, Internet of Things, Programming languages, Network-wide programming, Ubiquitous computing, Smart cities},
abstract = {The worldwide proliferation of mobile connected sensing, processing, and physical actuation devices has brought about a revolution in the way we live, and will inevitably guide the way in which we design applications for these networks. In this chapter we will show how the scalable development of applications for highly distributed, heterogenous large networks requires a shift from the current device-centric programming model to a network-centric semantic model, whereby individual devices are abstracted away and identified by the semantic descriptions of the services they provide. This requires the development of primitives that have network-wide semantics. The emphasis must also be shifted from manipulating individual points of data to manipulating streams of data to enable real-time processing and reasoning. This requires that the programming models not only take into account semantic descriptions of the streams rather than individual devices and data points, but also the various modalities of computing that are possible in this scenario; a computing continuum from in-network processing to cloud computing spanning a range of devices from cloud to edge.}
}
@article{KEMP2015553,
title = {ISO 27018 and personal information in the cloud: First year scorecard},
journal = {Computer Law & Security Review},
volume = {31},
number = {4},
pages = {553-555},
year = {2015},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2015.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S026736491500093X},
author = {Richard Kemp},
keywords = {Data protection, Privacy, Data security, Big data, Cloud services, Software as a service, Standards, ISO 27018},
abstract = {As ISO 27018 – the first international standard on the protection of personal data in the public cloud – approaches its first birthday, it is emerging as a source of reassurance for Cloud Service Customers contracting with ISO 27018 certified Cloud Service Providers anywhere across the spectrum of Cloud services where personal data could be involved.}
}
@article{KOCHOVSKI2019747,
title = {Trust management in a blockchain based fog computing platform with trustless smart oracles},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {747-759},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301281},
author = {Petar Kochovski and Sandi Gec and Vlado Stankovski and Marko Bajec and Pavel D. Drobintsev},
keywords = {Trust, Fog, Blockchain, Smart contract, Smart oracle},
abstract = {Trust is a crucial aspect when cyber-physical systems have to rely on resources and services under ownership of various entities, such as in the case of Edge, Fog and Cloud computing. The DECENTER’s Fog Computing Platform is developed to support Big Data pipelines, which start from the Internet of Things (IoT), such as cameras that provide video-streams for subsequent analysis. It is used to implement Artificial Intelligence (AI) algorithms across the Edge-Fog-Cloud computing continuum which provide benefits to applications, including high Quality of Service (QoS), improved privacy and security, lower operational costs and similar. In this article, we present a trust management architecture for DECENTER that relies on the use of blockchain-based Smart Contracts (SCs) and specifically designed trustless Smart Oracles. The architecture is implemented on Ethereum ledger (testnet) and three trust management scenarios are used for illustration. The scenarios (trust management for cameras, trusted data flow and QoS based computing node selection) are used to present the benefits of establishing trust relationships among entities, services and stakeholders of the platform.}
}
@article{RASTIELLO2018650,
title = {From diffuse damage to strain localization from an Eikonal Non-Local (ENL) Continuum Damage model with evolving internal length},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {331},
pages = {650-674},
year = {2018},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2017.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0045782517307582},
author = {Giuseppe Rastiello and Cédric Giry and Fabrice Gatuingt and Rodrigue Desmorat},
keywords = {Eikonal Non-Local damage, Regularization, Evolving non-local interactions, Strain Localization, Fast Marching method},
abstract = {Integral Non-Local (INL) formulations are often used to regularize Continuum Damage computations, in the presence of stress softening for instance. The introduction of a characteristic/internal length allows for avoiding pathological mesh dependency. Some questions concerning the identification of the characteristic length, its possible evolution during damage process and the need for special treatments of non-locality operators near boundaries (e.g. edges, cracks) are however still open. A physical request is that material points separated by a crack (or an highly damaged zone) should not interact. Despite what is done in standard Integral Non-Local theories, this can be obtained by allowing non-local interactions to evolve depending on mechanical fields (e.g. damage, strain, stress). The Eikonal Non-Local (ENL) formulation provides a novel interpretation of damage dependent non-local interactions. Based on the Wentzel–Kramers–Brillouin (WKB) approximation for high-frequency wave propagation in a damaged medium, this formulation defines the interaction distances as the solution of a stationary damage dependent Eikonal equation. It allows for the modeling of non-local interactions which gradually vanish in damaged zones, thus ensuring a progressive transition from diffuse damage to fracture in a natural way. The numerical implementation and properties of this regularization technique are investigated and discussed. From a numerical viewpoint, a Fast Marching method is used to compute non-local interaction distances between Gauss integration points. Geodesic distances are then used to define the kernel of weighting function to be used in integral non-local averaging. Several numerical results of quasi-statics simulations of quasi-brittle fracture in isotropic media are presented.}
}
@article{CHEN20112473,
title = {AES for multiscale localization modeling in granular media},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {200},
number = {33},
pages = {2473-2482},
year = {2011},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2011.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0045782511001630},
author = {Qiushi Chen and José E. Andrade and Esteban Samaniego},
keywords = {Assumed enhanced strain (AES) method, Strong discontinuities, Localization, Multiscale, DEM, Granular media},
abstract = {This work presents a multiscale strong discontinuity approach to tackle key challenges in modeling localization behavior in granular media: accommodation of discontinuities in the kinematic fields, and direct linkage to the underlying grain-scale information. Assumed enhanced strain (AES) concepts are borrowed to enhance elements for post-localization analysis, but are reformulated within a recently-proposed hierarchical multiscale computational framework. Unlike classical AES methods, where material properties are usually constants or assumed to evolve with some arbitrary phenomenological laws, this framework provides a bridge to extract evolutions of key material parameters, such as friction and dilatancy, based on grain scale computational or experimental data. More importantly, the phenomenological softening modulus typically used in AES methods is no longer required. Numerical examples of plane strain compression tests are presented to illustrate the applicability of this method and to analyze its numerical performance.}
}
@article{KHAN2020102537,
title = {A survey of subscription privacy on the 5G radio interface - The past, present and future},
journal = {Journal of Information Security and Applications},
volume = {53},
pages = {102537},
year = {2020},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2020.102537},
url = {https://www.sciencedirect.com/science/article/pii/S2214212620300235},
author = {Haibat Khan and Keith M. Martin},
keywords = {5G, Anonymity, GSM, LTE, Mobile networks, Privacy, UMTS, Unlinkability},
abstract = {End-user privacy in mobile telephony systems is nowadays of great interest because of the envisaged hyper-connectivity and the potential of the unprecedented services (virtual reality, machine-type communication, vehicle-to-everything, IoT, etc.) being offered by the new 5G system. This paper reviews the state of subscription privacy in 5G systems. As the work on 5G Release 15 – the first full set of 5G standards – has recently been completed, this seems to be an appropriate occasion for such a review. The scope of the privacy study undertaken is limited to the wireless part of the 5G system which occurs between the service provider’s base station and the subscriber’s mobile phone. Although 5G offers better privacy guarantees than its predecessors, this work highlights that there still remain significant issues which need rectifying. We undertook an endeavor to (i) compile the privacy vulnerabilities that already existed in the previous mobile telephony generations. Thereafter, (ii) the privacy improvements offered by the recently finalized 5G standard were aggregated. Consequently, (iii) we were able to highlight privacy issues from previous generations that remain unresolved in 5G Release 15. For completeness, (iv) we also explore new privacy attacks which surfaced after the publication of the 5G standard. To address the identified privacy gaps, we also present future research directions in the form of proposed improvements.}
}
@article{SIENIEK20141027,
title = {Fast Graph Transformation based Direct Solver Algorithm for Regular Three Dimensional Grids},
journal = {Procedia Computer Science},
volume = {29},
pages = {1027-1038},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.092},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914002695},
author = {Marcin Sieniek},
keywords = {step-and-flash imprint lithography, multi-scale, graph transformations, multi-frontal solver},
abstract = {This paper presents a graph-transformation-based multifrontal direct solver with an optimization technique that allows for a significant decrease of time complexity in some multi-scale simulations of the Step and Flash Imprint Lithography (SFIL). The multi-scale simulation consists of a macro-scale linear e lasticity model with thermal expansion coefficient and a nano-scale molecular statics model. The algorithm is exemplified with a photopolimerization simulation that involves densification of a polymer inside a feature followed by shrinkage of the feature after removal of the template. The solver is optimized thanks to a mechanism of reusing sub -domains with similar geometries and similar material properties. The graph transformation formalism is used to describe the algorithm - such an approach helps automatically localize sub-domains that can be reused.}
}
@article{ZKIK2019977,
title = {Secure Multipath Mutation SMPM in Moving Target Defense Based on SDN},
journal = {Procedia Computer Science},
volume = {151},
pages = {977-984},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.137},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919306015},
author = {Karim ZKIK and Anass Sebbar and Youssef Baddi and Mohammed Boulmalf},
keywords = {Moving target defense, Random Route Mutation, Software-defined networking, Pathfinder, Multipath Mutation, Security, Privacy},
abstract = {Software-defined networking (SDN) refers to a network architecture where the transfer state in the data plane is managed by a remote control plane in a centralized manner. SDN offer many advantage in terms of flexibility and automation to administrator but it suffer from many security issues. In other hand, Random Route Mutation (RRM) and path diversity represent one of the important research focuses about moving target defense (MTD). The main idea of using this technic, is to change periodically (or basing on events) used routes between sender and receiver in order to enhance mutation efficiency and decrease attackers capabilities to launch effective eavesdropping, denial of service or man in the middle attack. Using RRM and multi path technics can be very interesting in order to secure SDN and to detect and prevent intrusions. In this paper it is propose a new framework called SMPM which aims to secure and prevent intrusion by modeling SDN architectures and using a pathfinder algorithm called RRM-Pathfinder. The proposed framework calculates all possible paths from given source to destination and then, based on some criteria such as capacity, Overlap, Security and QoS, it selects and identifies the most cost-effective routes. The use of SMPM allow also to dynamically route packets using all pre-calculated paths which will permit to avoid sniffing and poisoning attacks such as Arp spoof and the man in the middle attacks and to ensure more confidentiality, integrity and privacy.}
}
@article{SONBOL202028,
title = {EdgeKV: Decentralized, scalable, and consistent storage for the edge},
journal = {Journal of Parallel and Distributed Computing},
volume = {144},
pages = {28-40},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520302884},
author = {Karim Sonbol and Öznur Özkasap and Ibrahim Al-Oqily and Moayad Aloqaily},
keywords = {Edge computing, Distributed systems, Key–value store, DHT, Consistency},
abstract = {Edge computing moves the computation closer to the data and the data closer to the user to overcome the high latency communication of cloud computing. Storage at the edge allows data access with high speeds that enable latency-sensitive applications in areas such as autonomous driving and smart grid. However, several distributed services are typically designed for the cloud and building an efficient edge-enabled storage system is challenging because of the distributed and heterogeneous nature of the edge and its limited resources. In this paper, we propose EdgeKV, a decentralized storage system designed for the network edge. EdgeKV offers fast and reliable storage, utilizing data replication with strong consistency guarantees. With a location-transparent and interface-based design, EdgeKV can scale with a heterogeneous system of edge nodes. We implement a prototype of the EdgeKV modules in Golang and evaluate it in both the edge and cloud settings on the Grid’5000 testbed. We utilize the Yahoo! Cloud Serving Benchmark (YCSB) to analyze the system’s performance under realistic workloads. Our evaluation results show that EdgeKV outperforms the cloud storage setting with both local and global data access with an average write response time and throughput improvements of 26% and 19% respectively under the same settings. Our evaluations also show that EdgeKV can scale with the number of clients, without sacrificing performance. Finally, we discuss the energy efficiency improvement when utilizing edge resources with EdgeKV instead of a centralized cloud.}
}
@article{AHN20094088,
title = {An axisymmetric computational model of generalized hydrodynamic theory for rarefied multi-species gas flows},
journal = {Journal of Computational Physics},
volume = {228},
number = {11},
pages = {4088-4117},
year = {2009},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2009.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S002199910900093X},
author = {Jae Wan Ahn and Chongam Kim},
keywords = {Generalized hydrodynamic equation, Hypersonic rarefied flow computations, Computational fluid dynamics, Second law of thermodynamics, Nonequilibrium effect},
abstract = {On the basis of the Eu’s generalized hydrodynamic (GH) theories for diatomic single species gas and monatomic multi-species gas, an axisymmetric GH computational model for multi-species gas containing monatomic and diatomic molecules is developed for the numerical simulation of hypersonic rarefied gas flows. The multi-species GH computational model includes monatomic and diatomic species of O2,N2, NO, O, N. The mass diffusion flux of the gas mixture is included in the GH constitutive relation. In addition, the physical relationship between the mass diffusion and heat fluxes is added to the evolution equation set. The multi-species GH theory includes the rotational nonequilibrium effect of diatomic molecules by introducing excess normal stress associated with the bulk viscosity. An efficient multi-species GH numerical solver for axisymmetric rarefied flows is then developed by adopting various numerical techniques, such as an adequate nonlinear equation solver for the GH constitutive relation, an accurate flux splitting scheme, multi-grid convergence acceleration and slip-wall boundary conditions. For validation, the proposed computational model is applied to hypersonic rarefied flows over a space shuttle nose, a sphere and a reentry body as well as 1D shock structure. By comparing the results of the multi-species GH model with those of the Navier–Stokes equation and the DSMC, the accuracy and physical consistency of the GH computational model are critically examined.}
}
@article{XU20107747,
title = {A unified gas-kinetic scheme for continuum and rarefied flows},
journal = {Journal of Computational Physics},
volume = {229},
number = {20},
pages = {7747-7764},
year = {2010},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2010.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0021999110003475},
author = {Kun Xu and Juan-Chen Huang},
keywords = {Unified scheme, Navier–Stokes equations, Free molecule flow},
abstract = {With discretized particle velocity space, a multiscale unified gas-kinetic scheme for entire Knudsen number flows is constructed based on the BGK model. The current scheme couples closely the update of macroscopic conservative variables with the update of microscopic gas distribution function within a time step. In comparison with many existing kinetic schemes for the Boltzmann equation, the current method has no difficulty to get accurate Navier–Stokes (NS) solutions in the continuum flow regime with a time step being much larger than the particle collision time. At the same time, the rarefied flow solution, even in the free molecule limit, can be captured accurately. The unified scheme is an extension of the gas-kinetic BGK-NS scheme from the continuum flow to the rarefied regime with the discretization of particle velocity space. The success of the method is due to the un-splitting treatment of the particle transport and collision in the evaluation of local solution of the gas distribution function. For these methods which use operator splitting technique to solve the transport and collision separately, it is usually required that the time step is less than the particle collision time. This constraint basically makes these methods useless in the continuum flow regime, especially in the high Reynolds number flow simulations. Theoretically, once the physical process of particle transport and collision is modeled statistically by the kinetic Boltzmann equation, the transport and collision become continuous operators in space and time, and their numerical discretization should be done consistently. Due to its multiscale nature of the unified scheme, in the update of macroscopic flow variables, the corresponding heat flux can be modified according to any realistic Prandtl number. Subsequently, this modification effects the equilibrium state in the next time level and the update of microscopic distribution function. Therefore, instead of modifying the collision term of the BGK model, such as ES-BGK and BGK–Shakhov, the unified scheme can achieve the same goal on the numerical level directly. Many numerical tests will be used to validate the unified method.}
}
@article{TANEJA2020105286,
title = {Machine learning based fog computing assisted data-driven approach for early lameness detection in dairy cattle},
journal = {Computers and Electronics in Agriculture},
volume = {171},
pages = {105286},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105286},
url = {https://www.sciencedirect.com/science/article/pii/S016816991931840X},
author = {Mohit Taneja and John Byabazaire and Nikita Jalodia and Alan Davy and Cristian Olariu and Paul Malone},
keywords = {Smart dairy farming, Fog computing, Internet of Things (IoT), Cloud computing, Smart farm, Data analytics, Microservices, Machine learning, Clustering, Classification, Data-driven},
abstract = {Timely lameness detection is one of the major and costliest health problems in dairy cattle that farmers and practitioners haven't yet solved adequately. The primary reason behind this is the high initial setup costs, complex equipment and lack of multi-vendor interoperability in currently available solutions. On the other hand, human observation based solutions relying on visual inspections are prone to late detection with possible human error, and are not scalable. This poses a concern with increasing herd sizes, as prolonged or undetected lameness severely compromises cows' health and welfare, and ultimately affects the milk productivity of the farm. To tackle this, we have developed an end-to-end IoT application that leverages advanced machine learning and data analytics techniques to monitor the cattle in real-time and identify lame cattle at an early stage. The proposed approach has been validated on a real world smart dairy farm setup consisting of a dairy herd of 150 cows in Waterford, Ireland. Using long-range pedometers specifically designed for use in dairy cattle, we monitor the activity of each cow in the herd. The accelerometric data from these sensors is aggregated at the fog node to form a time series of behavioral activities, which are further analyzed in the cloud. Our hybrid clustering and classification model identifies each cow as either Active, Normal or Dormant, and further, Lame or Non-Lame. The detected lameness anomalies are further sent to farmer's mobile device by way of push notifications. The results indicate that we can detect lameness 3 days before it can be visually captured by the farmer with an overall accuracy of 87%. This means that the animal can either be isolated or treated immediately to avoid any further effects of lameness. Moreover, with fog based computational assistance in the setup, we see an 84% reduction in amount of data transferred to the cloud as compared to the conventional cloud based approach.}
}
@article{XU20086779,
title = {Multiple temperature kinetic model and gas-kinetic method for hypersonic non-equilibrium flow computations},
journal = {Journal of Computational Physics},
volume = {227},
number = {14},
pages = {6779-6794},
year = {2008},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2008.03.035},
url = {https://www.sciencedirect.com/science/article/pii/S0021999108001824},
author = {Kun Xu and Xin He and Chunpei Cai},
keywords = {Multiple temperature kinetic model, Gas-kinetic method, Hypersonic and rarefied flows},
abstract = {It is well known that for increasingly rarefied flowfields, the predictions from continuum formulation, such as the Navier–Stokes equations lose accuracy. For the high speed diatomic molecular flow in the transitional regime, the inaccuracies are partially attributed to the single temperature approximations in the Navier–Stokes equations. Here, we propose a continuum multiple temperature model based on the Bhatnagar–Gross–Krook (BGK) equation for the non-equilibrium flow computation. In the current model, the Landau–Teller–Jeans relaxation model for the rotational energy is used to evaluate the energy exchange between the translational and rotational modes. Due to the multiple temperature approximation, the second viscosity coefficient in the Navier–Stokes equations is replaced by the temperature relaxation term. In order to solve the multiple temperature kinetic model, a multiscale gas-kinetic finite volume scheme is proposed, where the gas-kinetic equation is numerically solved for the fluxes to update the macroscopic flow variables inside each control volume. Since the gas-kinetic scheme uses a continuous gas distribution function at a cell interface for the fluxes evaluation, the moments of a gas distribution function can be explicitly obtained for the multiple temperature model. Therefore, the kinetic scheme is much more efficient than the DSMC method, especially in the near continuum flow regime. For the non-equilibrium flow computations, i.e., the nozzle flow and hypersonic rarefied flow over flat plate, the computational results are validated in comparison with experimental measurements and DSMC solutions.}
}
@article{WANG2020109071,
title = {Deep multiscale model learning},
journal = {Journal of Computational Physics},
volume = {406},
pages = {109071},
year = {2020},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.109071},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119307764},
author = {Yating Wang and Siu Wun Cheung and Eric T. Chung and Yalchin Efendiev and Min Wang},
keywords = {Multiscale, Deep learning, Upscaling, Neural network, Porous media},
abstract = {The objective of this paper is to design novel multi-layer neural networks for multiscale simulations of flows taking into account the observed fine data and physical modeling concepts. Our approaches use deep learning techniques combined with local multiscale model reduction methodologies to predict flow dynamics. Using reduced-order model concepts is important for constructing robust deep learning architectures since the reduced-order models provide fewer degrees of freedom. We consider flow dynamics in porous media as multi-layer networks in this work. More precisely, the solution (e.g., pressures and saturation) at the time instant n+1 depends on the solution at the time instant n and input parameters, such as permeability fields, forcing terms, and initial conditions. One can regard the solution as a multi-layer network, where each layer, in general, is a nonlinear forward map and the number of layers relates to the internal time steps. We will rely on rigorous model reduction concepts to define unknowns and connections between layers. It is critical to use reduced-order models for this purpose, which will identify the regions of influence and the appropriate number of variables. Furthermore, due to the lack of available observed fine data, the reduced-order model can provide us sufficient inexpensive data as needed. The designed deep neural network will be trained using both coarse simulation data which is obtained from the reduced-order model and observed fine data. We will present the main ingredients of our approach and numerical examples. Numerical results show that using deep learning with data generated from multiscale models as well as available observed fine data, we can obtain an improved forward map which can better approximate the fine scale model.}
}
@article{SINGH2020113133,
title = {A singularity free approach for Kirchhoff rods having uniformly distributed electrostatic charge},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {367},
pages = {113133},
year = {2020},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2020.113133},
url = {https://www.sciencedirect.com/science/article/pii/S0045782520303182},
author = {Raushan Singh and Ajeet Kumar},
keywords = {Kirchhoff rod, Supercoiling, Debye–Huckel potential, Electrostatic charge, Elastic ring},
abstract = {We present a singularity free formulation and its efficient numerical implementation for the spatial deformation of Kirchhoff rods having uniformly distributed electrostatic charge. Due to the presence of continuously distributed charge, the governing equations of the Kirchhoff rod become a system of integro-differential equations which is singular at every arc-length. We show that this singularity is of removable type which, once removed, makes the system well defined everywhere. No cut-off length or mollifier is used to remove this singularity. An efficient finite difference scheme is presented for the numerical solution of this singularity free system of equations. We show that the presented numerical scheme turns out to be computationally efficient compared to an alternate approach in which the uniformly distributed charge is modeled by placing equivalent lumped charge at discrete locations along the rod. The scheme is demonstrated through an example problem of supercoiling in a charged elastic ring when twist is inserted in it.}
}
@article{VOGMAN2014101,
title = {Dory–Guest–Harris instability as a benchmark for continuum kinetic Vlasov–Poisson simulations of magnetized plasmas},
journal = {Journal of Computational Physics},
volume = {277},
pages = {101-120},
year = {2014},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2014.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S0021999114005609},
author = {G.V. Vogman and P. Colella and U. Shumlak},
keywords = {Vlasov–Poisson, Dory–Guest–Harris instability, Plasma kinetic theory, Continuum kinetic benchmark, Electrostatic waves in magnetized plasma, Phase space},
abstract = {The Dory–Guest–Harris instability is demonstrated to be a well-suited benchmark for continuum kinetic Vlasov–Poisson algorithms. The instability is a special case of perpendicularly-propagating kinetic electrostatic waves in a warm uniformly magnetized plasma. A complete derivation of the closed-form linear theory dispersion relation for the instability is presented. The electric field growth rates and oscillation frequencies specified by the dispersion relation provide concrete measures against which simulation results can be quantitatively compared. A fourth-order continuum kinetic algorithm is benchmarked against the instability, and is demonstrated to have good convergence properties and close agreement with theoretical growth rate and oscillation frequency predictions. Second-order accurate simulations are also shown to be consistent with theoretical predictions, but require higher resolution for convergence. The Dory–Guest–Harris instability benchmark extends the scope of current standard test problems by providing a substantive means of validating continuum kinetic simulations of magnetized plasmas in higher-dimensional 3D (x,vx,vy) phase space. The linear theory analysis, initial conditions, algorithm description, and comparisons between theoretical predictions and simulation results are presented.}
}
@article{FORTI2021605,
title = {Lightweight self-organising distributed monitoring of Fog infrastructures},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {605-618},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19334582},
author = {Stefano Forti and Marco Gaglianese and Antonio Brogi},
keywords = {Fog computing, Lightweight monitoring, Internet of Things, Peer-to-peer architectures},
abstract = {Monitoring will play an enabling role in the orchestration of next-gen Fog applications. Particularly, monitoring of Fog computing infrastructures should deal with platform heterogeneity, scarce resource availability at the edge, and high dynamicity all along the Cloud-IoT continuum. In this article, we describe FogMon, a C++ distributed monitoring prototype targeting Fog computing infrastructures. FogMon monitors hardware resources at different Fog nodes, end-to-end network QoS between such nodes, and connected IoT devices. Besides, it features a self-organising peer-to-peer topology with self-restructuring mechanisms, and differential monitoring updates, which ensure scalability, fault-tolerance and low communication overhead. Experiments on a real testbed show how the footprint of FogMon is limited and how its self-restructuring topology makes it resilient to infrastructure dynamicity.}
}
@article{HUI2014181,
title = {A high order homogenization model for transient dynamics of heterogeneous media including micro-inertia effects},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {273},
pages = {181-203},
year = {2014},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2014.01.028},
url = {https://www.sciencedirect.com/science/article/pii/S0045782514000516},
author = {Tong Hui and Caglar Oskay},
keywords = {Multiscale modeling, Computational homogenization, Composites, Wave dispersion, Homogenization},
abstract = {This manuscript presents a multi-dimensional, high order homogenization model for elastic composite materials subjected to dynamic loading conditions. The proposed model is derived based on the asymptotic homogenization method with multiple spatial scales. The high order homogenization model permits implementation using the standard finite elements and shape functions since it does not involve higher order terms typically present in dispersion models. The high order homogenization model can accurately capture wave dispersion in the presence of non-uniform density and non-uniform moduli within the material microstructure. Employing the Hybrid Laplace Transform/Finite Element Method, both displacement and traction boundary conditions for the macroscopic problem have been implemented. Finite element formulations for the first and second order influence functions, and the macroscale initial boundary value problem are presented. The performance of the model is verified by comparing model predictions to the local homogenization and the direct numerical simulations. The high order homogenization model is shown to predict the wave dispersion with very reasonable accuracy and cost. The proposed model can also capture the phononic bands – frequency bands within which the micro-inertia effects completely block wave propagation.}
}
@article{YU20061906,
title = {A simple mixed finite element for static limit analysis},
journal = {Computers & Structures},
volume = {84},
number = {29},
pages = {1906-1917},
year = {2006},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2006.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0045794906002495},
author = {X. Yu and F. Tin-Loi},
keywords = {Limit analysis, Mixed finite element, Plasticity, Collapse loads},
abstract = {In this paper, we investigate the behavior of a simple mixed finite element for the limit analysis of plane structures. In particular, its ability to overcome incompressibility locking in plane strain situations is investigated. The element is constructed from a piecewise constant displacement field and a piecewise bilinear stress field, and is used within a mathematical programming based discrete representation of the classical static formulation. Several benchmark examples of both plane stress and plane strain situations are solved to illustrate the predictive accuracy and to assess the large-scale capability of the element. The results are compared with those obtained by a recent sophisticated enhanced strain mixed element formulation.}
}
@article{STEFFENEL201872,
title = {Improving Data Locality in P2P-based Fog Computing Platforms},
journal = {Procedia Computer Science},
volume = {141},
pages = {72-79},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.151},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318015},
author = {Luiz Angelo Steffenel and Manuele Kirsch Pinheiro},
keywords = {fog computing, P2P overlay, data locality, big data, distributed hash table},
abstract = {Fog computing extends the Cloud Computing paradigm to the edge of the network, relying on computing services intelligently distributed to best meet the applications needs such as low communication latency, data caching or confidentiality reinforcement. While P2P is especially prone to implement Fog computing platforms, it usually lacks important elements such as controlling where the data is stored and who will handle the computing tasks. In this paper we propose both a mapping approach for data-locality and a location-aware scheduling for P2P-based middlewares, improving the data management performance on fog environments. Experimental results comparing the data access performances demonstrate the interest of such techniques.}
}
@article{GONZALEZVALVERDE2016249,
title = {Phenomenological modelling and simulation of cell clusters in 3D cultures},
journal = {Computers in Biology and Medicine},
volume = {77},
pages = {249-260},
year = {2016},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2016.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0010482516302177},
author = {I. González-Valverde and C. Semino and J.M. García-Aznar},
keywords = {3D cell cluster, Hybrid model, Tissue engineering, Multi-physics model, Oxygen transport},
abstract = {Cell clustering and aggregation are fundamental processes in the development of several tissues and the progression of many diseases. The formation of these aggregates also has a direct impact on the oxygen concentration in their surroundings due to cellular respiration and poor oxygen diffusion through clusters. In this work, we propose a mathematical model that is capable of simulating cell cluster formation in 3D cultures through combining a particle-based and a finite element approach to recreate complex experimental conditions. Cells are modelled considering cell proliferation, cell death and cell-cell mechanical interactions. Additionally, the oxygen concentration profile is calculated through finite element analysis using a reaction-diffusion model that considers cell oxygen consumption and diffusion through the extracellular matrix and the cell clusters. In our model, the local oxygen concentration in the medium determines both cell proliferation and cell death. Numerical predictions are also compared with experimental data from the literature. The simulation results indicate that our model can predict cell clustering, cluster growth and oxygen distribution in 3D cultures. We conclude that the initial cell distribution, cell death and cell proliferation dynamics determine the size and density of clusters. Moreover, these phenomena are directly affected by the oxygen transport in the 3D culture.}
}
@article{CELIK2010813,
title = {Unsupervised colour image segmentation using dual-tree complex wavelet transform},
journal = {Computer Vision and Image Understanding},
volume = {114},
number = {7},
pages = {813-826},
year = {2010},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2010.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1077314210000676},
author = {Turgay Celik and Tardi Tjahjadi},
keywords = {Unsupervised colour image segmentation, Multiscale edge detection, Dual-tree complex wavelet transform, Discrete wavelet transform, Multiscale analysis, Region growing, Colour difference},
abstract = {In this paper we present an effective unsupervised colour image segmentation algorithm which uses multiscale edge information and spatial colour content. The multiscale edge information is extracted using the dual-tree complex wavelet transform. Binary morphological operators are applied to the edge information to detect seed regions which are large enough to exclude boundary-only regions. The segmentation of homogeneous regions is obtained using region growing followed by region merging in the CIE L∗a∗b∗ colour space. We also present an edge preserving smoothing filter as a pre-process for the algorithm. We compare our algorithm with state-of-the-art algorithms and show its superior performance.}
}
@article{AMORES201941,
title = {Average-chain behavior of isotropic incompressible polymers obtained from macroscopic experimental data. A simple structure-based WYPiWYG model in Julia language},
journal = {Advances in Engineering Software},
volume = {130},
pages = {41-57},
year = {2019},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818310779},
author = {Víctor Jesús Amores and José María Benítez and Francisco Javier Montáns},
keywords = {Eight chain model, WYPiWYG hyperelasticity, Micromechanics, Julia},
abstract = {Elastomeric materials and soft biological tissues are made up of synthetic and protein fibers, respectively. The uncoiling of these fibers during loading produces a non-linear elastic macroscopic behavior in the regime of finite strains. Many hyperelastic models have been developed to reproduce this behavior assuming the existence of a strain energy function. In structure-based models, the analytical energy function is obtained from the stored energy of all the material constituents. This stored energy is given frequently by the entropy of the chain network obtained from Langevin statistical treatment of the possible configurations adopted by the chains, and a representative cell for their spatial distribution. One of the most used models is the eight chain model, being its salient feature that it reproduces the overall response of isotropic hyperelastic materials with only two material parameters obtained from a tensile test. On the other hand, in WYPiWYG hyperelasticity the stored energies are numerical instead of analytical and capture, to any precision, the experimental tests on the material. However, due to their phenomenological nature, their determination requires more tests. In this work, we develop a microstructure-based WYPiWYG hyperelastic model in which the average chain behavior is obtained from macroscopic tests through a simple automatic inverse procedure. We show that, without assuming a probability distribution function nor any particular chain arrangement, we obtain, at the same computational cost, better predictions than the 8-chain model. Code of the model and of the examples in the Julia programming language are included.}
}
@article{YOUSEFPOUR2019289,
title = {All one needs to know about fog computing and related edge computing paradigms: A complete survey},
journal = {Journal of Systems Architecture},
volume = {98},
pages = {289-330},
year = {2019},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1383762118306349},
author = {Ashkan Yousefpour and Caleb Fung and Tam Nguyen and Krishna Kadiyala and Fatemeh Jalali and Amirreza Niakanlahiji and Jian Kong and Jason P. Jue},
keywords = {Fog computing, Edge computing, Cloud computing, Internet of things (IoT), Cloudlet, Mobile edge computing, Multi-access edge computing, Mist computing},
abstract = {With the Internet of Things (IoT) becoming part of our daily life and our environment, we expect rapid growth in the number of connected devices. IoT is expected to connect billions of devices and humans to bring promising advantages for us. With this growth, fog computing, along with its related edge computing paradigms, such as multi-access edge computing (MEC) and cloudlet, are seen as promising solutions for handling the large volume of security-critical and time-sensitive data that is being produced by the IoT. In this paper, we first provide a tutorial on fog computing and its related computing paradigms, including their similarities and differences. Next, we provide a taxonomy of research topics in fog computing, and through a comprehensive survey, we summarize and categorize the efforts on fog computing and its related computing paradigms. Finally, we provide challenges and future directions for research in fog computing.}
}
@article{KHEBBEB2020101821,
title = {A Maude-Based rewriting approach to model and verify Cloud/Fog self-adaptation and orchestration},
journal = {Journal of Systems Architecture},
volume = {110},
pages = {101821},
year = {2020},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101821},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120301132},
author = {Khaled Khebbeb and Nabil Hameurlain and Faiza Belala},
keywords = {Self-adaptation, Orchestration, Fog computing, Cloud computing, Formal methods, Rewriting logic, Linear temporal logic, Maude},
abstract = {In the IoT-Fog-Cloud landscape, IoT devices are connected to numerous software applications in order to fully operate. Some applications are deployed on the Fog layer, providing low-latency access to resource, whilst others are deployed on the Cloud to provide important resource capabilities and process heavy computation. In this distributed landscape, the deployment infrastructure has to adapt to the highly dynamic requirements of the IoT layer. However, due to their intrinsic properties, the Fog layer may lack of providing sufficient amount of resource while the Cloud layer fails ensuring low-latency requirements. In this paper, we present a rewriting-based approach to design and verify the Cloud-Fog self-adaption and orchestration behaviors in order to manage infrastructure reconfiguration towards achieving low-latency and resources quantity trade-offs. We rely of the formal specification language Maude to provide an executable solution of these behaviors basing on the rewriting logic and we express properties with linear temporal logic (LTL) to qualitatively verify the adaptations correctness.}
}
@article{JEFERRY2015227,
title = {Challenges Emerging from Future Cloud Application Scenarios},
journal = {Procedia Computer Science},
volume = {68},
pages = {227-237},
year = {2015},
note = {1st International Conference on Cloud Forward: From Distributed to Complete Computing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.238},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915030835},
author = {Keith Jeferry and George Kousiouris and Dimosthenis Kyriazis and Jörn Altmann and Augusto Ciuffoletti and Ilias Maglogiannis and Paolo Nesi and Bojan Suzic and Zhiming Zhao},
keywords = {Cloud computing, future application scenarios, challenges, complete computing},
abstract = {The cloud computing paradigm encompasses several key differentiating elements and technologies, tackling a number of inefficiencies, limitations and problems that have been identified in the distributed and virtualized computing domain. Nonetheless, and as it is the case for all emerging technologies, their adoption led to the presentation of new challenges and new complexities. In this paper we present key application areas and capabilities of future scenarios, which are not tackled by current advancements and highlight specific requirements and goals for advancements in the cloud computing domain. We discuss these requirements and goals across different focus areas of cloud computing, ranging from cloud service and application integration, development environments and abstractions, to interoperability and relevant to it aspects such as legislation. The future application areas and their requirements are also mapped to the aforementioned areas in order to highlight their dependencies and potential for moving cloud technologies forward and contributing towards their wider adoption.}
}
