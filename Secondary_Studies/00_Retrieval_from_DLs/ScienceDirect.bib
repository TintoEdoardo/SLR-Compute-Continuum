@article{DAMSGAARD2023109872,
title = {Approximate computing in B5G and 6G wireless systems: A survey and future outlook},
journal = {Computer Networks},
volume = {233},
pages = {109872},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.109872},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623003171},
author = {Hans Jakob Damsgaard and Aleksandr Ometov and Md Munjure Mowla and Adam Flizikowski and Jari Nurmi},
keywords = {Approximate computing, Beyond 5G, Mobile networks},
abstract = {As modern 5G systems are being deployed, researchers question whether they are sufficient for the oncoming decades of technological evolution. Growing numbers of interconnected intelligent devices put these networks under tremendous pressure, demanding their development. Paving the way for beyond 5G and 6G systems, commonly denoted by B5G herein, therefore means seeking enablers to increase efficiency from different perspectives. One novel look on this is the application of inexact computations where nine 9s reliability is not needed, for example, in non-critical mobile broadband traffic. The paradigm of Approximate Computing (AxC) focuses on such areas where constrained quality degradation results in savings that benefit the users and operators. This paper surveys the state-of-the-art publications on the intersection of AxC and B5G systems, identifying and emphasizing trends and tendencies in existing work and directions for future research. The work highlights resource allocation algorithms as particularly mesmerizing in the former, while research related to Intelligent Reflective Surfaces appears the most prominent in the latter. In both, problems are often NP-hard and, thus, only solvable using heuristics or approximations, Successive Convex Approximation and Reinforcement Learning are most frequently applied.}
}
@article{ISHII2022103048,
title = {Precise path computation based on functional block-based disaggregation for future heterogeneous access-metro networks},
journal = {Optical Fiber Technology},
volume = {73},
pages = {103048},
year = {2022},
issn = {1068-5200},
doi = {https://doi.org/10.1016/j.yofte.2022.103048},
url = {https://www.sciencedirect.com/science/article/pii/S1068520022002310},
author = {Kiyo Ishii and Shu Namiki},
keywords = {Disaggregation, Network resource management, FBD model},
abstract = {Emerging 5G/6G mobile services are intended to cover a broad range of use cases, including applications requiring high bandwidth, low latency, and/or high reliability. To meet these diverse requirements, various optical network technologies and architectures, including optical node structures, transmission technologies, and virtualization technologies, have been extensively investigated. These novel technologies will be integrated to form a platform of future optical access-metro networks that support various mobile services. Such an optical access-metro platform will comprise heterogeneous node structures based on various optical functional blocks (e.g., wavelength selective switches, optical splitters, or arrayed waveguide gratings). To realize a versatile optical access-metro platform, a network resource management system that can universally handle heterogeneous node structures is indispensable. This study investigates the applicability of a functional block-based disaggregation (FBD) approach to such a resource-management system. Here, the previously proposed FBD model is extended to incorporate latency aware path computation. The results demonstrated that the FBD model could be successfully used with heterogeneous network structures, including both passive and switchable optical nodes. The precise path computation capability of the model was also demonstrated, and the scalability of the computation time was quantitatively evaluated in a parallel processing environment. Precise path computation can effectively consider both intra- and inter-node fiber connection lengths, which are useful for handling latency requirements based on an accurate representation of the propagation delay.}
}
@article{CHEN2022100017,
title = {A Review on Discrete Element Method Simulation in Laser Powder Bed Fusion Additive Manufacturing},
journal = {Chinese Journal of Mechanical Engineering: Additive Manufacturing Frontiers},
volume = {1},
number = {1},
pages = {100017},
year = {2022},
issn = {2772-6657},
doi = {https://doi.org/10.1016/j.cjmeam.2022.100017},
url = {https://www.sciencedirect.com/science/article/pii/S2772665722000071},
author = {Hui Chen and Yajing Sun and Weihao Yuan and Shengyong Pang and Wentao Yan and Yusheng Shi},
keywords = {Laser powder bed fusion, Discrete element method, Additive manufacturing, Modelling, Numerical simulation},
abstract = {Laser powder bed fusion (LPBF) is a popular metal additive manufacturing technique. Generally, the materials employed for LPBF are discrete and particulate metal matters. Thus, the discontinuous behaviors exhibited by the powder materials cannot be simulated solely using conventional continuum-based computational approaches, such as finite-element or finite-difference methods. The discrete element method (DEM) is a proven numerical method to model discrete matter, such as powder particles, by tracking the motion and temperature of individual particles. Recently, DEM simulation has gained popularity in LPBF studies. However, it has not been widely applied. This study reviews the existing applications of DEM in LPBF processing, such as powder spreading and fusion. A review of the existing literature indicates that DEM is a promising approach in the study of the kinetic and thermal fluid behaviors of powder particles in LPBF additive manufacturing.}
}
@article{VITALI2022423,
title = {Special issue on co-design of data and computation management in Fog Computing},
journal = {Future Generation Computer Systems},
volume = {129},
pages = {423-424},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004301},
author = {Monica Vitali and Pierluigi Plebani and David Bermbach and Erik Elmroth}
}
@article{KARL2023105049,
title = {On fully symmetric implicit closure approximations for fiber orientation tensors},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {318},
pages = {105049},
year = {2023},
issn = {0377-0257},
doi = {https://doi.org/10.1016/j.jnnfm.2023.105049},
url = {https://www.sciencedirect.com/science/article/pii/S0377025723000617},
author = {Tobias Karl and Matti Schneider and Thomas Böhlke},
keywords = {Fiber orientation, Composites, Closure approximation, Folgar–Tucker equation, Anisotropy, Homogenization},
abstract = {A novel closure approximation method for fiber orientation tensors is proposed namely the fully symmetric implicit closure. Besides the full index symmetry, implicitly formulated closures based on the contraction condition fulfill the trace condition and the trace-preserving property of the Folgar–Tucker equation. As a first example, the fully symmetric implicit quadratic closure is considered as a simple modification of a recently proposed symmetric quadratic closure. It is shown that this closure can be realized by a fiber orientation distribution function. Secondly, the fully symmetric implicit hybrid closure is proposed as a counter-example of a closure not being based on a orientation distribution function. Both closures are compared against classical approximations in view of orientation evolution in a simple shear flow. Furthermore, the capability of predicting the effective viscous and elastic behavior of fiber suspensions and solid composites is investigated for a given fiber orientation state. The results show that the proposed implicit closures can be used to approximate the maximum entropy closure. Thereby, both the quadratic and the hybrid approach alleviate the high computational burden of the maximum entropy closure, as their evaluation requires solving a one-dimensional problem only. In addition, the predicted effective behavior based on the implicit closures shows an overall good agreement with predictions based on measured orientation data.}
}
@article{LU2022103391,
title = {A cohesive viscoelastic-elastoplastic-damage model for DEM and its applications to predict the rate- and time-dependent behaviour of asphalt concretes},
journal = {International Journal of Plasticity},
volume = {157},
pages = {103391},
year = {2022},
issn = {0749-6419},
doi = {https://doi.org/10.1016/j.ijplas.2022.103391},
url = {https://www.sciencedirect.com/science/article/pii/S074964192200170X},
author = {Dai Xuan Lu and Nhu H.T. Nguyen and Ha H. Bui},
keywords = {Discrete element method, Asphalt concrete, Viscoelastic, Cohesive, Plasticity, Damage, Rate effect, SCB test, Creep, Relaxation},
abstract = {Asphalt concrete is a composite heterogeneous material comprised of aggregates bonded together by bitumen binder. The mechanical behaviour of this material shows great dependence on loading rates and time (e.g., creep, relaxation). Such heterogeneity in the internal structure and complex behaviour of asphalt concrete present a challenge for numerical methods to fully capture its responses under different loading rates and durations, especially when large deformation and crack development are present in the material. This study proposes a modelling approach capable of capturing the behaviour of asphalt concrete under different loading rates and durations. In this approach, the heterogeneous internal structure of asphalt concrete is naturally reproduced by the discrete element method (DEM). In parallel, a new inter-particle contact model is developed for the DEM to describe the grain-level behaviour of asphalt concrete. This contact model couples a viscoelastic-damage law with a cohesive-elastoplastic-damage law, enabling the contact model to characterise the rate and time dependency, viscoelastic damage, and plastic-damage behaviour of asphalt concrete. Moreover, thanks to the use of DEM, the proposed approach can naturally capture crack initiation and development. Through comparisons and verifications with a wide range of experiments on asphalt concrete, including the relaxation test, creep test, and dynamic semi-circular bending test, the proposed approach shows its capability of overcoming the limitations of previous DEM models in reproducing the complex behaviours of asphalt material under various loading conditions. Insights into the failure mechanisms of asphalt material under complex loading conditions and its transitional behaviours from diffuse to localised failure can be thus naturally derived from the proposed DEM approach. This study demonstrates the effectiveness of the proposed approach for investigating the behaviour of asphalt concrete.}
}
@article{YEH2022100,
title = {Realizing dynamic resource orchestration on cloud systems in the cloud-to-edge continuum},
journal = {Journal of Parallel and Distributed Computing},
volume = {160},
pages = {100-109},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521002045},
author = {Tsozen Yeh and Shengchieh Yu},
keywords = {Cloud computing, Edge computing, Hadoop, HDFS},
abstract = {Cloud computing has been widely utilized to handle the huge volume of data from many cutting-edge research areas such as Big Data and Internet of Things (IoT). The fast growing of edge devices makes it difficult for cloud systems to process all data and jobs originating from edge devices, which leads to the development of edge computing by completing jobs on edges instead of clouds. Unfortunately, edge devices generally possess only limited computing power. Therefore, jobs demanding heavy computation under strict time constraints could have more difficulties to successfully complete their work on edges than on clouds in the Cloud-to-Edge continuum. If cloud systems could dynamically orchestrate cloud resources to expedite the execution of those jobs, not only their timely execution could be assured, also the loading of edge devices could be reduced. The Apache Hadoop is considered one of the most popular cloud systems in industry and academia. However, it does not support dynamic resource allocation. Previously we proposed and implemented a new model which can dynamically adjust the computing resources assigned to given jobs in the Hadoop cloud system to speed up their execution. Like other computer software, cloud systems completely rely on their underlying operating systems to access hardware components such as CPUs and hard drives. In this paper, we report our efforts to improve our model to collaborate with the Linux operating system to accelerate the execution of jobs with high priority to a greater extent. Compared with what our original model achieved, experiments show that our ameliorated model could further quicken the execution of prioritized jobs in Hadoop by up to around 21%. As a result, jobs from edges that require substantial computing resources promptly could have better chances to get accomplished on cloud systems.}
}
@article{JALALIKHALILABADI2023100550,
title = {Task scheduling in fog environment — Challenges, tools & methodologies: A review},
journal = {Computer Science Review},
volume = {48},
pages = {100550},
year = {2023},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2023.100550},
url = {https://www.sciencedirect.com/science/article/pii/S1574013723000175},
author = {Zahra {Jalali Khalil Abadi} and Najme Mansouri and Mahshid Khalouie},
keywords = {Fog computing, Cloud computing, Task scheduling, Literature review},
abstract = {Even though cloud computing offers many advantages, it can be a poor choice sometimes because of its slow response to existing requests, leading to the need for fog computing. Scheduling tasks in a fog environment is a major challenge. It is important that IoT clients execute their tasks in a timely manner and obtain lower-cost services; however, they are also looking for tasks to be executed in a secure manner. In this paper, we review the advantages, limitations, and issues associated with scheduling algorithms proposed by a number of different researchers for fog environments. For fog computing developers, we compare different simulation tools to help them choose the product that is most appropriate and flexible for simulating the application they are considering. Finally, open issues and promising research directions associated with task scheduling in fog computing are discussed.}
}
@article{HANNOUSSE2021100415,
title = {Securing microservices and microservice architectures: A systematic mapping study},
journal = {Computer Science Review},
volume = {41},
pages = {100415},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100415},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000551},
author = {Abdelhakim Hannousse and Salima Yahiouche},
keywords = {Microservices, Microservice architectures, Security, Systematic mapping},
abstract = {Microservice architectures (MSA) are becoming trending alternatives to existing software development paradigms notably for developing complex and distributed applications. Microservices emerged as an architectural design pattern aiming to address the scalability and ease the maintenance of online services. However, security breaches have increased threatening availability, integrity and confidentiality of microservice-based systems. A growing body of literature is found addressing security threats and security mechanisms to individual microservices and microservice architectures. The aim of this study is to provide a helpful guide to developers about already recognized threats on microservices and how they can be detected, mitigated or prevented; we also aim to identify potential research gaps on securing MSA. In this paper, we conduct a systematic mapping in order to categorize threats on MSA with their security proposals. Therefore, we extracted threats and details of proposed solutions reported in selected studies. Obtained results are used to design a lightweight ontology for security patterns of MSA. The ontology can be queried to identify source of threats, security mechanisms used to prevent each threat, applicability layer and validation techniques used for each mechanism. The systematic search yielded 1067 studies of which 46 are selected as primary studies. The results of the mapping revealed an unbalanced research focus in favor of external attacks; auditing and enforcing access control are the most investigated techniques compared with prevention and mitigation. Additionally, we found that most proposed solutions are soft-infrastructure applicable layer compared with other layers such as communication and deployment. We also found that performance analysis and case studies are the most used validation techniques of security proposals.}
}
@article{COLONNELLI2022282,
title = {Distributed workflows with Jupyter},
journal = {Future Generation Computer Systems},
volume = {128},
pages = {282-298},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003976},
author = {Iacopo Colonnelli and Marco Aldinucci and Barbara Cantalupo and Luca Padovani and Sergio Rabellino and Concetto Spampinato and Roberto Morelli and Rosario {Di Carlo} and Nicolò Magini and Carlo Cavazzoni},
keywords = {Distributed computing, Jupyter notebooks, Streamflow, Workflow management systems},
abstract = {The designers of a new coordination interface enacting complex workflows have to tackle a dichotomy: choosing a language-independent or language-dependent approach. Language-independent approaches decouple workflow models from the host code’s business logic and advocate portability. Language-dependent approaches foster flexibility and performance by adopting the same host language for business and coordination code. Jupyter Notebooks, with their capability to describe both imperative and declarative code in a unique format, allow taking the best of the two approaches, maintaining a clear separation between application and coordination layers but still providing a unified interface to both aspects. We advocate the Jupyter Notebooks’ potential to express complex distributed workflows, identifying the general requirements for a Jupyter-based Workflow Management System (WMS) and introducing a proof-of-concept portable implementation working on hybrid Cloud-HPC infrastructures. As a byproduct, we extended the vanilla IPython kernel with workflow-based parallel and distributed execution capabilities. The proposed Jupyter-workflow (Jw) system is evaluated on common scenarios for High Performance Computing (HPC) and Cloud, showing its potential in lowering the barriers between prototypical Notebooks and production-ready implementations.}
}
@article{GOSCINSKI2023157,
title = {Special issue on Distributed Intelligence at the Edge for the Future Internet of Things},
journal = {Journal of Parallel and Distributed Computing},
volume = {171},
pages = {157-162},
year = {2023},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S074373152200209X},
author = {Andrzej Goscinski and Flavia C. Delicato and Giancarlo Fortino and Anna Kobusińska and Gautam Srivastava}
}
@article{DONG2022103462,
title = {A peridynamic approach to solving general discrete dislocation dynamics problems in plasticity and fracture: Part II. Applications},
journal = {International Journal of Plasticity},
volume = {159},
pages = {103462},
year = {2022},
issn = {0749-6419},
doi = {https://doi.org/10.1016/j.ijplas.2022.103462},
url = {https://www.sciencedirect.com/science/article/pii/S0749641922002406},
author = {Wenbo Dong and Hengjie Liu and Juan Du and Xu Zhang and Minsheng Huang and Zhenhuan Li and Ziguang Chen and Florin Bobaru},
keywords = {Discrete dislocation dynamics, Peridynamics, Plasticity, Indentation, Fracture, Hall-Petch effect},
abstract = {In Part II of this work we extend the method introduced in Part I to consider dislocation dynamic evolution through dislocations nucleation, glide, pile-up, and annihilation. The SP DDD-PD scheme is employed to investigate uniaxial tension in a single crystal and a polycrystal and verify its accuracy. The model is then used to simulate elastoplastic fracture by considering interactions between dislocations and crack growth. For Mode I elastoplastic fracture in a single crystal, we observe that the crack path is “attracted” towards regions of high density of gliding dislocations, leading to an undulating crack paths, as observed in experiments but never replicated by continuum-level computational models before. Tests on different sample sizes show how the proximity of constraints to the crack tip can lead to plastic hardening. Ductile-to-brittle transition happens naturally in this model when the crack, under Mode I displacement-controlled loading, approaches a free edge. A new way to calibrate the critical bond strain based on the material toughness or fracture energy is proposed. The present SP DDD-PD scheme can be used to investigate complicated elastoplastic fracture problems in which the interaction between dislocation motion and damage is critical.}
}
@article{HOSSAIN2023,
title = {The role of microservice approach in edge computing: Opportunities, challenges, and research directions},
journal = {ICT Express},
year = {2023},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2023.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S2405959523000760},
author = {Md. Delowar Hossain and Tangina Sultana and Sharmen Akhter and Md Imtiaz Hossain and Ngo Thien Thu and Luan N.T. Huynh and Ga-Won Lee and Eui-Nam Huh},
keywords = {Edge computing, Microservices, Monolithic architectures, Microservice security, AI},
abstract = {Edge computing has emerged as a promising computing paradigm that enables real-time data processing and analysis closer to the data source and boosts decision-making applications in a safe manner. On the other hand, the microservice is a new type of architecture that can be dynamically deployed, migrating across edge clouds on demand. Therefore, the combination of these two technologies can provide numerous benefits, including improved performance, reduced latency, and better resource utilization. In this paper, we present a thorough analysis of state-of-the-art research on the use of microservices in edge computing environments. We take into consideration several distinct microservice research directions, including coordination, orchestration, repositories, scheduling, autoscaling, deployment, resource management, and different security issues. Furthermore, we explore the potential applications of microservices in edge computing across various domains. Finally, the unsolved research issues and future directions of emerging trends in this area are also discussed.}
}
@article{WANG2023105529,
title = {A coupled FEM-DEM study on mechanical behaviors of granular soils considering particle breakage},
journal = {Computers and Geotechnics},
volume = {160},
pages = {105529},
year = {2023},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2023.105529},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X23002860},
author = {Yu Wang and Jia-Yan Nie and Shiwei Zhao and Hao Wang},
keywords = {Coupled FEM-DEM modeling, Particle breakage, Biaxial compression test, Particle strength, Particle shape, Confining pressure},
abstract = {Particle breakage often occurs in the fields of geological, geotechnical and hydraulic engineering like rock avalanche, landslides and high earth-rock dam, which affects the design and countermeasures of relevant engineering structures. Investigation of particle breakage’s effect on mechanical behaviors of granular soils has always been the hotspot within the community of particulate materials. In this study, a coupled finite element method (FEM) and discrete element method (DEM) modeling has been developed to tackle boundary value problems (BVPs) related with particle breakage. In the method, the particle breakage simulation is implemented in DEM through the conventional bonded particle modeling (BPM), while the stress–strain characteristics of representative volume element (RVE) captured by DEM is directly transmitted into the FEM solver to avoid the complicated and phenomenological constitutive equations incorporating into particle breakage. As an illustration, we apply the proposed multiscale modeling to investigating mechanical behaviors of quartz sands subjected to particle breakage through the high confining pressure biaxial compression simulation. And effects of particle strength, particle shape and confining pressure on particle breakage characteristics and further mechanical behaviors of quartz sands are also discussed accompanied with the underlying micromechanisms. Results demonstrate that the coupled FEM-DEM modeling can be able to deal with the complex geomechanics problems involving particle breakage.}
}
@article{JIANG2022103271,
title = {Multi-scale Crystal Viscoplasticity Approach for Estimating Anisotropic Steady-State Creep Properties of Single-Crystal SnAgCu Alloys},
journal = {International Journal of Plasticity},
volume = {153},
pages = {103271},
year = {2022},
issn = {0749-6419},
doi = {https://doi.org/10.1016/j.ijplas.2022.103271},
url = {https://www.sciencedirect.com/science/article/pii/S0749641922000547},
author = {Q. Jiang and A. Deshpande and A. Dasgupta},
keywords = {Crystal viscoplasticity, Anisotropic steady-state creep, Grain orientation, Lead-free solder, Hill-Garofalo formulation},
abstract = {Creep can have a significant impact on the reliability of Sn-based solder alloys even at room temperature because of their relatively low melting temperatures. SnAgCu (SAC) solder joints used in electronic packaging typically consist of only a few highly anisotropic grains, which consist of Sn dendrites embedded in a near-eutectic Sn-Ag component. The unique grain structure of each joint leads to stochastic variations in the thermomechanical response of such joint. Therefore, grain-scale testing and modeling are recommended to better characterize the anisotropic behavior and to estimate the influence of stochastic variability of grain structure on the viscoplastic response of the joint. This work aims to investigate the anisotropic steady-state creep behavior of single-crystal SAC solder joints. The orientation-dependent viscoplastic behavior of individual SAC grains is modeled with a multi-scale crystal-viscoplasticity approach for representing the relevant dislocation mechanics. The response predicted by the crystal viscoplasticity model is then captured in an equivalent homogenized continuous-scale constitutive model (Hill-Garofalo formulation). This modeling strategy was implemented in numerical simulations, as an illustrative example, to analyze the behavior of single-grain solder joints subjected to combined steady compression (from heat-sink clamping force) and thermal cyclic loading. The cyclic ratcheting in the presence of the steady compressive force causes: (i) transverse expansion of the solder ball, potentially leading to eventual short circuits; and (ii) cyclic fatigue damage leading to eventual failure of the solder joint. The proposed grain-scale modeling approach is shown to be able to address the stochastic variability of both these damage mechanisms, as a function of grain orientation. The present methodology can be used to predict the realistic behavior of solder joints, based on their microstructure, and provide valuable insights for their reliability analysis.}
}
@article{ZHANG2023105432,
title = {A new creep contact model for frozen soils and its application},
journal = {Computers and Geotechnics},
volume = {159},
pages = {105432},
year = {2023},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2023.105432},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X23001891},
author = {Ge Zhang and Enlong Liu and Ruihong Wang and Bingtang Song},
keywords = {Frozen soils, Contact model, Discrete element simulation, Creep mechanical properties, Secondary development},
abstract = {To study the bond effect on the creep mechanical properties of frozen soils, a creep contact model for frozen soils is proposed based on the generalized Kelvin body. Tensile and shear strength micro-parameters were introduced to reflect the cementation properties of frozen soils. A subroutine (DLL), called by the particle flow program (PFC3D), was generated using the language C++. Triaxial creep tests were carried out on frozen soils under different axial stress conditions. A comparison of the numerical simulation and laboratory test results showed that the proposed contact model can well reflect the creep mechanical properties of frozen soils. Based on the calibrated model micro-parameters, a series of triaxial creep simulations were carried out to study the effects of the cementation state, confining pressure, and damage state on the creep mechanical properties. The simulation results showed that the cementation state, confining pressure, and damage state have important influences on the creep mechanical properties of frozen soils. The characteristics of the creep curves, number of cracks, and number of force chains with the creep time were thoroughly analyzed. These results can provide a basis for studies on macro-meso creep mechanical properties of frozen soils.}
}
@article{ISLAM202346,
title = {Optimal placement of applications in the fog environment: A systematic literature review},
journal = {Journal of Parallel and Distributed Computing},
volume = {174},
pages = {46-69},
year = {2023},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522002465},
author = {Mohammad Mainul Islam and Fahimeh Ramezani and Hai Yan Lu and Mohsen Naderpour},
keywords = {Fog computing, Application placement, Service placement, Resource management},
abstract = {The fog-computing paradigm complements cloud computing to support the deployment and execution of latency-sensitive applications at the network edge by offering enhanced computational power. Optimal placement of such applications over a fog network comprising geographically distributed, heterogeneous, and resource-constrained fog nodes is a core challenge in fog-computing paradigm research. This study systematically reviews existing research on optimal fog application placement over the cloud-to-thing continuum. Surveyed articles are analyzed in four aspects: i) layers of the cloud-to-thing continuum considered for placing an application; ii) application characteristics that are considered in making placement decisions; iii) application placement mechanism; iv) tools and technology for placing an application. This review also categorizes the research problems associated with fog application placement. Finally, based on this review, we suggest directions for future adaptive fog-application placement research.}
}
@article{PATEL2023105910,
title = {Molecular mechanics and failure mechanisms in B. mori Silk Fibroin-hydroxyapatite composite interfaces: Effect of crystal thickness and surface characteristics},
journal = {Journal of the Mechanical Behavior of Biomedical Materials},
volume = {143},
pages = {105910},
year = {2023},
issn = {1751-6161},
doi = {https://doi.org/10.1016/j.jmbbm.2023.105910},
url = {https://www.sciencedirect.com/science/article/pii/S1751616123002631},
author = {Mrinal Patel and Satinder Paul Singh and Devendra Kumar Dubey},
keywords = { silk, Hydroxyapatite, Silk Fibroin, Interfacial interaction, Molecular dynamics, Interface debonding},
abstract = {Bombyx mori Silk Fibroin-hydroxyapatite (B. mori SF-HA) bio-nanocomposite is a prospective biomaterial for tissue engineered graft for bone repair. Here, B. mori SF is primarily a soft and tough organic phase, and HA is a hard and stiff mineral phase. In biomaterial design, an understanding about the nanoscale mechanics of SF-HA interface, such as interfacial interaction and interface debonding mechanisms between the two phases is essential for obtaining required functionality. To investigate such nanoscale behavior, molecular dynamics method is a preferred approach. Present study focuses on understanding of the interface debonding mechanisms at SF-HA interface in B. mori SF-HA bio-nanocomposite at nanometer length scale. For this purpose, nanoscale atomistic models of SF-HA interface are also developed based on the HA crystal size and HA surface type (Ca2+ dominated and OH− dominated) in contact with SF. Mechanical behavior analysis of these SF-HA interface models under pull-out type test were performed using Molecular Dynamics (MD) simulations. Surface pull-off strength values in the range of 0.4–0.8 GPa were obtained for SF-HA interface models, for different HA crystal thicknesses, wherein, the pull-off strength values are found to increase with increase in HA thicknesses. Analyses show that deformation mechanisms in SF-HA interface deformation, is a combination of shear deformation in SF phase followed by disintegration of SF phase from HA block. Furthermore, higher rupture force values were obtained for SF-HA interface with Ca2+ dominated HA surface in contact with SF phase, indicating that SF protein has a higher affinity for Ca2+ dominated surface of HA phase. Current work contributes in developing an understanding of mechanistic interactions between organic and inorganic phases in B. mori SF-HA composite nanostructure.}
}
@article{ROBLESENCISO2023109476,
title = {A multi-layer guided reinforcement learning-based tasks offloading in edge computing},
journal = {Computer Networks},
volume = {220},
pages = {109476},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109476},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622005102},
author = {Alberto Robles-Enciso and Antonio F. Skarmeta},
keywords = {Internet of Things, Fog computing, Edge computing, Task offloading, Resource allocation, Markov decision process, Reinforcement learning, Q-learning},
abstract = {The breakthrough in Machine Learning (ML) techniques and the popularity of the Internet of Things (IoT) has increased interest in applying Artificial Intelligence (AI) techniques to the new paradigm of Edge Computing. One of the challenges in edge computing architectures is the optimal distribution of the generated tasks between the devices in each layer (i.e., cloud-fog-edge). In this paper, we propose to use Reinforcement Learning (RL) to solve the Task Assignment Problem (TAP) at the edge layer and then we propose a novel multi-layer extension of RL (ML-RL) techniques that allows edge agents to query an upper-level agent with more knowledge to improve the performance in complex and uncertain situations. We first formulate the task assignment process considering the trade-off between energy consumption and execution time. We then present a greedy solution as a baseline and implement our multi-layer RL proposal in the PureEdgeSim simulator. Finally several simulations of each algorithm are evaluated with different numbers of devices to verify scalability. The simulation results show that reinforcement learning solutions outperformed the heuristic-based solutions and our multi-layer approach can significantly improve performance in high device density scenarios.}
}
@article{ANISETTI202334,
title = {An assurance process for Big Data trustworthiness},
journal = {Future Generation Computer Systems},
volume = {146},
pages = {34-46},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001371},
author = {Marco Anisetti and Claudio A. Ardagna and Filippo Berto},
keywords = {Non-functional assurance, Big Data transparency, Trustworthiness, Security, Distributed systems, Monitoring},
abstract = {Modern (industrial) domains are based on large digital ecosystems where huge amounts of data and information need to be collected, shared, and analyzed by multiple actors working within and across organizational boundaries. This data-driven ecosystem poses strong requirements on data management and data analysis, as well as on data protection and system trustworthiness. However, although Big Data has reached its functional maturity and represents a key enabler for enterprises to compete in the global market, the assurance and trustworthiness of Big Data computations (e.g., security, privacy) are still in their infancy. While functionally appealing, Big Data does not provide a transparent environment with clear non-functional properties, impairing the users’ ability to evaluate its behavior and clashing with modern data-privacy regulations. In this paper, we present a novel assurance process for Big Data, which evaluates the Big Data pipelines, and the Big Data ecosystem underneath, to provide a comprehensive measure of their trustworthiness. To the best of our knowledge, this approach is the first attempt to address the general problem of Big Data trustworthiness in an holistic way. We experimentally evaluate our solution in a real Big Data Analytics-as-a-Service environment, first presenting a detailed walkthrough evaluation, and then showing its feasibility and negligible performance overhead (i.e., approx 1 min).}
}
@article{SOUZA2023446,
title = {EdgeSimPy: Python-based modeling and simulation of edge computing resource management policies},
journal = {Future Generation Computer Systems},
volume = {148},
pages = {446-459},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23002340},
author = {Paulo S. Souza and Tiago Ferreto and Rodrigo N. Calheiros},
keywords = {Simulation, Modeling, Edge computing, Resource management, Containers, Python},
abstract = {The increasing popularity of applications with tight latency requirements has motivated research on Edge Computing, which positions computing resources near data sources at the Internet’s edge. Despite the emergence of simulation tools that make prototype validation less complex, time-consuming, and expensive, researchers and practitioners still face significant challenges when developing resource management strategies for the edge, as existing simulators fall short in providing a fine-grained model of edge applications provisioning. To overcome this challenge, we propose EdgeSimPy, a simulation framework written in Python for modeling and evaluating resource management policies in Edge Computing environments. EdgeSimPy features a modular architecture that incorporates several functional abstractions for edge servers, network devices, and applications with built-in models for user mobility, application composition, and power consumption that allow the simulation of various scenarios. Furthermore, we propose a novel conceptual model that accurately represents the entire lifecycle of edge applications and ensures seamless integration with real application traces. In addition to submitting EdgeSimPy to an in-depth verification that checks the simulator implementation, we discuss case studies that show EdgeSimPy in action in different large-scale scenarios.}
}
@article{GARCIA2021792,
title = {Data-flow driven optimal tasks distribution for global heterogeneous systems},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {792-805},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002806},
author = {Jordi Garcia and Francesc Aguiló and Adrià Asensio and Ester Simó and Marisa Zaragozá and Xavi Masip-Bruin},
keywords = {Edge computing, Distributed computing, Heterogeneous systems, Task distribution, Task offloading, Resources allocation},
abstract = {As a result of advances in technology and highly demanding users expectations, more and more applications require intensive computing resources and, most importantly, high consumption of data distributed throughout the environment. For this reason, there has been an increasing number of research efforts to cooperatively use geographically distributed resources, working in parallel and sharing resources and data. In fact, an application can be structured into a set of tasks organized through interdependent relationships, some of which can be effectively executed in parallel, notably speeding up the execution time. In this work a model is proposed aimed at offloading tasks execution in heterogeneous environments, considering different nodes computing capacity connected through distinct network bandwidths, and located at different distances. In the envisioned model, the focus is on the overhead produced when accessing remote data sources as well as the data transfer cost generated between tasks at run-time. The novelty of this approach is that the mechanism proposed for tasks allocation is data-flow aware, considering the geographical location of both, computing nodes and data sources, ending up in an optimal solution to a highly complex problem. Two optimization strategies are proposed, the Optimal Matching Model and the Staged Optimization Model, as two different approaches to obtain a solution to the task scheduling problem. In the optimal model approach a global solution for all application’s tasks is considered, finding an optimal solution. Differently, the staged model approach is designed to obtain a local optimal solution by stages. In both cases, a mixed integer linear programming model has been designed intended to minimizing the application execution time. In the studies carried out to evaluate this proposal, the staged model provides the optimal solution in 76% of the simulated scenarios, while it also dramatically reduces the solving time with respect to optimal. Both models have pros and cons and, in fact, can be used together to complement each other. The optimal model finds the global optimal solution at high running time cost, which makes this model unpractical on some scenarios. The staged model instead, is faster enough to be used on those scenarios; however, the given solution might not be optimal in some cases.}
}
@article{PELLE2023122,
title = {P4-assisted seamless migration of serverless applications towards the edge continuum},
journal = {Future Generation Computer Systems},
volume = {146},
pages = {122-138},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001450},
author = {István Pelle and Francesco Paolucci and Balázs Sonkoly and Filippo Cugini},
keywords = {Serverless, FaaS, Edge, AWS Lambda, AWS Greengrass, P4},
abstract = {Serverless computing has recently been presented as an effective technology for handling short-lived compute tasks in the cloud. It has the potential of becoming an attractive option also in the context of edge computing where resource-aware deployment, constrained by both limited edge computing resources and experienced latency, plays a vital role. In this paper, we present and experimentally validate a framework that oversees serverless applications in an edge computing scenario. It completely automates serverless application deployment and provides hitless dynamic migration of application compute tasks between a pair of edge nodes, paving the way for handling significantly more complex cases. The framework relies on an integrated deployment, monitoring and offloading infrastructure that enhances AWS IoT Greengrass features and performance. Our implementation provides two separate options for relocating compute tasks by steering application traffic towards the most suitable node. One builds on an on-the-fly application component reconfiguration, while the other selects the suitable node through P4 in-network processing of resource metrics emitted by the nodes. Our experimental demonstration evaluates the migration performance using a latency-sensitive application decomposed to serverless functions. Results reveal extremely fast dynamic reconfiguration and traffic rerouting operations. The used methods avoid congestion peaks at the edge and show no end-to-end latency increase upon migration between the nodes.}
}
@article{WANG2022323,
title = {An edge–cloud integrated framework for flexible and dynamic stream analytics},
journal = {Future Generation Computer Systems},
volume = {137},
pages = {323-335},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002576},
author = {Xin Wang and Azim Khan and Jianwu Wang and Aryya Gangopadhyay and Carl Busart and Jade Freeman},
keywords = {Edge computing, Internet of Things (IoT), Cloud computing, Edge–cloud integration, Stream data analytics, Concept drift, Hybrid learning, Long short-term memory (LSTM)},
abstract = {With the popularity of Internet of Things (IoT), edge computing and cloud computing, more and more stream analytics applications are being developed including real-time trend prediction and object detection on top of IoT sensing data. One popular type of stream analytics is the recurrent neural network (RNN) deep learning model based time series or sequence data prediction and forecasting. Different from traditional analytics that assumes data are available ahead of time and will not change, stream analytics deals with data that are being generated continuously and data trend/distribution could change (a.k.a. concept drift), which will cause prediction/forecasting accuracy to drop over time. One other challenge is to find the best resource provisioning for stream analytics to achieve good overall latency. In this paper, we study how to best leverage edge and cloud resources to achieve better accuracy and latency for stream analytics using a type of RNN model called long short-term memory (LSTM). We propose a novel edge–cloud integrated framework for hybrid stream analytics that supports low latency inference on the edge and high capacity training on the cloud. To achieve flexible deployment, we study different approaches of deploying our hybrid learning framework including edge-centric, cloud-centric and edge–cloud integrated. Further, our hybrid learning framework can dynamically combine inference results from an LSTM model pre-trained based on historical data and another LSTM model re-trained periodically based on the most recent data. Using real-world and simulated stream datasets, our experiments show the proposed edge–cloud deployment is the best among all three deployment types in terms of latency. For accuracy, the experiments show our dynamic learning approach performs the best among all learning approaches for all three concept drift scenarios.}
}
@article{SICARI2022102822,
title = {Insights into security and privacy towards fog computing evolution},
journal = {Computers & Security},
volume = {120},
pages = {102822},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102822},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822002164},
author = {Sabrina Sicari and Alessandra Rizzardi and Alberto Coen-Porisini},
keywords = {Fog computing, Internet of things, Security, Privacy, Cloud computing, Fog networking},
abstract = {The incremental diffusion of the Internet of Things (IoT) technologies and applications represents the outcome of a world ever more connected by means of heterogeneous and mobile devices. IoT scenarios imply the presence of multiple data producers (e.g., sensors, actuators, RFID, NFC) and consumers (e.g., end-user devices, such as smartphones, tablets, and PCs). A variety of standards and protocols must cooperate to efficiently gather, process, and share the information. The fog computing paradigm, due to its distributed nature, represents a viable solution to cope with interoperability, scalability, security, and privacy issues, which naturally emerge, since it operates as an intermediate layer between data consumers/producers and traditional cloud systems. This paper analyzes the evolution in the modeling of new methodologies, related to fog computing and IoT, showing how moving security and privacy tasks toward the edge of the network provide both advantages and new challenges to be faced in this research field. The proposed discussion provides an overview of requirements for the realization of secure and privacy-aware IoT-based fog computing infrastructures.}
}
@article{SENANAYAKE2022104862,
title = {An experiment-based cohesive-frictional constitutive model for cemented materials},
journal = {Computers and Geotechnics},
volume = {149},
pages = {104862},
year = {2022},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2022.104862},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X22002099},
author = {S.M.C.U. Senanayake and A. Haque and H.H. Bui},
keywords = {Constitutive model, Cemented soils, Discrete element method, X-Ray CT, Damage, Plasticity},
abstract = {Experiment-based damage laws at the contact bond level are often absent in the computational modelling of cemented materials using cohesive contact-bond models. In this paper, in-situ X-Ray Computed Tomography on element-scale cement bonds subjected to pure compression, shear and tensile loading was conducted to investigate their failure mechanisms and associated damage laws. A new cohesive-frictional constitutive model was subsequently developed using results obtained from X-Ray CT tests. The new constitutive model was capable of describing the behaviour of contact bonds subjected to complex loading conditions, including mixed-mode and compression damage. The new model was then implemented in an open-access Discrete Element Code (YADE) to simulate several challenging laboratory-scale tests (e.g., triaxial, Brazilian and Splitting tests) and very good agreements with the experimental data were achieved.}
}
@article{VEIGA20234,
title = {Towards containerized, reuse-oriented AI deployment platforms for cognitive IoT applications},
journal = {Future Generation Computer Systems},
volume = {142},
pages = {4-13},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22004320},
author = {Tiago Veiga and Hafiz Areeb Asad and Frank Alexander Kraemer and Kerstin Bach},
keywords = {Cognitive IoT, Self-adaptive IoT, Cognitive architecture, Container-based deployment},
abstract = {IoT applications with their resource-constrained sensor devices can benefit from adjusting their operations to the phenomena they sense and the environments they operate in, leading to the paradigm of self-adaptive, autonomous, or cognitive IoT. On the other side, current AI deployment platforms focus on the provision and reuse of machine learning models through containers that can be wired together to build new applications. The challenge is that composition mechanisms of the AI platforms, albeit effective due to their simplicity, are in fact too simplistic to support cognitive IoT applications, in which sensor devices also benefit from the machine learning results. Our objective is to perform a gap analysis between the requirements of cognitive IoT applications on the one side and the current functionalities of AI deployment platforms on the other side. In this work, we provide an overview of the paradigms in AI deployment platforms and the requirements of cognitive IoT applications. We study a use case for person counting in a skiing area through camera sensors, and how this use case benefits from letting the IoT sensors have access to operational knowledge in the form of visual attention models. We describe the implementation of the IoT application using an AI deployment platform, analyze its shortcomings, and necessary workarounds. From the use case, we identify and generalize five gaps that limit the usage of deployment platforms: the transparent management of multiple instances of components, a more seamless integration with IoT devices, explicit definition of data flow triggers, and the availability of templates for cognitive IoT architectures and reuse below the top-level.}
}
@article{ROSENDO202271,
title = {Distributed intelligence on the Edge-to-Cloud Continuum: A systematic literature review},
journal = {Journal of Parallel and Distributed Computing},
volume = {166},
pages = {71-94},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522000843},
author = {Daniel Rosendo and Alexandru Costan and Patrick Valduriez and Gabriel Antoniu},
keywords = {Edge computing, Distributed intelligence, Big Data Analytics, Computing Continuum, Reproducibility},
abstract = {The explosion of data volumes generated by an increasing number of applications is strongly impacting the evolution of distributed digital infrastructures for data analytics and machine learning (ML). While data analytics used to be mainly performed on cloud infrastructures, the rapid development of IoT infrastructures and the requirements for low-latency, secure processing has motivated the development of edge analytics. Today, to balance various trade-offs, ML-based analytics tends to increasingly leverage an interconnected ecosystem that allows complex applications to be executed on hybrid infrastructures where IoT Edge devices are interconnected to Cloud/HPC systems in what is called the Computing Continuum, the Digital Continuum, or the Transcontinuum. Enabling learning-based analytics on such complex infrastructures is challenging. The large scale and optimized deployment of learning-based workflows across the Edge-to-Cloud Continuum requires extensive and reproducible experimental analysis of the application execution on representative testbeds. This is necessary to help understand the performance trade-offs that result from combining a variety of learning paradigms and supportive frameworks. A thorough experimental analysis requires the assessment of the impact of multiple factors, such as: model accuracy, training time, network overhead, energy consumption, processing latency, among others. This review aims at providing a comprehensive vision of the main state-of-the-art libraries and frameworks for machine learning and data analytics available today. It describes the main learning paradigms enabling learning-based analytics on the Edge-to-Cloud Continuum. The main simulation, emulation, deployment systems, and testbeds for experimental research on the Edge-to-Cloud Continuum available today are also surveyed. Furthermore, we analyze how the selected systems provide support for experiment reproducibility. We conclude our review with a detailed discussion of relevant open research challenges and of future directions in this domain such as: holistic understanding of performance; performance optimization of applications; efficient deployment of Artificial Intelligence (AI) workflows on highly heterogeneous infrastructures; and reproducible analysis of experiments on the Computing Continuum.}
}
@article{LI2023105788,
title = {A multi-scale computational model for the passive mechanical behavior of right ventricular myocardium},
journal = {Journal of the Mechanical Behavior of Biomedical Materials},
volume = {142},
pages = {105788},
year = {2023},
issn = {1751-6161},
doi = {https://doi.org/10.1016/j.jmbbm.2023.105788},
url = {https://www.sciencedirect.com/science/article/pii/S1751616123001418},
author = {David S. Li and Emilio A. Mendiola and Reza Avazmohammadi and Frank B. Sachse and Michael S. Sacks},
keywords = {Myocardium mechanics, Image based modeling, Finite element modeling},
abstract = {We have previously demonstrated the importance of myofiber–collagen mechanical interactions in modeling the passive mechanical behavior of right ventricle free wall (RVFW) myocardium. To gain deeper insights into these coupling mechanisms, we developed a high-fidelity, micro-anatomically realistic 3D finite element model of right ventricle free wall (RVFW) myocardium by combining high-resolution imaging and supercomputer-based simulations. We first developed a representative tissue element (RTE) model at the sub-tissue scale by specializing the hyperelastic anisotropic structurally-based constitutive relations for myofibers and ECM collagen, and equi-biaxial and non-equibiaxial loading conditions were simulated using the open-source software FEniCS to compute the effective stress–strain response of the RTE. To estimate the model parameters of the RTE model, we first fitted a ’top-down’ biaxial stress–strain behavior with our previous structurally based (tissue-scale) model, informed by the measured myofiber and collagen fiber composition and orientation distributions. Next, we employed a multi-scale approach to determine the tissue-level (5 x 5 x 0.7 mm specimen size) RVFW biaxial behavior via ’bottom-up’ homogenization of the fitted RTE model, recapitulating the histologically measured myofiber and collagen orientation to the biaxial mechanical data. Our homogenization approach successfully reproduced the tissue-level mechanical behavior of our previous studies in all biaxial deformation modes, suggesting that the 3D micro-anatomical arrangement of myofibers and ECM collagen is indeed a primary mechanism driving myofiber–collagen interactions.}
}
@article{DOGANI2023120,
title = {Auto-scaling techniques in container-based cloud and edge/fog computing: Taxonomy and survey},
journal = {Computer Communications},
volume = {209},
pages = {120-150},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423002086},
author = {Javad Dogani and Reza Namvar and Farshad Khunjush},
keywords = {Cloud computing, Fog computing, Edge computing, Auto-scaling, Container-based virtualization},
abstract = {The long-held dream of computing as a service was realized with the emergence of cloud computing. Recently, fog and edge computing have been introduced as extensions of cloud networks, providing networking, processing, data management, and storage on network nodes near Internet of Things (IoT) devices to bridge the gap between the cloud and IoT devices. As the foundation of distributed computing, virtualization enables more effective use of physical computer hardware. Operating system virtualization through containers has recently been proposed as a promising alternative to virtual machines (VMs). Containers are lightweight packages that contain all application dependencies, system libraries, and third-party software packages. This research aims to review auto-scaling solutions for container-based virtualization in cloud and edge/fog computing applications. Auto-scaling plays a crucial role in the broad adoption of cloud computing by allocating and releasing computing resources in response to fluctuating resource requirements. However, designing and implementing an efficient auto-scaler for container-based applications in cloud and edge/fog computing presents challenges due to diverse application resource requirements and dynamic workload characteristics. Our research presents a comprehensive classification system for articles, covering key parameters such as auto-scaling techniques, experiments, workloads, and metrics, among others. We provide a detailed analysis of the results, offering valuable insights into open challenges and identifying promising directions for future research in this field.}
}
@article{DELUCIA2023207,
title = {Unlocking the potential of edge computing for hyperspectral image classification: An efficient low-energy strategy},
journal = {Future Generation Computer Systems},
volume = {147},
pages = {207-218},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001802},
author = {Gianluca {De Lucia} and Marco Lapegna and Diego Romano},
keywords = {Hyperspectral classification, Edge Computing, Principal Component Analysis, GPU computing},
abstract = {Despite recent improvements, the computing capability of Edge Computing devices is still inferior to high-end servers, so special methodologies are required to consider the computing environment while developing algorithms. In the present work, we propose a hybrid technique to make the classification of Hyperspectral Images feasible and effective through a Convolutional Neural Network on low-power and high-performance sensor devices. More specifically, we combine two strategies: we initially use the Principal Component Analysis method to discard non-significant wavelengths and shrink the dataset; then, we apply a process acceleration method to boost performance by implementing a form of GPU-based parallelism. The experiments demonstrate the technique’s effectiveness in terms of performance and energy consumption: it enables correct classifications even with low-power devices often deployed on Unmanned Aerial Vehicles, where the network connection is unpredictable or erratic.}
}
@article{QIN2021124741,
title = {A discrete-continuum coupled numerical method for fracturing behavior in concrete dams considering material heterogeneity},
journal = {Construction and Building Materials},
volume = {305},
pages = {124741},
year = {2021},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2021.124741},
url = {https://www.sciencedirect.com/science/article/pii/S095006182102496X},
author = {Xiangnan Qin and Jinjun Guo and Chongshi Gu and Xudong Chen and Bo Xu},
keywords = {Fracturing behavior, Concrete dams, Material heterogeneity, Cohesive element},
abstract = {This study aims to propose a systematic discrete-continuum coupled computational approach for fracturing behavior in concrete dams, with consideration of material heterogeneity. Innovative techniques are developed to realize batch insertion of cohesive element and parameter randomization of material property, and their feasibility and efficiency are verified with fracturing modelling of concrete specimens. Based on numerical investigation of hydraulic and thermal loads in discrete-continuum mesh scheme, a complete approach of fracturing modelling in concrete dams with stochastic material property is established. A case study with Chencun arch dam is conducted, and the numerical results exhibit high consistency with observational information, which demonstrates that the proposed method is feasible to characterize the mechanical performance and fracturing behavior of concrete dams with material heterogeneity.}
}
@article{YADAV2022105033,
title = {An atomistic-based finite deformation continuum membrane model for monolayer Transition Metal Dichalcogenides},
journal = {Journal of the Mechanics and Physics of Solids},
volume = {168},
pages = {105033},
year = {2022},
issn = {0022-5096},
doi = {https://doi.org/10.1016/j.jmps.2022.105033},
url = {https://www.sciencedirect.com/science/article/pii/S0022509622002137},
author = {Upendra Yadav and Susanta Ghosh},
keywords = {Crystal-elasticity, Molybdenum Disulfide, Transition Metal Dichalcogenide (TMD), Cauchy–Born rule, 2D materials},
abstract = {A finite-deformation crystal-elasticity membrane model for Transition Metal Dichalcogenide (TMD) monolayers is presented. Monolayer TMDs are multi-atom-thick two-dimensional (2D) crystalline membranes having atoms arranged on three parallel surfaces. In the present formulation, the deformed configuration of a TMD-membrane is represented through the deformation map of its middle surface and two stretches normal to the middle surface. Crystal-elasticity-based kinematic rules are employed to express the deformed bond lengths and bond angles of TMDs in terms of the continuum strains. The continuum hyper-elastic strain energy of the TMD membrane is formulated from its inter-atomic potential. The relative shifts between two simple lattices of TMDs are also considered in the constitutive relation. A smooth finite element framework using B-splines is developed to numerically implement the present continuum membrane model. The proposed model generalizes the crystal-elasticity-based membrane theory of purely 2D membranes, such as graphene, to the multi-atom-thick TMD crystalline membranes. The significance of relative shifts and two normal stretches are demonstrated through numerical results. The proposed atomistic-based continuum model accurately matches the material moduli, complex post-buckling deformations, and the equilibrium energies predicted by the purely atomistic simulations. It also accurately reproduces the experimental results for large-area TMD samples containing tens of millions of atoms.}
}
@article{BAO2023112610,
title = {The inclusion of scalar dissipation rate in modeling of an n-dodecane spray flame using flamelet generated manifold},
journal = {Combustion and Flame},
volume = {249},
pages = {112610},
year = {2023},
issn = {0010-2180},
doi = {https://doi.org/10.1016/j.combustflame.2022.112610},
url = {https://www.sciencedirect.com/science/article/pii/S0010218022006186},
author = {Hesheng Bao and Hayri Yigit Akargun and Dirk Roekaerts and Bart Somers},
keywords = {ECN Spray A, Flamelet generated manifold, Ignition, Soot, Scalar dissipation rate},
abstract = {In this work, an extension of the Flamelet Generated Manifold (FGM) method is developed suitable for igniting turbulent flames. To create the FGM, the strongly stretched flamelet equations (SSFE) are solved. Whereas in the standard basic method a single representative flamelet strain rate is used, in the new method a range of strain rates is taken into account. This allows including the effect of a varying turbulent scalar dissipation rate (SDR) during ignition. The new approach is validated by applying it in an Large Eddy Simulation (LES) of the Engine Combustion Network (ECN) Spray A turbulent flame for which detailed experimental data are available. First, in a priori validation step, the performance of the new extended FGM, the multi-strainrate FGM (mFGM), is validated by the simulation of ignition and species profiles in laminar flames along the so-called S-curve diagram and comparing with full chemistry calculations. The sub-grid scale (SGS) spray dispersion model is validated against the inert spray experiments in terms of vapor and liquid penetration as well as the spatial distribution of mixture fraction and its root mean square. Finally, the performance of the extended FGM is evaluated by comparison with the ECN Spray A flame. It is found that compared to the single-strain-rate FGM, the prediction of the ignition delay is improved considerably. This is related to the effect of the inclusion of the effect of the SDR, which is mainly on the second-stage ignition, i.e. the high-temperature chemistry. The low-temperature combustion is also affected as it occurs in richer mixtures than observed for the single-strain-rate FGM. Especially the formaldehyde, associated with low-temperature combustion, occurs in wider distribution. Finally, also predictions of soot evolution are studied. To improve the soot prediction capabilities, a new correction to the retrieved source term of the important pre-cursor, acetylene, is introduced. The above modeling developments have been made using a customized OpenFOAM solver developed by the authors. This work demonstrates the importance of including the SSFE SDR as independent parameter in an FGM based on igniting flamelets.}
}
@article{ZANELLA2022100663,
title = {BarMan: A run-time management framework in the resource continuum},
journal = {Sustainable Computing: Informatics and Systems},
volume = {35},
pages = {100663},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100663},
url = {https://www.sciencedirect.com/science/article/pii/S2210537922000075},
author = {Michele Zanella and Filippo Sciamanna and William Fornaciari},
keywords = {Fog computing, Resource management, Programming model, Task allocation strategy},
abstract = {Over the last years, the number of IoT devices has grown exponentially, highlighting the current Cloud infrastructure limitations. In this regard, Fog and Edge computing began to move part of the computation closer to data sources by exploiting interconnected devices as part of a single heterogeneous and distributed system in a computing continuum viewpoint. Since these devices are typically heterogeneous in terms of performance, features, and capabilities, this perspective should encompass programming models and run-time management layers. This work presents and evaluates the BarMan open-source framework by implementing a Fog video surveillance use-case. BarMan leverages a task-based programming model combined with a run-time resource manager and the novel BeeR framework to deploy the application’s tasks transparently. This enables the possibility of considering aspects related to the energy and power dissipated by the devices and the single application. Moreover, we developed a task allocation policy to maximize application performance, considering run-time aspects, such as load and connectivity, of the time-varying available devices. Through an experimental evaluation performed on a real cluster equipped with heterogeneous embedded boards, we evaluated different execution scenarios to show the framework’s functionality and the benefit of a distributed approach, leading up to an improvement of 66% on the frame processing latency w.r.t. a monolithic solution.}
}
@article{MIMRAN2022102890,
title = {Security of Open Radio Access Networks},
journal = {Computers & Security},
volume = {122},
pages = {102890},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102890},
url = {https://www.sciencedirect.com/science/article/pii/S016740482200284X},
author = {Dudu Mimran and Ron Bitton and Yehonatan Kfir and Eitan Klevansky and Oleg Brodt and Heiko Lehmann and Yuval Elovici and Asaf Shabtai},
keywords = {Open radio access networks, 5G, Risk assessment, Threat analysis},
abstract = {The Open Radio Access Network (O-RAN) is a promising radio access network (RAN) architecture aimed at reshaping the RAN industry toward an open, adaptive, and intelligent RAN. In this paper, we perform a comprehensive security analysis of O-RANs. Specifically, we review the architectural blueprint designed by the O-RAN Alliance, leader in the cellular ecosystem. As part of the security analysis, we provide a detailed overview of the O-RAN architecture; present an ontology for evaluating the security of a system that is currently at an early development stage; identify O-RAN’s high-risk areas; enumerate O-RAN’s threat actors; and model potential threats to O-RAN. The significance of this work is in providing an updated attack surface to cellular network operators. Based on the attack surface, cellular network operators can carefully deploy the appropriate countermeasures to improve O-RAN’s security.}
}
@article{PALLEWATTA2022121,
title = {QoS-aware placement of microservices-based IoT applications in Fog computing environments},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {121-136},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000206},
author = {Samodha Pallewatta and Vassilis Kostakos and Rajkumar Buyya},
keywords = {Fog computing, Microservice applications, Internet of Things, Application placement, QoS-awareness},
abstract = {The Fog computing paradigm, offering cloud-like services at the edge of the network, has become a feasible model to support computing and storage capabilities required by latency-sensitive and bandwidth-hungry Internet of Things (IoT) applications. As fog devices are distributed, heterogeneous and resource-constrained, efficient application scheduling mechanisms are required to harvest the full potential of such computing environments. Due to the rapid evolution in IoT ecosystems and also to better suit fog environment characteristics, IoT application development has moved from the monolithic architecture towards the microservices architecture that enhances scalability, maintainability and extensibility of the applications. This architecture improves the granularity of service decomposition, thus providing scope for improvement in QoS-aware placement policies. Existing application placement policies lack proper utilisation of these features of microservices architecture, thus failing to produce efficient placements. In this paper, we harvest the characteristics of microservice architecture to propose a scalable QoS-aware application scheduling policy for batch placement of microservices-based IoT applications within fog environments. Our proposed policy, QoS-aware Multi-objective Set-based Particle Swarm Optimisation (QMPSO), aims at maximising the satisfaction of multiple QoS parameters (makespan, budget and throughput) while focusing on the utilisation of limited fog resources. Besides, QMPSO adapts and improves the Set-based Comprehensive Learning Particle Swarm Optimisation (S-CLPSO) algorithm to achieve better convergence in the fog application placement problem. We evaluate our policy in a simulated fog environment. The results show that compared to the state-of-the-art solutions, our placement algorithm significantly improves QoS in terms of makespan satisfaction (up to 35% improvement) and budget satisfaction (up to 70% improvement) and ensures optimum usage of computing and network resources, thus providing a robust approach for QoS-aware placement of microservices-based heterogeneous applications within fog environments.}
}
@article{CAIAZZA2022213,
title = {Edge computing vs centralized cloud: Impact of communication latency on the energy consumption of LTE terminal nodes},
journal = {Computer Communications},
volume = {194},
pages = {213-225},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422002730},
author = {Chiara Caiazza and Silvia Giordano and Valerio Luconi and Alessio Vecchio},
keywords = {Edge computing, Energy saving, IoT communication},
abstract = {Edge computing brings several advantages, such as reduced latency, increased bandwidth, and improved locality of traffic. One aspect that is not sufficiently understood is to what extent the different communication latency experienced in the edge-cloud continuum impacts on the energy consumption of clients. We studied the energy consumption of a request–response communication scheme when an LTE node communicates with edge-based or cloud-based servers. Results show that the reduced latency of edge servers bring significant benefits in terms of energy consumption. Experiments also show how the energy savings brought by edge computing are influenced by the prevalent direction of data transfer (upload vs download), load of the server, and daytime/nighttime operation.}
}
@article{MARTIN202215,
title = {Kafka-ML: Connecting the data stream with ML/AI frameworks},
journal = {Future Generation Computer Systems},
volume = {126},
pages = {15-33},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.07.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002995},
author = {Cristian Martín and Peter Langendoerfer and Pouya Soltani Zarrin and Manuel Díaz and Bartolomé Rubio},
keywords = {Kafka-ML, Apache Kafka, Machine Learning, Artificial Intelligence, Data streams, Docker, Kubernetes, Distributed systems},
abstract = {Machine Learning (ML) and Artificial Intelligence (AI) depend on data sources to train, improve, and make predictions through their algorithms. With the digital revolution and current paradigms like the Internet of Things, this information is turning from static data to continuous data streams. However, most of the ML/AI frameworks used nowadays are not fully prepared for this revolution. In this paper, we propose Kafka-ML, a novel and open-source framework that enables the management of ML/AI pipelines through data streams. Kafka-ML provides an accessible and user-friendly Web user interface where users can easily define ML models, to then train, evaluate, and deploy them for inferences. Kafka-ML itself and the components it deploys are fully managed through containerization technologies, which ensure their portability, easy distribution, and other features such as fault-tolerance and high availability. Finally, a novel approach has been introduced to manage and reuse data streams, which may eliminate the need for data storage or file systems.}
}
@article{VELASQUEZ2022311,
title = {Resource orchestration in 5G and beyond: Challenges and opportunities},
journal = {Computer Communications},
volume = {192},
pages = {311-315},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422002213},
author = {Karima Velasquez and David {Perez Abreu} and Marilia Curado and Edmundo Monteiro},
keywords = {5G, 6G, Orchestration, Architecture, Network automation},
abstract = {5G networks have strict constraints regarding the services in terms of latency, reliability, and availability, which pose additional challenges to the traditional orchestration solutions, and 6G networks will increment the number of slices and services deployed over different technological domains, adding more difficulties for the orchestrator. Distributed and automated solutions will be essential for this context. This article identifies the main challenges in 5G/6G orchestration and then describes the utility of Artificial Intelligence-driven solutions, outlining an orchestrator architecture for 5G networks. The architecture is then explored as an orchestration solution for two ongoing research projects focused on the deployment of critical services over 5G networks.}
}
@article{HUANG2023104792,
title = {BIM and IoT data fusion: The data process model perspective},
journal = {Automation in Construction},
volume = {149},
pages = {104792},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104792},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523000523},
author = {Xiongwei Huang and Yongping Liu and Lizhen Huang and Erling Onstein and Christoph Merschbrock},
keywords = {Building information modeling, Internet of Things, Data fusion, Information fusion, Digital twin},
abstract = {Digitalization in the architecture, engineering, and construction (AEC) industry has highlighted the process of gathering and combining data from numerous sources. This paper uses data fusion from information science to investigate how data from information systems like Building Information Modeling (BIM) and the Internet of Things (IoT) could be coupled to enable a data-driven AEC. However, given the large amount of data, the technological diversity, the heterogeneous data schema, and hierarchical data abstraction, BIM and IoT data fusion are not addressed systematically. This study aims to develop a BIM and IoT data fusion framework to facilitate the appropriate data process flow for various applications. To investigate the research topic, an integrative review was conducted. Specifically, a general and tailored four-step data fusion process model was proposed as the review’s guiding framework. Critical evaluations were incorporated into a road map to provide a comprehensive perspective of BIM and IoT data fusion. In addition, based on the findings, a two-level and interconnected data fusion framework was proposed, which illustrates the data process flow and differentiates the data abstraction level, data processing technologies and purpose of the fusion. Furthermore, the fusion challenges and future trends are also highlighted.}
}
@article{KUMARI2022109137,
title = {Task offloading in fog computing: A survey of algorithms and optimization techniques},
journal = {Computer Networks},
volume = {214},
pages = {109137},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109137},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622002547},
author = {Nidhi Kumari and Anirudh Yadav and Prasanta K. Jana},
keywords = {Fog computing, Task offloading, Delay & latency, Energy & power consumption, Cost, Machine learning},
abstract = {The exponential growth in Internet of Things (IoT) devices and the limitations of cloud computing in terms of latency and quality of service for time-sensitive applications have led to the unfolding of the efficient middleware technology called fog. Fog computing circumvents the limitations of the cloud by creating a seamless continuum between the things/IoT/end-user devices and the cloud and reducing the effective distance. However, fog computing faces challenges for offloading tasks for their remote computation at some level. Hence, the optimality of task offloading is the primary research area in fog computing. Several contemporary papers exist on this important subject. The research gap in reviewing all these task offloading algorithms has motivated us for their presentation in the form of a detailed survey in this paper. There exist some survey papers which deal with the task offloading. However, none of them has covered the basics of optimization techniques and their solution approaches. The primary objective of this paper is to provide the readers with a complete overview of the journey from a task offloading idea to its mathematical problem formulation and finally to its solution with all details of optimization techniques. We begin by introducing fog computing, and task offloading process followed by several task offloading factors governing decision-making process and their surveys. A section is fully dedicated to the survey of offloading objectives with examples. We also present several optimization approaches used in task offloading. Finally, the last section dedicates to the challenges and future direction in fog computing. The outcomes of the survey will benefit readers in learning the optimization used in task offloading, and it will also provide them a systematic design of offloading scheme with specific objectives.}
}
@article{DEBAUCHE20227494,
title = {Cloud and distributed architectures for data management in agriculture 4.0 : Review and future trends},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {9},
pages = {7494-7514},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821002664},
author = {Olivier Debauche and Saïd Mahmoudi and Pierre Manneback and Frédéric Lebeau},
keywords = {Agriculture 4.0, Smart farming, Smart agriculture, Lambda architecture, Kappa architecture, Edge computing, Fog computing, Micro-service architecture, Data lake, Data house, Blockchain, Osmotic computing, Dew computing},
abstract = {The Agriculture 4.0, also called Smart Agriculture or Smart Farming, is at the origin of the production of a huge amount of data that must be collected, stored, and processed in a very short time. Processing this massive quantity of data needs to use specific infrastructure that use adapted IoT architectures. Our review offers a comparative panorama of Central Cloud, Distributed Cloud Architectures, Collaborative Computing Strategies, and new trends used in the context of Agriculture 4.0. In this review, we try to answer 4 research questions: (1) Which storage and processing architectures are best suited to Agriculture 4.0 applications and respond to its peculiarities? (2) Can generic architectures meet the needs of Agriculture 4.0 application cases? (3) What are the horizontal development possibilities that allow the transition from research to industrialization? (4) What are the vertical valuations possibilities to move from algorithms trained in the cloud to embedded or stand-alone products? For this, we compare architectures with 8 criteria (User Proximity, Latency & Jitter, Network stability, high throughput, Reliability, Scalability, Cost Effectiveness, Maintainability), and analyze the advantages and disadvantages of each of them.}
}
@incollection{PANTO202285,
title = {3 - Numerical modeling for the seismic assessment of masonry structures},
editor = {Tiago Ferreira and Hugo Rodrigues},
booktitle = {Seismic Vulnerability Assessment of Civil Engineering Structures At Multiple Scales},
publisher = {Woodhead Publishing},
pages = {85-126},
year = {2022},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-12-824071-7},
doi = {https://doi.org/10.1016/B978-0-12-824071-7.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240717000056},
author = {Bartolomeo Pantò and Ivo Caliò},
keywords = {Unreinforced masonry buildings, Cultural heritage, Architectural heritage, Non-linear static analyses, Seismic assessments, Performance-based approaches, Discrete elements, Macro-elements, Limit analysis},
abstract = {Historical unreinforced masonry buildings represent the largest percentage of architectural and cultural heritage in several seismic areas around the world. Past and recent events have demonstrated the high vulnerability of these constructions, which are often seriously damaged even in the occurrence of moderate magnitude earthquakes, with significant economic, social and human life losses. An effective seismic prevention action, planned in the medium or long term, is the only practicable way to reduce the dramatic effects of earthquakes on historical masonry constructions and to preserve their cultural value for future generations. This requires an in-depth knowledge of the dynamic response of masonry structures and reliable numerical tools for its prediction. In order to face this challenge, numerous experimental and numerical studies have been set up within the academic community over the last few decades. This effort has led to a new generation of numerical methods adopted by technical codes to guide engineers toward seismic assessments and retrofitting interventions. In this chapter, the most advanced numerical strategies and seismic assessment procedures are presented and critically discussed in the light of the current standards of engineering practices and potential future developments.}
}
@article{SCHMIDT2022104500,
title = {New double surface constitutive model of intermittent plastic flow applied to near 0 K adiabatic shear bands},
journal = {Mechanics of Materials},
volume = {175},
pages = {104500},
year = {2022},
issn = {0167-6636},
doi = {https://doi.org/10.1016/j.mechmat.2022.104500},
url = {https://www.sciencedirect.com/science/article/pii/S0167663622002642},
author = {Rafał Schmidt and Błażej Skoczeń and Jan Bielski and Elwira Schmidt},
keywords = {Plasticity, Shear bands, Intermittent plastic flow, Phase transformation, Cryogenic temperatures},
abstract = {One of the most interesting phenomena that occur in ductile materials strained in the proximity of absolute zero are the adiabatic shear bands. In the stainless steels, massively used to construct superconducting magnets, their occurrence can be detected by various techniques, including magnetometric methods (feritscope). Formation of adiabatic shear bands is strictly correlated to the occurrence of the intermittent plastic flow (IPF), characteristic of stainless steels strained in liquid or superfluid helium. Evidence for the occurrence of the phase transformation during nucleation and formation of the shear bands in the proximity of absolute zero is for the first time given. The rate of shear bands propagation along the sample as well as the amount of secondary phase is measured. In order to describe the intermittent plastic flow, novel double surface model has been developed. It includes type Huber-Mises-Hencky yield surface, and new recovery surface reflecting the lower bound for the stress oscillations. The yield surface can move and expand due to nonlinear mixed (kinematic and isotropic) hardening, resulting from the phase transformation, whereas, the recovery surface remains constant. The serrations occur between the yield surface and the recovery surface, with the yield surface coming back to its initial position after each serration. Each serration corresponds to formation of single shear band there, where the easy slip planes are available, and the mechanism of anchoring the shear bands by the secondary phase is explained. New double surface model has been correlated to the experimental data and applied to explain stress oscillations during the regular stage of the intermittent plastic flow, when the shear bands gradually cover the gauge length of the sample.}
}
@article{FORTAS2022102859,
title = {Formal verification of IoT applications using rewriting logic: An MDE-based approach},
journal = {Science of Computer Programming},
volume = {222},
pages = {102859},
year = {2022},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2022.102859},
url = {https://www.sciencedirect.com/science/article/pii/S0167642322000922},
author = {Abdelouahab Fortas and Elhillali Kerkouche and Allaoua Chaoui},
keywords = {Internet of things, Formal verification, Rewriting logic, Model checking, Model driven engineering},
abstract = {Internet of Things (IoT) systems are complex assemblies of components that collaborate to achieve common goals. These components are based on heterogeneous technologies, and they communicate using various communication protocols. This heterogeneity makes the design and the development of IoT applications a challenging issue. Diverse approaches based on Model-Driven Engineering (MDE) have been proposed to overcome this major issue using suitable modeling languages. ThingML is a promising UML profile for modeling IoT applications that aims to address the challenges of heterogeneity. However, ThingML does not have rigorous semantics, making it unsuitable for formal verification and analysis of system designs. This paper proposes an MDE-based formal approach to define the formal semantics of the ThingML language using rewriting logic and its language Maude. The main advantage of our approach over other approaches lies in the universality and versatility of Maude's mathematical notation, which implements all ThingML concepts and their behavioral aspects in a unified formal logic. The existing Maude language verification tools provide powerful analysis techniques, including simulation and model checking, which enable rigorous analysis and verification of ThingML designs. The contributions of this work include the following: (i) we propose a semantics mapping between ThingML concepts and Maude constructs, (ii) we define and implement an operational semantics for the ThingML action language in the Maude language, and (iii) we develop a tool that enables the automatic transformation of ThingML specifications into Maude. Our approach is illustrated through a case study.}
}
@article{AFZAL2023103581,
title = {A holistic survey of multipath wireless video streaming},
journal = {Journal of Network and Computer Applications},
volume = {212},
pages = {103581},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103581},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522002223},
author = {Samira Afzal and Vanessa Testoni and Christian Esteve Rothenberg and Prakash Kolan and Imed Bouazizi},
keywords = {Wireless video streaming, Multipath routing, Packet scheduling, Heterogeneous networks},
abstract = {Demand for wireless video streaming services increases with users expecting to access high-quality video streaming experiences. Ensuring Quality of Experience (QoE) is quite challenging due to varying bandwidth and time constraints. Since most of today’s mobile devices are equipped with multiple network interfaces, one promising approach is to benefit from multipath communications. Multipathing leads to higher aggregate bandwidth and distributing video traffic over multiple network paths improves stability, seamless connectivity, and QoE. However, most of current transport protocols do not match the requirements of video streaming applications or are not designed to address relevant issues, such as networks heterogeneity, head-of-line blocking, and delay constraints. In this comprehensive survey, we first review video streaming standards and technology developments. We then discuss the benefits and challenges of multipath video transmission over wireless. We provide a holistic literature review of multipath wireless video streaming, shedding light on the different alternatives from an end-to-end layered stack perspective, reviewing key multipath wireless scheduling functions, unveiling trade-offs of each approach, and presenting a suitable taxonomy to classify the state-of-the-art. Finally, we discuss open issues and avenues for future work.}
}
@article{CAMPIONI2023181,
title = {Enabling civil–military collaboration for disaster relief operations in smart city environments},
journal = {Future Generation Computer Systems},
volume = {139},
pages = {181-195},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003041},
author = {Lorenzo Campioni and Filippo Poltronieri and Cesare Stefanelli and Niranjan Suri and Mauro Tortonesi and Konrad Wrona},
keywords = {Humanitarian assistance and disaster recovery (HADR), Internet of things (ioT), Fog computing, Context-aware services, Distributed system, Location based services (LBS)},
abstract = {To address the aftermath of ever more frequent natural disasters striking highly-populated urban areas, Humanitarian Assistance and Disaster Relief (HADR) operations increasingly involve the coordinated efforts of multiple agencies and in particular Civil–Military Cooperation (CIMIC). In such scenarios, rescuers operate in a disrupted environment, must maintain a high operational tempo, and need to quickly make high-impact decisions. Effectively supporting emergency responders through IT services that implement accurate situation awareness presents a formidable set of challenges, including the discovery and integration of surviving IT assets with purposely deployed ones; effective information prioritization; resilient communications; and secure information sharing. We believe that in the highly digital modern urban environment, often referred to as smart cities, these challenges can be effectively addressed only by integrating by design HADR support into the smart city middleware. This paper presents Aceso — a proof-of-concept smart city middleware that provides location- and context-sensitive services with full support for HADR operations. Aceso provides a set of functions, ranging from resource discovery to secure information sharing, that can be quickly activated in case of unpredictable and adverse events to facilitate HADR operations. Furthermore, Aceso leverages the Value-of-Information (VoI) methodology to handle the processing and dissemination of mission-critical information. To validate the capabilities of Aceso, we devised a fictional HADR scenario, set in the city of Helsinki, Finland, that involves the collaboration of multiple responder teams with different roles. The validation results confirm Aceso’s usefulness in prioritizing the processing and dispatching of critical information and in realizing federation-wide sharing of this information among HADR teams.}
}
@article{ZHENG2023105,
title = {A package-aware scheduling strategy for edge serverless functions based on multi-stage optimization},
journal = {Future Generation Computer Systems},
volume = {144},
pages = {105-116},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23000547},
author = {Senjiong Zheng and Bo Liu and Weiwei Lin and Xiaoying Ye and Keqin Li},
keywords = {Serverless function offloading, Dependency package awareness, Package caching strategy},
abstract = {Serverless computing offers a promising deployment model for edge IoT applications. However, serverless functions that rely on large libraries suffer from severe library loading latency when containerized, which is unfriendly to edge latency-sensitive applications. Most function offload strategies in edge environments ignore the impact of this latency. We also found that the measures taken by serverless platforms to reduce loading latency may not work in edge environments. To remedy that, this paper proposes a function offloading strategy to minimize loading latency, a new way to deeply integrate placement optimization with cache optimization. In this way, we first design a package caching policy suitable for edge environments based on the consistency of execution topology. Then a Double Layers Dynamic Programming algorithm (DLDP) is proposed to solve the problem of function offloading considering the dependent packages using a multi-stage progressive optimization approach. The caching policy is embedded in the scheduling algorithm through a phased optimization approach to achieve joint optimization. Extensive experiments on the cluster trace from Alibaba show that DLDP reduces the loading latency of packages by more than 97.84% and significantly outperforms four baselines in the application completion time by more than 55.67%.}
}
@article{AHMAD2023100568,
title = {Deep learning models for cloud, edge, fog, and IoT computing paradigms: Survey, recent advances, and future directions},
journal = {Computer Science Review},
volume = {49},
pages = {100568},
year = {2023},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2023.100568},
url = {https://www.sciencedirect.com/science/article/pii/S1574013723000357},
author = {Shahnawaz Ahmad and Iman Shakeel and Shabana Mehfuz and Javed Ahmad},
keywords = {Deep learning, Cloud computing, Edge computing, Fog computing, IoT, Models},
abstract = {In recent times, the machine learning (ML) community has recognized the deep learning (DL) computing model as the Gold Standard. DL has gradually become the most widely used computational approach in the field of machine learning, achieving remarkable results in various complex cognitive tasks that are comparable to, or even surpassing human performance. One of the key benefits of DL is its ability to learn from vast amounts of data. In recent years, the DL field has witnessed rapid expansion and has found successful applications in various conventional areas. Significantly, DL has outperformed established ML techniques in multiple domains, such as cloud computing, robotics, cybersecurity, and several others. Nowadays, cloud computing has become crucial owing to the constant growth of the IoT network. It remains the finest approach for putting sophisticated computational applications into use, stressing the huge data processing. Nevertheless, the cloud falls short because of the crucial limitations of cutting-edge IoT applications that produce enormous amounts of data and necessitate a quick reaction time with increased privacy. The latest trend is to adopt a decentralized distributed architecture and transfer processing and storage resources to the network edge. This eliminates the bottleneck of cloud computing as it places data processing and analytics closer to the consumer. Machine learning (ML) is being increasingly utilized at the network edge to strengthen computer programs, specifically by reducing latency and energy consumption while enhancing resource management and security. To achieve optimal outcomes in terms of efficiency, space, reliability, and safety with minimal power usage, intensive research is needed to develop and apply machine learning algorithms. This comprehensive examination of prevalent computing paradigms underscores recent advancements resulting from the integration of machine learning and emerging computing models, while also addressing the underlying open research issues along with potential future directions. Because it is thought to open up new opportunities for both interdisciplinary research and commercial applications, we present a thorough assessment of the most recent works involving the convergence of deep learning with various computing paradigms, including cloud, fog, edge, and IoT, in this contribution. We also draw attention to the main issues and possible future lines of research. We hope this survey will spur additional study and contributions in this exciting area.}
}
@article{AHMAD2022123073,
title = {Two-equation continuum model of drying appraised by comparison with pore network simulations},
journal = {International Journal of Heat and Mass Transfer},
volume = {194},
pages = {123073},
year = {2022},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2022.123073},
url = {https://www.sciencedirect.com/science/article/pii/S0017931022005464},
author = {Faeez Ahmad and Marc Prat and Evangelos Tsotsas and Abdolreza Kharaghani},
keywords = {Two-equation continuum model, Pore network simulations, Scale transition, Non-local equilibrium effect, Porous media drying},
abstract = {A two-equation non-local-equilibrium (NLE) continuum model of isothermal drying is assessed by comparison with pore network simulations considering a rigid capillary porous medium that is fully saturated initially. This continuum model consists of a transport equation for the liquid and of a transport equation for the vapor. The two main variables are the liquid saturation and the vapor partial pressure. The two equations are coupled by a phase-change term and mass transport at the medium surface is modeled by considering the individual boundary conditions for the two continuum model equations. The macroscopic parameters that appear in the NLE continuum model include classical parameters such as the effective liquid and vapor diffusivities, as well as non-classical and new parameters such as the specific interfacial area and the fraction of dry surface pores. These parameters are determined for the porous microstructure corresponding to the cubic network used to perform the pore network simulations. The results obtained by the two-equation NLE continuum model are compared with pore network simulation data. Comparisons reveal that the two-equation NLE continuum model can capture with a reasonable degree of accuracy the NLE effect as well as the phase distributions and drying kinetics of the pore network model drying simulations.}
}
@article{SALIS202320,
title = {An Edge-Cloud based Reference Architecture to support cognitive solutions in Process Industry},
journal = {Procedia Computer Science},
volume = {217},
pages = {20-30},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.198},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022761},
author = {Antonio Salis and Angelo Marguglio and Gabriele {De Luca} and Silvia Razzetti and Walter Quadrini and Sergio Gusmeroli},
keywords = {Industry 4.0, process industry, smart manufacturing, reference architecture, cloud, edge computing, cognitive computing, artificial intelligence, big data analytics},
abstract = {Process Industry is one of the leading sectors of the world economy, characterized however by intense environmental impact, and very high-energy consumption. Despite a traditional low innovation pace in PI, in the recent years a strong push at worldwide level towards the dual objective of improving the efficiency of plants and the quality of products, significantly reducing the consumption of electricity and CO2 emissions has taken momentum. Digital Technologies (namely Smart Embedded Systems, IoT, Data, AI and Edge-to-Cloud Technologies) are enabling drivers for a Twin Digital-Green Transition, as well as foundations for human centric, safe, comfortable and inclusive workplaces. Currently, digital sensors in plants produce a large amount of data, which in most cases constitutes just a potential and not a real value for Process Industry, often locked-in in close proprietary systems and seldomly exploited. Digital technologies, with process modelling-simulation via digital twins, can build a bridge between the physical and the virtual worlds, bringing innovation with great efficiency and drastic reduction of waste. In accordance with the guidelines of Industrie 4.0 this work proposes a modular and scalable Reference Architecture, based on open source software, which can be implemented both in brownfield and greenfield scenarios. The ability to distribute processing between the edge, where the data have been created, and the cloud, where the greatest computational resources are available, facilitates the development of integrated digital solutions with cognitive capabilities. The reference architecture is being validated in the three pilot plants, paving the way to the development of integrated planning solutions, with scheduling and control of the plants, optimizing the efficiency and reliability of the supply chain, and balancing energy efficiency.}
}
@article{DICHOLKAR2022104863,
title = {Convergence enhancement of SIMPLE-like steady-state RANS solvers applied to airfoil and cylinder flows},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {220},
pages = {104863},
year = {2022},
issn = {0167-6105},
doi = {https://doi.org/10.1016/j.jweia.2021.104863},
url = {https://www.sciencedirect.com/science/article/pii/S0167610521003366},
author = {Antariksh Dicholkar and Frederik Zahle and Niels N. Sørensen},
keywords = {Computational fluid dynamics, Reynolds-averaged Navier–Stokes, Steady-state, Aerodynamics, Aerodynamic shape optimization, Wind turbine},
abstract = {Iterative steady Reynolds-averaged Navier–Stokes (RANS) solvers are widely used to obtain polars representing the aerodynamic characteristics of airfoils. Unsteadiness is encountered in the flow for many of the simulated angles of attack due to phenomenon such as vortex shedding and separation bubbles. In such cases, the steady RANS solver may become numerically unstable. As a consequence it is unable to find a converged solution, with the solver residuals often entering limit cycle oscillations. The BoostConv method is an alternative to the classical Newton’s method to compute unstable steady-state solutions of dynamical systems. It has been previously applied for obtaining steady laminar solutions of the Navier–Stokes equations. However, it demonstrated a lack of robustness by failing to always find the steady-state turbulent solutions of the RANS equations. To overcome this problem, we propose a modification in the application of the BoostConv method to compute steady-state turbulent solutions of the RANS equations. The modified BoostConv method is coupled with an iterative steady RANS solver for computing converged two-dimensional steady-state flows over airfoils. The proposed modification in the application procedure introduces a relaxation parameter that improves the convergence and robustness of the BoostConv method. We show this by finding lift and drag curves with angles of attack spanning 360°for airfoils representative of those used in modern wind turbines. The flow solutions are converged to machine precision at all angles of attack using the novel application procedure of the BoostConv method. Additionally, for flow cases convergent with the original BoostConv method, the modified BoostConv method showed faster convergence rates. The proposed modification provides an enhancement of the BoostConv method that can be easily integrated into existing implementations. By improving the method’s robustness and convergence rates in finding steady-state turbulent solutions of the RANS equations, it opens up the modified BoostConv method to be used in a wide variety of industrial flow cases.}
}
@article{GALLEGOMADRID2023556,
title = {The role of vehicular applications in the design of future 6G infrastructures},
journal = {ICT Express},
volume = {9},
number = {4},
pages = {556-570},
year = {2023},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2023.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S2405959523000383},
author = {Jorge Gallego-Madrid and Ramon Sanchez-Iborra and Jordi Ortiz and Jose Santa},
keywords = {6G, Vehicular communications, V2X, Applications},
abstract = {A great lack of 5G design is the traditional bottom-up development of network evolution, which has not effectively considered the requirements of applications and, particularly, vehicle to everything (V2X) applications. This paper provides a service-centric approach towards 6G V2X, with a concise overview of the upcoming hyper-connected vehicular ecosystem and its integration in the whole 6G fabric, analysing its particular infrastructure needs, as a way to reach key performance indicators (KPIs). We also present a 6G-oriented platform design able to manage the life-cycle of V2X applications across different domains by means of intelligent orchestration decisions.}
}
@article{TANHADOUST2022109334,
title = {Temperature-dependent multiscale modeling of graphene sheet under finite deformation},
journal = {Diamond and Related Materials},
volume = {129},
pages = {109334},
year = {2022},
issn = {0925-9635},
doi = {https://doi.org/10.1016/j.diamond.2022.109334},
url = {https://www.sciencedirect.com/science/article/pii/S0925963522005167},
author = {A. Tanhadoust and M. Jahanshahi and A.R. Khoei},
keywords = {Graphene sheet, Multiscale modeling, Nonlinear elastic properties, Cauchy–Born hypothesis, Molecular dynamics},
abstract = {The homogenized constitutive models that have been utilized to simulate the behavior of nanostructures are typically based on the Cauchy–Born hypothesis, which seeks the fundamental properties of material via relating atomistic information to an assumed homogeneous deformation field. It is well known that temperature has a profound effect on the validity and size-dependency of the Cauchy-Born hypothesis in finite deformations. In this study, a temperature-related Cauchy–Born formulation is established for graphene sheets and its performance is examined through a direct comparison between the continuum-based constitutive model and molecular dynamics analysis. It is demonstrated that if the temperature increases, the validity surfaces shrink, which is not highly dependent on the size of specimen. At finite strains, graphene sheet exhibits material softening and hardening, which are manifested in the elastic constants. The developed constitutive model is used to study the effect of applied deformations at various temperature levels on the elastic constants of graphene sheets. Finally, a novel multiscale finite element approach is developed for the analysis of graphene sheets in finite strain regime based on the proposed atomistic-continuum model. The method can be used efficiently in the simulation of large specimens where the application of molecular dynamics requires considerable computational efforts. The efficiency and robustness of the proposed multiscale approach are shown through several numerical examples.}
}
@article{MWASE2022292,
title = {Communication-efficient distributed AI strategies for the IoT edge},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {292-308},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000218},
author = {Christine Mwase and Yi Jin and Tomi Westerlund and Hannu Tenhunen and Zhuo Zou},
keywords = {Distributed AI/ML, Communication efficient AI/ML, Fog/edge computing, Edge ML, IIoT},
abstract = {The impact that artificial intelligence (AI) has made across several industries in today’s society is clearly seen in applications ranging from medical diagnosis to customer service chatbots, to financial trading. It is also evident that AI has a huge role to play in emerging and future applications and will be increasingly used in mission-critical and time-sensitive applications such as remote surgeries, cybersecurity and self-driving cars. To satisfy the latency, security and privacy requirements that such applications require, AI which has gained its merit by utilising resource-heavy cloud infrastructure, needs to perform well in resource-constrained environments at the network edge. To address this need, this paper characterises the cloud-to-thing continuum and provides an architecture for enabling AI in fully edge-based scenarios. In addition, the paper provides strategies to tackle the communication inefficiencies that arise from the distributed nature of fully edge-based scenarios. Performance improvements exhibited by these strategies in state-of-the art research is presented, as well as directions where further advancements can be made. The material is presented in a simple manner to catalyse the understanding and hence the participation of multidisciplinary researchers in addressing this challenge.}
}
@article{BUKHARI2022114,
title = {Fog node discovery and selection: A Systematic literature review},
journal = {Future Generation Computer Systems},
volume = {135},
pages = {114-128},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001625},
author = {Afnan Bukhari and Farookh Khadeer Hussain and Omar K. Hussain},
keywords = {Fog computing, Fog nodes, Fog service, Fog node discovery, Fog node selection, Trust-based selection},
abstract = {Fog computing is a new computing paradigm that extends cloud services by providing computing resources in the form of fog nodes closer to the edge devices. Fog computing supports mobility, location-awareness, low latency, and saves network bandwidth to efficiently process and store the data generated by edge devices. A prerequisite to achieve these aims is first the discovery of optimal fog nodes before selecting the most appropriate one/s. To achieve these aims, there are various selection criteria that need to be satisfied. The aim of this study is to provide a detailed overview of the state of the art in the area of fog node discovery and selection. To achieve this aim, we first define the different requirements that need to be met in identifying optimal fog nodes. We then identify the existing literature between 2014–2021 and compare them against the defined requirements in identifying optimal fog nodes. Based on the analysis, we present the gaps and open issues in the existing literature relating to optimal fog node discovery and selection.}
}
@article{MONTOYAMUNOZ2022107252,
title = {Reliability provisioning for Fog Nodes in Smart Farming IoT-Fog-Cloud continuum},
journal = {Computers and Electronics in Agriculture},
volume = {200},
pages = {107252},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107252},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005658},
author = {Ana Isabel Montoya-Munoz and Rodrigo A.C. da Silva and Oscar M. Caicedo Rendon and Nelson L.S. da Fonseca},
keywords = {Reliability, Smart Farming, Internet of Things, Fog Computing, Optimization, Redundancy},
abstract = {Reliability is essential in Smart Farming supported by the IoT-Fog-Cloud continuum. Smart Farms’ unprotection may cause significant economic losses and low yields of production. This paper introduces an optimization model for providing reliability and, consequently, service continuity to the IoT-Fog-Cloud continuum-based smart farms. The proposed model allows Smart Farming stakeholders to find the optimal number of Fog Nodes needed to deploy farming services considering the heterogeneity in the fog capabilities, resource demands, redundancy techniques, and reliability requirements. The model was solved using linear programming and evaluated with different demands and protection schemes. Results show that protection schemes guarantee high reliability and reveal that a shared redundancy scheme reduces deployment cost and yet provides reliability. Results also indicate that deployment costs and resources depend on the type of fog-based smart farm services to serve. Moreover, they show that deploying more low-resource hardware can be less expensive for low-reliability demands than deploying with a few high-resource hardware.}
}
@article{ZHONG2023104791,
title = {Blockchain-driven integration technology for the AEC industry},
journal = {Automation in Construction},
volume = {150},
pages = {104791},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104791},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523000511},
author = {Botao Zhong and Xing Pan and Lieyun Ding and Qiang Chen and Xiaowei Hu},
keywords = {Architecture, engineering, and construction (AEC), Blockchain-driven integration technology (BDIT), Critical review, Technological developments, Technological applications, Further evolutions},
abstract = {In the architecture, engineering, and construction (AEC) industry, blockchain-driven integration technology (BDIT) has witnessed rapid development. A critical literature review of BDIT can contribute toward innovation for the AEC industry. In this study, a quantitative mapping of 247 BDIT literatures from 2017 to 2022 was conducted. Following the clue of quantitative work, two critical levels of technological development and application for BDIT were analyzed. The findings suggest: (1) the technological developments of BDIT may involve technological integration (i.e., integration of blockchain and internet of thing/building information modeling/edge computing) and knowledge framework; and (2) the technological applications of BDIT may involve information automation management and building information management. To further explore the trends of BDIT, some future evolutions (i.e., integration of blockchain and federated learning/digital twin/cloud-edge-end), application scenarios and challenges of BDIT were discussed. This study provides valuable theoretical and practical references for future research on BDIT in the AEC industry.}
}
@article{VOLPERT2023243,
title = {The view on systems monitoring and its requirements from future Cloud-to-Thing applications and infrastructures},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {243-257},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003843},
author = {Simon Volpert and Philipp Eichhammer and Florian Held and Thomas Huffert and Hans P. Reiser and Jörg Domaschka},
keywords = {Internet of Things, Monitoring, Edge/fog computing, Observability, Instrumentation, Cloud-to-Thing continuum},
abstract = {Monitoring infrastructures is a key cornerstone for both security and resilience mechanisms, as well as, infrastructure management helping to ensure availability and reliability. Its importance is increasing in the light of Internet-of-Things (IoT) infrastructures growing both in size and complexity, hence, widening the attack surface and decreasing the possibilities of manual management. There is a multitude of different monitoring concepts providing solutions to those problems. However, in the context of IoT those traditional concepts are limited in their adoption capabilities due to fundamental differences in architecture and structure. In this paper, we create a systematization of knowledge about the impact of IoT infrastructure characteristics on requirements for and architecture of monitoring. In particular, we first discuss definitions of monitoring resulting in an own definition, before elaborating on the terminology encompassing monitoring. We further discuss requirements imposed by IoT systems combined with an analysis of monitoring properties relevant for IoT. As part of this paper, we also focus on highlighting current and future directions of IoT architecture and infrastructure.}
}
@article{SHINDE2021108598,
title = {A network operator-biased approach for multi-service network function placement in a 5G network slicing architecture},
journal = {Computer Networks},
volume = {201},
pages = {108598},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108598},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621004989},
author = {Swapnil Sadashiv Shinde and Dania Marabissi and Daniele Tarchi},
keywords = {5G, Network slicing, Edge Computing, Network function placement, Genetic Algorithms},
abstract = {The 5G communication standard is characterized by an increased softwarization, allowing a higher flexibility able to cope with different requirements and services. In particular, Network Function Virtualization (NFV) is a recently introduced technology that enables a software implementation of different network functions exploiting virtualization techniques, hence, enabling their flexible deployment upon system requirements. Boosted by NFV, the concept of network slicing is gaining great attention in 5G networks. The idea is that physical communication and computing resources are sliced in multiple end-to-end logical networks, each one tailored to best support a specific service. The advantages of NFV, in the network slicing context, are even more evident in distributed computing environments, such as the edge-to-cloud continuum, recently introduced for enabling a flexible deployment of multiple functions. In particular, thanks to the introduction of cloud-native technologies, based on the usage of containerization and microservice technologies, the virtual network functions (VNFs) deployment and their orchestration is an easy operation, allowing the on-the-fly network configuration. Gaining from the NFV, Network Slicing and Edge-to-Cloud continuum paradigms, we propose a new network function allocation problem for multi-service 5G networks, able to deploy network functions on a distributed computing environment depending on the service requests. The proposed approach jointly considers Radio Access Network (RAN) and Core Network (CN) functions and, differently from other approaches, introduces an option able to bias the function placement depending on the service requirements, allowing a fast-and-easy operator-side deployment of the network functions. We propose to solve the problem through a Genetic Algorithm able to approach the optimal solution but with reduced complexity and execution time. The performance is compared with two other heuristic algorithms and with an exhaustive search algorithm, introduced as benchmarks, showing the benefits of the selected solution in terms of performance, flexibility and complexity.}
}
@article{VESCOVI2022100606,
title = {Linking scientific instruments and computation: Patterns, technologies, and experiences},
journal = {Patterns},
volume = {3},
number = {10},
pages = {100606},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100606},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002318},
author = {Rafael Vescovi and Ryan Chard and Nickolaus D. Saint and Ben Blaiszik and Jim Pruyne and Tekin Bicer and Alex Lavens and Zhengchun Liu and Michael E. Papka and Suresh Narayanan and Nicholas Schwarz and Kyle Chard and Ian T. Foster},
keywords = {Experiment automation, workflow, Globus, synchrotron light source, big data, machine learning, data fabric, computing fabric, trust fabric, scientific facility},
abstract = {Summary
Powerful detectors at modern experimental facilities routinely collect data at multiple GB/s. Online analysis methods are needed to enable the collection of only interesting subsets of such massive data streams, such as by explicitly discarding some data elements or by directing instruments to relevant areas of experimental space. Thus, methods are required for configuring and running distributed computing pipelines—what we call flows—that link instruments, computers (e.g., for analysis, simulation, artificial intelligence [AI] model training), edge computing (e.g., for analysis), data stores, metadata catalogs, and high-speed networks. We review common patterns associated with such flows and describe methods for instantiating these patterns. We present experiences with the application of these methods to the processing of data from five different scientific instruments, each of which engages powerful computers for data inversion,model training, or other purposes. We also discuss implications of such methods for operators and users of scientific facilities.}
}
@incollection{FOTOPOULOS2022241,
title = {Chapter 8 - The edge-cloud continuum in wearable sensing for respiratory analysis},
editor = {Rui Pedro Paiva and Paulo de Carvalho and Vassilis Kilintzis},
booktitle = {Wearable Sensing and Intelligent Data Analysis for Respiratory Management},
publisher = {Academic Press},
pages = {241-271},
year = {2022},
isbn = {978-0-12-823447-1},
doi = {https://doi.org/10.1016/B978-0-12-823447-1.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234471000026},
author = {Anaxagoras Fotopoulos and Pantelis Z. Lappas and Alexis Melitsiotis},
keywords = {Artificial intelligence, Edge computing, Internet of Medical Things, Multisource fusion, P4 health care},
abstract = {Edge computing is seen as a set of remotely available computer system resources that drive the computing power at the source of data to improve energy efficiency and security, as well as decrease latency. Although the computation capability of biomedical wearables has increased extremely during the past decade, it is still challenging to perform sophisticated artificial intelligence (AI) algorithms in a resource-constrained environment for energy-efficiency and (near) real-time processing, along the edge-cloud continuum. The aim of this chapter is twofold. The first is to outline the role of edge computing on the Internet of Medical Things, in which wearable technologies are used as the sensory equipment for respiratory analysis, at the transition of patient monitoring from hospital to home. The second is to discuss the potential of explainable AI in the P4 health-care context for respiratory analysis, by highlighting computational intelligence and multisource fusion approaches to achieve continuous monitoring of respiratory analysis.}
}
@article{XUE2023107835,
title = {Learning the nonlinear dynamics of mechanical metamaterials with graph networks},
journal = {International Journal of Mechanical Sciences},
volume = {238},
pages = {107835},
year = {2023},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2022.107835},
url = {https://www.sciencedirect.com/science/article/pii/S0020740322007147},
author = {Tianju Xue and Sigrid Adriaenssens and Sheng Mao},
keywords = {Machine learning, Mechanical metamaterials, Graph neural networks},
abstract = {The dynamics of soft mechanical metamaterials provides opportunities for many exciting engineering applications. Previous works have shown tremendous success in describing the unique nonlinear dynamics of certain types of soft mechanical metamaterials. However, capturing the nonlinear dynamic response of these materials especially those with complex geometries, can be a challenge due to the strong nonlinearity and large computational cost. An efficient and reliable framework to predict the overall response of the metamaterials based on the geometry of their building blocks is not only key to understanding the unique behavior of metamaterials, but also vital to the rational design of such materials. In this work, we propose metamaterial graph network (MGN), a machine learning approach to address this challenge. MGN is based on a graph that represents the lattice-like metamaterial structure. The trained MGN is capable of simulating the dynamics of a metamaterial structure with over 200 by 200 unit cells, a task that is practically impossible for traditional direct numerical simulation using the finite element method. We also verify the accuracy of the proposed MGN against several representative numerical examples. In the first two examples, we show that MGN successfully captures the well-known pattern transformation behavior of porous metamaterials. In the later examples, we consider wave propagation in a dynamic setting and show that MGN produces quantitatively accurate results compared with direct numerical simulation. An additional feature of MGN is that defects/inhomogeneities can be easily incorporated into the metamaterial structure. We expect our method to open a new avenue for the study and modeling of mechanical metamaterials.}
}
@article{SARTI2023519,
title = {Anticipate, Ensemble and Prune: Improving Convolutional Neural Networks via Aggregated Early Exits},
journal = {Procedia Computer Science},
volume = {222},
pages = {519-528},
year = {2023},
note = {International Neural Network Society Workshop on Deep Learning Innovations and Applications (INNS DLIA 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923009559},
author = {Simone Sarti and Eugenio Lomurno and Matteo Matteucci},
keywords = {Early Exits, Ensemble, Pruning, AEP, Image classification, Convolutional Neural Networks},
abstract = {Today, artificial neural networks are the state of the art for solving a variety of complex tasks, especially in image classification. Such architectures consist of a sequence of stacked layers with the aim of extracting useful information and having it processed by a classifier to make accurate predictions. However, intermediate information within such models is often left unused. In other cases, such as in edge computing contexts, these architectures are divided into multiple partitions that are made functional by including early exits, i.e., intermediate classifiers, with the goal of reducing the computational and temporal load without extremely compromising the accuracy of the classifications. In this paper, we present Anticipate, Ensemble and Prune (AEP), a new training technique based on a weighted ensemble of early exits, which aims at exploiting the information in the structure of networks to maximise their performance. Through a comprehensive set of experiments, we show how the use of this approach can yield average accuracy improvements of up to 15% over traditional training. AEP's internal pruning operation also allows reducing the number of parameters by up to 41%, lowering the number of multiplications and additions by 18% and the latency time to make inference by 16%. By using AEP, it is also possible to learn weights that allow early exits to achieve better accuracy values than those obtained from single-output reference models. The code will be available on GitHub after acceptance of the paper.}
}
@article{XUE2023307,
title = {Integration of blockchain and edge computing in internet of things: A survey},
journal = {Future Generation Computer Systems},
volume = {144},
pages = {307-326},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003521},
author = {He Xue and Dajiang Chen and Ning Zhang and Hong-Ning Dai and Keping Yu},
keywords = {Blockchain, Edge computing, Internet of things, Resource management, Security and privacy, Data management},
abstract = {As an important technology to ensure data security, consistency, traceability, etc., blockchain has been increasingly used in Internet of Things (IoT) applications. The integration of blockchain and edge computing (IBEC) can further improve the resource utilization in terms of network, computing, storage, and security. This paper aims to present a survey on the IBEC. In particular, we first give an overview of blockchain and edge computing respectively. We then present a general architecture of an IBEC system. We next study the various applications of the IBEC in IoT. We also discuss the optimizations of the IBEC system and solutions from perspectives of resource management and performance improvement. Finally, we analyze and summarize the existing challenges posed by the IBEC system and the potential solutions in the future.}
}
@article{MANIK2021104357,
title = {A natural vector/matrix notation applied in an efficient and robust return-mapping algorithm for advanced yield functions},
journal = {European Journal of Mechanics - A/Solids},
volume = {90},
pages = {104357},
year = {2021},
issn = {0997-7538},
doi = {https://doi.org/10.1016/j.euromechsol.2021.104357},
url = {https://www.sciencedirect.com/science/article/pii/S0997753821001236},
author = {Tomáš Mánik},
keywords = {Continuum plasticity, Plastic anisotropy, Yield function, Return mapping algorithm, Vector notation},
abstract = {A fast and robust stress-integration algorithm is the key to full exploitation of advanced anisotropic yield functions in computational mechanics. Poor global convergence of a direct application of the Newton-Raphson scheme has been rectified by applying line search strategies during the Newton iterations. In this work the line-search approach is further improved by a better first guess. The new algorithm is implemented into a user-defined material subroutine (UMAT) in a finite-element (FE) software and tested. The implementation is made easier and more efficient by a new advantageous vector/matrix notation for symmetric second- and fourth-order tensors, which is the second result of this work. Benefits of this notation are discussed with respect to formulation of continuum-plasticity models as well as their implementations. FE simulations were run to demonstrate the performance of the new implementation, which is available as open-source software via GitLab repository (see Appendix). The new return-mapping algorithm implementation runs equally fast and robust as the simple von Mises and Hill standard implementations in the Abaqus/Standard software. This enables full exploitation of advanced yield functions as the new standard in industrial FE applications.}
}
@article{AGOSTA2022104679,
title = {Towards EXtreme scale technologies and accelerators for euROhpc hw/Sw supercomputing applications for exascale: The TEXTAROSSA approach},
journal = {Microprocessors and Microsystems},
volume = {95},
pages = {104679},
year = {2022},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2022.104679},
url = {https://www.sciencedirect.com/science/article/pii/S0141933122002095},
author = {Giovanni Agosta and Marco Aldinucci and Carlos Alvarez and Roberto Ammendola and Yasir Arfat and Olivier Beaumont and Massimo Bernaschi and Andrea Biagioni and Tommaso Boccali and Berenger Bramas and Carlo Brandolese and Barbara Cantalupo and Mauro Carrozzo and Daniele Cattaneo and Alessandro Celestini and Massimo Celino and Iacopo Colonnelli and Paolo Cretaro and Pasqua D’Ambra and Marco Danelutto and Roberto Esposito and Lionel Eyraud-Dubois and Antonio Filgueras and William Fornaciari and Ottorino Frezza and Andrea Galimberti and Francesco Giacomini and Brice Goglin and Daniele Gregori and Abdou Guermouche and Francesco Iannone and Michal Kulczewski and Francesca {Lo Cicero} and Alessandro Lonardo and Alberto R. Martinelli and Michele Martinelli and Xavier Martorell and Giuseppe Massari and Simone Montangero and Gianluca Mittone and Raymond Namyst and Ariel Oleksiak and Paolo Palazzari and Pier Stanislao Paolucci and Federico Reghenzani and Cristian Rossi and Sergio Saponara and Francesco Simula and Federico Terraneo and Samuel Thibault and Massimo Torquati and Matteo Turisini and Piero Vicini and Miquel Vidal and Davide Zoni and Giuseppe Zummo},
keywords = {High-performance computing},
abstract = {In the near future, Exascale systems will need to bridge three technology gaps to achieve high performance while remaining under tight power constraints: energy efficiency and thermal control; extreme computation efficiency via HW acceleration and new arithmetic; methods and tools for seamless integration of reconfigurable accelerators in heterogeneous HPC multi-node platforms. TEXTAROSSA addresses these gaps through a co-design approach to heterogeneous HPC solutions, supported by the integration and extension of HW and SW IPs, programming models, and tools derived from European research.}
}
@article{WANG2022103354,
title = {Context-aware distribution of fog applications using deep reinforcement learning},
journal = {Journal of Network and Computer Applications},
volume = {203},
pages = {103354},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103354},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000236},
author = {Nan Wang and Blesson Varghese},
keywords = {Fog computing, Decentralised cloud, Edge computing, Context-aware distribution},
abstract = {Fog computing is an emerging paradigm that aims to meet the increasing computation demands arising from the billions of devices connected to the Internet. Offloading services of an application from the Cloud to the edge of the network can improve the overall latency of the application since it can process data closer to user devices. Diverse Fog nodes ranging from Wi-Fi routers to mini-clouds with varying resource capabilities makes it challenging to determine which services of an application need to be offloaded. In this paper, a context-aware mechanism for distributing applications across the Cloud and the Fog is proposed. The mechanism dynamically generates (re)deployment plans for the application to maximise the performance efficiency of the application by taking operational conditions, such as hardware utilisation and network state, and running costs into account. The mechanism relies on deep Q-networks to generate a distribution plan without prior knowledge of the available resources on the Fog node, the network condition, and the application. The feasibility of the proposed context-aware distribution mechanism is demonstrated on two use-cases, namely a face detection application and a location-based mobile game. The benefits are increased utility of dynamic distribution by 50% and 20% for the two use-cases respectively when compared to a static distribution approach used in existing research.}
}
@article{HLOBIL2022106714,
title = {Scaling of strength in hardened cement pastes - Unveiling the role of microstructural defects and the susceptibility of C-S-H gel to physical/chemical degradation by multiscale modeling},
journal = {Cement and Concrete Research},
volume = {154},
pages = {106714},
year = {2022},
issn = {0008-8846},
doi = {https://doi.org/10.1016/j.cemconres.2022.106714},
url = {https://www.sciencedirect.com/science/article/pii/S0008884622000059},
author = {Michal Hlobil and Konstantinos Sotiriadis and Adéla Hlobilová},
keywords = {Finite element analysis, Chemical degradation, Microstructural damage, Multiscale modeling, Portland cement paste},
abstract = {The scaling of strength across hierarchically structured composite materials is accented by the presence of defects with distinct morphology and origin appearing within the microstructure. In hardened Portland cement pastes, these defects manifest as capillary pores, microcracks, and air voids. The transition of material strength from the nano-to-microscale is bridged in the present paper by a validated computational model based on a hierarchical representation of the paste microstructure, resting solely on realistic microstructural features. The model unravels and quantifies the strength scaling mechanisms arising from the various types of defects and critically assesses the susceptibility of C-S-H gel to physical and/or chemical deterioration, quantifying their impact on the gel's structural integrity which subsequently projects as a decrease of the material's load-bearing capacity. The fully-developed model is subsequently used as a quantitative tool to assess the strength degradation of cement paste exposed to a magnesium sulfate attack occurring at low temperature.}
}
@article{ZAPPATORE20231,
title = {Semantic models for IoT sensing to infer environment–wellness relationships},
journal = {Future Generation Computer Systems},
volume = {140},
pages = {1-17},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003211},
author = {Marco Zappatore and Antonella Longo and Angelo Martella and Beniamino {Di Martino} and Antonio Esposito and Serena Angela Gracco},
keywords = {IoT interoperability, Semantic API, Environmental sensing, Mobile Crowd Sensing, Ontology patterns},
abstract = {Every time an Internet of Things (IoT) solution is deployed, every time a smartphone owner connects her/his wireless device to a wearable activity-tracker, every time groups of citizens use geo-mapping applications to move around the city, choosing the least crowded path, data are produced and information have to be exchanged appropriately via APIs. Even if novel added-value IoT-based applications appear on the market with increasing speed, true semantic interoperability is far from being achieved, thus limiting the large-scale exploitation, the scalability and the time-to-market of novel apps. Currently, connecting different data prosumers with multiple data sources is still hampered by the lack of standardized and sustainable solutions, especially due to the significant heterogeneity of IoT platforms. In such a landscape, ontologies come to the rescue, thanks to their formal semantics, knowledge representation formats, and shared vocabularies. In this paper we examine, from an ontological perspective, how to describe environmental sensing and wellness monitoring, two of the most popular application cases of Mobile Crowd Sensing (MCS) and IoT, respectively. To this purpose, an ontology of sensor-agnostic APIs is proposed, along with a set of MCS-dedicated ontology modules (and the supporting platform), leveraging on standard and reusable domain ontologies. Moreover, it will be shown how to properly combine the proposed ontologies in order to support complex functionalities based on inference rules addressing the environment–wellness relationships. Finally, specific semantic modeling patterns suitable for typical IoT and MCS scenarios will be discussed.}
}
@article{SAVAGLIO2021107562,
title = {Introduction to the Special Section on Research challenges and directions of Data Mining in Edge Computing systems (VSI-dmec)},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107562},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107562},
url = {https://www.sciencedirect.com/science/article/pii/S004579062100505X},
author = {Claudio Savaglio and Teemu Leppänen and Giuseppe {Di Fatta}}
}
@incollection{KARACA2022149,
title = {Chapter 9 - Computational fractional-order calculus and classical calculus AI for comparative differentiability prediction analyses of complex-systems-grounded paradigm},
editor = {Yeliz Karaca and Dumitru Baleanu and Yu-Dong Zhang and Osvaldo Gervasi and Majaz Moonis},
booktitle = {Multi-Chaos, Fractal and Multi-Fractional Artificial Intelligence of Different Complex Systems},
publisher = {Academic Press},
pages = {149-168},
year = {2022},
isbn = {978-0-323-90032-4},
doi = {https://doi.org/10.1016/B978-0-323-90032-4.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323900324000067},
author = {Yeliz Karaca and Dumitru Baleanu},
keywords = {Complexity, Artificial neural network, Classical calculus, Computational complexity, Data-driven fractional modeling, Differentiability prediction analyses, Fractional calculus, Mathematical biology and neuroscience, Mittag-Leffler function, Optimized fractional-order calculus},
abstract = {Modern science having embarked on the thorough and accurate interpretation of natural and physical phenomena has proven to provide successful models for the analysis of complex systems and harnessing of control over the various processes therein. Computational complexity, in this regard, comes to the foreground by providing applicable sets of ideas or integrative paradigms to recognize and understand the complex systems' intricate properties. Thus, while making the appropriate, adaptable and evolutive decisions in complex dynamic systems, it is essential to acknowledge different degrees of acceptance of the problems and construct the model it to account for its inherent constraints or limits. In this respect, while hypothesis-driven research has its inherent limitations regarding the investigation of multifactorial and heterogeneous diseases, a data-driven approach enables the examination of the way variables impact one another, which paves the way for the interpretation of dynamic and heterogeneous mechanisms of diseases. Fractional Calculus (FC), in this scope characterized by complexity, provides the applicable means and methods to solve integral, differential and integro-differential equations so FC enables the generalization of integration and differentiation possible in a flexible and consistent manner owing to its capability of reflecting the systems' actual state properties, which exhibit unpredictable variations. The fractional integration and differentiation of fractional-order is capable of providing better characterization of nonstationary and locally self-similar attributes in contrast to constant-order fractional calculus. It becomes possible to model many complex systems by fractional-order derivatives based on fractional calculus so that related syntheses can be realized in a robust and effective way. To this end, our study aims at providing an intermediary facilitating function both for the physicians and individuals by establishing accurate and robust model based on the integration of fractional-order calculus and Artificial Neural Network (ANN) for the diagnostic and differentiability predictive purposes with the diseases which display highly complex properties. The integrative approach we have proposed in this study has a multistage quality the steps of which are stated as follows: first of all, the Caputo fractional-order derivative, one of the fractional-order derivatives, has been used with two-parametric Mittag-Leffler function on the stroke dataset and cancer cell dataset, manifesting biological and neurological attributes. In this way, new fractional models with varying degrees have been established. Mittag-Leffler function, with its distributions of extensive application domains, can address irregular and heterogeneous environments for the solution of dynamic problems; thus, Mittag-Leffler function has been opted for accordingly. Following this application, the new datasets (mlf_stroke dataset and mlf_cancer cell dataset) have been obtained by employing Caputo fractional-order derivative with the two-parametric Mittag-Leffler function (α,β). In addition, classical derivative (calculus) was applied to the raw datasets; and cd_stroke dataset and cd_cancer cell dataset were obtained. Secondly, the performance of the new datasets as obtained from the Caputo fractional derivative with the two-parametric Mittag-Leffler function, the datasets obtained from the classical derivative application and the raw datasets have been compared by using feed forward back propagation (FFBP) algorithm, one of the algorithms of ANN (along with accuracy rate, sensitivity, precision, specificity, F1-score, multiclass classification (MCC), ROC curve). Based on the accuracy rate results obtained from the application with FFBP, the Caputo fractional-order derivative model that is most suitable for the diseases has been generated. The experimental results obtained demonstrate the applicability of the complex-systems-grounded paradigm scheme as proposed through this study, which has no existing counterpart. The integrative multi-stage method based on mathematical-informed framework with comparative differentiability prediction analyses can point toward a new direction in the various areas of applied sciences to address formidable challenges of critical decision making and management of chaotic processes in different complex dynamic systems.}
}
@article{MOHAMMADIAN2023103731,
title = {NET-RAT: Non-equilibrium traffic model based on risk allostasis theory},
journal = {Transportation Research Part A: Policy and Practice},
volume = {174},
pages = {103731},
year = {2023},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2023.103731},
url = {https://www.sciencedirect.com/science/article/pii/S0965856423001519},
author = {Saeed Mohammadian and Zuduo Zheng and Mazharul Haque and Ashish Bhaskar},
keywords = {Continuum model, Macroscopic model, Traffic flow, Risk allostasis theory},
abstract = {Empirical studies of vehicle trajectories have shown that psychological theories of driver behaviour can shed light on car-following processes and the associated empirical traffic phenomena. Numerous continuum models have been derived from car-following relations in order to model macroscopic traffic flow dynamics through collective intercations between car-following processes. However, the existing continuum models cannot capture the psychological processes underlying drivers’ car-following in accordance to behavioural thoeries, and thus, have little implications for investigating empirical traffic phenomena in relation to human psychological factors. This paper develops a novel continuum model (Non-Equilibrium Traffic Model based on Risk Allostasis Theory, i.e., NET-RAT) from a car-following model, extended in this work, by incorporating drivers’ behavioural adaptions in relation to perceived risk. We first extend the full-velocity difference car-following model (FVDM) to incorporate drivers’ pereception of risk and its impacts on drivers’ adaptation time to frontal stimuli using car-following information (e.g., speed, spacing, etc) and corresponding safety surroage measures. Risk allostasis theory is used to model the impacts of perceived risk on drivers’ stimulus–response behavioural adaptation. We then derive NET-RAT by continuum approximations of the extended FVDM and the corresponding behavioural components. Theoretical investigations show that NET-RAT has desirable analytical properties regarding macroscopic traffic flow dynamics, and that such properties can also be explained meaningfully from a behavioural perspective. Furthermore, we investigate NET-RAT’s performance for real-world traffic by using data from the German A5 autobahn in order to study the impacts of drivers’ risk perception on the complex real-world traffic phenomena (e.g., wide scattering, traffic instabilities, hysteresis, etc.). The investigations show that NET-RAT can meaningfully capture such complex phenomena by linking them to the interplay between drivers’ dynamic adaption process in relation to their perceived car-following risk.}
}
@article{SABBIONI2022108993,
title = {DIFFUSE: A DIstributed and decentralized platForm enabling Function composition in Serverless Environments},
journal = {Computer Networks},
volume = {210},
pages = {108993},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.108993},
url = {https://www.sciencedirect.com/science/article/pii/S138912862200161X},
author = {Andrea Sabbioni and Lorenzo Rosa and Armir Bujari and Luca Foschini and Antonio Corradi},
keywords = {Serverless computing, Function composition, Shared memory, Middleware, Latency, Distributed system},
abstract = {Serverless computing is an emerging proposition in the cloud offering landscape that promotes a higher level of abstraction, further decoupling software operations from the underlying hardware. Often recognized as an economically driven computational approach, the serverless model relies on the execution of short-lived stateless functions, enabling a fine-grained accounting and control of resources. In this context, function composition represents an appealing feature, allowing the composition of two or more functions to create tailored processing pipelines, incentivizing modularity and reusability of functions, while paving the way to application-specific run-time optimizations. This work presents DIFFUSE: a DIstributed and decentralized platForm enabling Function composition in Serverless Environments. DIFFUSE embodies an innovative infrastructural support, enabling the efficient and transparent composition of functions by relying on pluggable middleware support, serving as a conveyor of messages among the platform components. Broadening the deployment spectrum of our proposal, we assess different middleware solutions, each presenting distinct delivery profiles, evidencing the tradeoffs that emerge.}
}
@article{RZEPKA20221,
title = {SDN-based fog and cloud interplay for stream processing},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {1-17},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000140},
author = {Michał Rzepka and Piotr Boryło and Marcos D. Assunção and Artur Lasoń and Laurent Lefèvre},
keywords = {Stream processing, Fog computing, Edge computing, SDN},
abstract = {This paper focuses on SDN-based approaches for deploying stream processing workloads on heterogeneous environments comprising wide-area networks, cloud and fog resources. Stream processing applications impose strict latency requirements to operate appropriately. Deploying workloads in the fog reduces unnecessary delays, but its computational resources may not handle all the tasks. On the other hand, offloading the tasks to the cloud is constrained by limited network resources and involves additional transmission delays that exceed latency thresholds. Adaptive workload deployment may solve these issues by ensuring that resource and latency requirements are satisfied for all the data streams processed by an application. This paper’s main contribution consists of dynamic workload placement algorithms operating on stream processing requests with latency constraints. Provisioning of computing infrastructure exploits the interplay between fog and cloud under limited network capacity. The algorithms aim to maximize the ratio of successfully handled requests by effectively utilizing available resources while meeting application latency constraints. Experiments demonstrate that the goal can be achieved by detailed analysis of requests and ensuring balanced computing and network resources utilization. As a result, up to 30% improvement over the reference algorithms in success rate is observed.}
}
@incollection{2022xvii,
title = {Preface},
editor = {Rui Pedro Paiva and Paulo de Carvalho and Vassilis Kilintzis},
booktitle = {Wearable Sensing and Intelligent Data Analysis for Respiratory Management},
publisher = {Academic Press},
pages = {xvii-xviii},
year = {2022},
isbn = {978-0-12-823447-1},
doi = {https://doi.org/10.1016/B978-0-12-823447-1.00012-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234471000129}
}
@article{VILA2022101699,
title = {Edge-to-cloud sensing and actuation semantics in the industrial Internet of Things},
journal = {Pervasive and Mobile Computing},
volume = {87},
pages = {101699},
year = {2022},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2022.101699},
url = {https://www.sciencedirect.com/science/article/pii/S1574119222001122},
author = {Marc Vila and Víctor Casamayor and Schahram Dustdar and Ernest Teniente},
keywords = {Industrial Internet of Things, Interoperability, Computing Continuum, Context-awareness, Semantics, Autonomous cars},
abstract = {There are billions of devices worldwide deployed, connected, and communicating to other systems. Sensors and actuators, which can be stationary or movable devices. These Edge devices are considered part of the Internet of Things (IoT) devices, which can be referred to as a tier of the Computing Continuum paradigm. There are two main concerns at stake in the success of this ecosystem. The interoperability between devices and systems is the first. Mainly, because most of them communicate uniquely and differently from each other, leading to heterogeneous data. The second issue is the lack of decision-making capacity to conduct actuations, such as communicating through different computing tiers based on latency constraints due to a certain measured factor. In this article, we propose an ontology to improve device interoperability in the IoT. In addition, we also explain how to ease data communication between Computing Continuum devices, providing tools to enhance data management and decision-making. A use case is also presented, using the automotive industry, where quickness in maneuver determination is key to avoid accidents. It is exemplified using two Raspberry Pi devices, connected using different networks and choosing the appropriate one depending on context-aware conditions.}
}
@article{GAGLIANESE202377,
title = {Assessing and enhancing a Cloud-IoT monitoring service over federated testbeds},
journal = {Future Generation Computer Systems},
volume = {147},
pages = {77-92},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001668},
author = {M. Gaglianese and S. Forti and F. Paganelli and A. Brogi},
keywords = {Cloud-IoT monitoring, Federated testbeds, Lightweight monitoring, Fog computing, Fault resilience, Self-organising systems},
abstract = {Monitoring resource availability along Cloud-IoT networks in a lightweight and fault-resilient manner is a challenging research problem due to scarce resource availability, infrastructure dynamics, and platform heterogeneity. In this article, we illustrate a thorough experimental assessment of a self-organising and fault-tolerant monitoring service, FogMon, especially targeting Cloud-IoT settings and capable of probing hardware resource, latency and bandwidth. The assessment is carried out over networks made up of 20 to 40 nodes across two testbeds within the Fed4Fire+ federation. As a result of the assessment, we agilely improved and refined FogMon into FogMon 2, which settles at TRL5 and improves on monitoring accuracy and fault-resiliency. Experimental results show how FogMon 2 can promptly and suitably handle different types of infrastructure failure, with an average relative error of 10% on measurements and limited footprint on hardware and network resources.}
}
@article{BADIDI2023958,
title = {On workflow scheduling for latency-sensitive edge computing applications},
journal = {Procedia Computer Science},
volume = {220},
pages = {958-963},
year = {2023},
note = {The 14th International Conference on Ambient Systems, Networks and Technologies Networks (ANT) and The 6th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.03.132},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923006671},
author = {Elarbi Badidi},
keywords = {Edge computing, cloud computing, latency, scheduling},
abstract = {With the rapid proliferation of edge computing-based solutions, many edge computing applications use the cloud for data processing and analysis. However, latency-sensitive applications have low latency requirements and can be very bandwidth hungry, so processing their collected data through cloud servers is not efficient and cost-effective. For example, traffic management and health condition monitoring applications, require real-time data processing at the network edge to respond immediately to unexpected events. These applications are typically executed as workflows with dependent tasks that require careful scheduling to allocate the appropriate resources to the tasks so that the execution is done in a way that satisfies the users’ target functions. In this work, we evaluate some traditional scheduling heuristics for the execution of workflow tasks in an edge-computing scenario. Our goal is to compare their performance in terms of execution time and cost and show that these heuristics, previously used in scheduling in the context of cloud environments, can also be used in edge computing scenarios. The results show that the MinMin and PSO scheduling algorithms offer the best results with regard to execution time and cost.}
}
@article{DELTOROLLORENS2021106636,
title = {An isogeometric finite element-boundary element approach for the vibration analysis of submerged thin-walled structures},
journal = {Computers & Structures},
volume = {256},
pages = {106636},
year = {2021},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2021.106636},
url = {https://www.sciencedirect.com/science/article/pii/S0045794921001589},
author = {Alvaro {del Toro Llorens} and Josef Kiendl},
keywords = {Isogeometric analysis, Finite element method, Boundary element method, Modal analysis, Generalized added mass matrix},
abstract = {In this paper, the isogeometric formulations of the finite element and boundary element methods are applied to the dynamic analysis of thin-walled structures submerged in an infinite, inviscid, and incompressible fluid medium. This fluid–structure interaction problem is decoupled using the modal analysis technique, and the fluid effect on the structure is taken into account through the generalized added mass matrix. The structure is modeled with NURBS-based Kirchhoff–Love shell elements. The fluid response is computed using a regularized boundary integral equation. We take advantage of the geometry preserving property of the NURBS refinement techniques to reduce the computational cost without the need for a projection scheme. The implementation is benchmarked with three test cases, and good accuracy is obtained for a relatively low number of degrees of freedom.}
}
@article{PULIAFITO2023101808,
title = {Balancing local vs. remote state allocation for micro-services in the cloud–edge continuum},
journal = {Pervasive and Mobile Computing},
volume = {93},
pages = {101808},
year = {2023},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2023.101808},
url = {https://www.sciencedirect.com/science/article/pii/S1574119223000664},
author = {Carlo Puliafito and Claudio Cicconetti and Marco Conti and Enzo Mingozzi and Andrea Passarella},
keywords = {FaaS, Function-as-a-service, Distributed computing, Edge computing, Micro-services, Stateful functions},
abstract = {In the world of cloud technologies, serverless computing has now settled as a stable and promising resident. This gives a cloud provider the flexibility to provide its users with both Platform-as-a-Service (PaaS), i.e., the back-end application runs in a dedicated container, or Function-as-a-Service (FaaS), i.e., the back-end logic is offered as elementary functions that are invoked by the client applications. In parallel, edge computing has attracted a significant interest, due its enticing promises of reducing the outbound traffic of telco operators, while at the same time cutting down the user latency. As a result, in the near future, PaaS and FaaS containers are going to cohabit in a versatile computation infrastructure spanning from the far edge up to the cloud. In this paper we propose a mathematical formulation of a resource allocation problem that optimizes the assignment of both types of containers and can be solved efficiently by an edge orchestrator. We evaluate the proposed solution via extensive simulation experiments, which show that our approach, which takes into account the characteristics of PaaS vs. FaaS, provides significant performance benefits compared to less sophisticated strategies, despite its relatively low run-time complexity.}
}
@article{XU20221609,
title = {Real-time determination of sandy soil stiffness during vibratory compaction incorporating machine learning method for intelligent compaction},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
volume = {14},
number = {5},
pages = {1609-1625},
year = {2022},
issn = {1674-7755},
doi = {https://doi.org/10.1016/j.jrmge.2022.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1674775522001561},
author = {Zhengheng Xu and Hadi Khabbaz and Behzad Fatahi and Di Wu},
keywords = {Intelligent compaction, Machine learning method, Finite element modelling, Acceleration response},
abstract = {An emerging real-time ground compaction and quality control, known as intelligent compaction (IC), has been applied for efficiently optimising the full-area compaction. Although IC technology can provide real-time assessment of uniformity of the compacted area, accurate determination of the soil stiffness required for quality control and design remains challenging. In this paper, a novel and advanced numerical model simulating the interaction of vibratory drum and soil beneath is developed. The model is capable of evaluating the nonlinear behaviour of underlying soil subjected to dynamic loading by capturing the variations of damping with the cyclic shear strains and degradation of soil modulus. The interaction of the drum and the soil is simulated via the finite element method to develop a comprehensive dataset capturing the dynamic responses of the drum and the soil. Indeed, more than a thousand three-dimensional (3D) numerical models covering various soil characteristics, roller weights, vibration amplitudes and frequencies were adopted. The developed dataset is then used to train the inverse solver using an innovative machine learning approach, i.e. the extended support vector regression, to simulate the stiffness of the compacted soil by adopting drum acceleration records. Furthermore, the impacts of the amplitude and frequency of the vibration on the level of underlying soil compaction are discussed. The proposed machine learning approach is promising for real-time extraction of actual soil stiffness during compaction. Results of the study can be employed by practising engineers to interpret roller drum acceleration data to estimate the level of compaction and ground stiffness during compaction.}
}
@article{NURNOBY20231102,
title = {A Real-Time Deep Learning-based Smart Surveillance Using Fog Computing: A Complete Architecture},
journal = {Procedia Computer Science},
volume = {218},
pages = {1102-1111},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923000893},
author = {M Fasial Nurnoby and Tarek Helmy},
keywords = {Fog-Computing, Deep Learning, Computer Vision, Smart Video Surveillance, Internet of Things},
abstract = {Fog computing offers low-latency and real-time big-data processing capabilities closer to the network edge. This particular benefit addresses the main bottleneck in a centralized cloud framework, which is, it cannot process latency-sensitive large video frames generated from the Internet of Things-based video surveillance cameras in real-time. Besides, the recent advancements in the computer vision field offer many state-of-the-art image processing capabilities that can be utilized for real-time surveillance data processing. Deploying those processing powers at several fog computing layers can bring novel solutions for computer vision-based real-time security solutions. This paper proposes a deep learning-based framework for smart video surveillance that can process the real-time frames on two consecutive fog layers, one for action recognition and the other for criminal threat-based response generation. The proposed architecture consists of three major modules. The first module is responsible for capturing surveillance videos by deploying RaspberryPi cameras in a distributed network. The second module is responsible for action recognition using a deep learning-based model installed inside NVIDIA Jetson Nano-devices placed on two fog layers. Finally, the security response is generated and broadcast to the law-enforcement agency. To evaluate the proposed model, experiments on semantic segmentation-based scene object recognition were run. The experimental results came up with a suitable recognition model that can be deployed in the fog layers of our proposed framework.}
}
@article{COSTA2022109189,
title = {Monitoring fog computing: A review, taxonomy and open challenges},
journal = {Computer Networks},
volume = {215},
pages = {109189},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109189},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622002845},
author = {Breno Costa and João Bachiega and Leonardo Rebouças Carvalho and Michel Rosa and Aleteia Araujo},
keywords = {Monitoring, Orchestration, Fog computing, Taxonomy, Fog monitoring},
abstract = {Fog computing is a distributed paradigm that provides computational resources in the users’ vicinity. Fog orchestration is a set of functionalities that coordinate the dynamic infrastructure and manage the services to guarantee the Service Level Agreements. Monitoring is an orchestration functionality of prime importance. It is the basis for resource management actions, collecting status of resource and service and delivering updated data to the orchestrator. There are several cloud monitoring solutions and tools, but none of them comply with fog characteristics and challenges. Fog monitoring solutions are scarce, and they may not be prepared to compose an orchestration service. This paper updates the knowledge base about fog monitoring, assessing recent subjects in this context like observability, data standardization and instrumentation domains. We propose a novel taxonomy of fog monitoring solutions, supported by a systematic review of the literature. Fog monitoring proposals are analyzed and categorized by this new taxonomy, offering researchers a comprehensive overview. This work also highlights the main challenges and open research questions.}
}
@article{TUSA2023473,
title = {End-to-end slices to orchestrate resources and services in the cloud-to-edge continuum},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {473-488},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.11.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003971},
author = {Francesco Tusa and Stuart Clayman},
keywords = {Internet of things, End-to-end slice, Bare-metal cloud, Slice abstraction, Orchestration, Programmability, Multi-tenant},
abstract = {Fog computing, combined with traditional cloud computing, offers an inherently distributed infrastructure – referred to as the cloud-to-edge continuum – that can be used for the execution of low-latency and location-aware IoT services. The management of such an infrastructure is complex: resources in multiple domains need to be accessed by several tenants, while an adequate level of isolation and performance has to be guaranteed. This paper proposes the dynamic allocation of end-to-end slices to perform the orchestration of resources and services in such a scenario. These end-to-end slices require a unified resource management approach that encompasses both data centre and network resources. Currently, fog orchestration is mainly focused on the management of compute resources, likewise, the slicing domain is specifically centred solely on the creation of isolated network partitions. A unified resource orchestration strategy, able to integrate the selection, configuration and management of compute and network resources, as part of a single abstracted object, is missing. This work aims to minimise the silo-effect, and proposes end-to-end slices as the foundation for the comprehensive orchestration of compute resources, network resources, and services in the cloud-to-edge continuum, as well acting as the basis for a system implementation. The concept of the end-to-end slice is formally described via a graph-based model that allows for dynamic resource discovery, selection and mapping via different algorithms and optimisation goals; and a working system is presented as the way to build slices across multiple domains dynamically, based on that model. These are independently accessible objects that abstract resources of various providers – traded via a Marketplace – with compute slices, allocated using the bare-metal cloud approach, being interconnected to each other via the connectivity of network slices. Experiments, carried out on a real testbed, demonstrate three features of the end-to-end slices: resources can be selected, allocated and controlled in a softwarised fashion; tenants can instantiate distributed IoT services on those resources transparently; the performance of a service is absolutely not affected by the status of other slices that share the same resource infrastructure.}
}
@article{HUANG2022114811,
title = {Adaptive stochastic morphology simulation and mesh generation of high-quality 3D particulate composite microstructures with complex surface texture},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {393},
pages = {114811},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.114811},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522001372},
author = {Junjie Huang and Fangqian Deng and Lingfei Liu and Jianqiao Ye},
keywords = {Particulate composite materials, Microstructure, Surface texture, Heat kernel smoothing},
abstract = {Particulate composite materials have a broad range of potential applications in engineering and other disciplines. Accurate modeling of their microstructures and fast generation of the finite element meshes play a vital role in investigating many micromechanical phenomena and improving understanding of the underlying failure mechanisms. Due to the exceedingly intricate multiscale internal structures that they possess, the modeling and meshing of their microstructures still remain difficult in general. In this work, we present a computational framework and methodology for the representation, simulation, and mesh generation of 3D stochastic microstructures of particulate composites. Towards this goal, we propose a multi-level multiscale scheme that allows for capturing the multiscale structures of particulate composite materials at both the coarse and fine scales. A briging scale approach based on heat kernel smoothing is also presented to seamlessly link the coarse and fine scales. In addition to the microstructural modeling of particulate composite materials, we also develop an adaptive curvature-based surface and volume mesh generation algorithm for particulate composite microstructures with complex surface texture. Following the implementation of the morphology and mesh generation algorithm, a series of numerical examples are presented to demonstrate the capability and potential of the proposed method.}
}
@article{SHARMA2020723,
title = {Osmotic computing-based service migration and resource scheduling in Mobile Augmented Reality Networks (MARN)},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {723-737},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19310738},
author = {Vishal Sharma and Dushantha Nalin K. Jayakody and Marwa Qaraqe},
keywords = {Osmotic computing, Edge-computing, Augmented reality, Resource scheduling, Service migrations},
abstract = {Resources and services between the servers in Mobile Augmented Reality Networks (MARN) are tedious to manage. These networks comprise users possessing Augmented Reality (AR)-Virtual Reality (VR) applications. Low latency, robustness, and tolerance are the key requirements of these networks, which can be attained by using near-user solutions such as edge computing. However, management of services and scheduling them to near-user servers in an integrated environment of edge and public/private infrastructure are complex tasks. These require an optimal solution, which can be obtained by using “Osmotic Computing”, that has been recently proposed as a paradigm for the integration of edge and public/private cloud. This paper uses osmotic computing for effectively migrating and scheduling the services between the servers of the different layers. The paper also presents the details on various components that are used for applying osmotic computing to a network followed by core applications, types, service classification, migration, and scheduling through the rules of osmotic game formulated for its operations. The evaluations are conducted on 100,000 requests and the proposed approach shows significant performance with the probability of the error being 0.1 at 55.72% conservation of the energy and memory resources for the entire network despite the increasing number of users. The proposed approach also satisfies the conditions of the joint optimization functions presented in the system model and demonstrates that the system holds true even with varying users, thus, proving its robustness and tolerance against the number of users.}
}
@article{KOJIC2018156,
title = {Mass release curves as the constitutive curves for modeling diffusive transport within biological tissue},
journal = {Computers in Biology and Medicine},
volume = {92},
pages = {156-167},
year = {2018},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2016.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0010482516301706},
author = {M. Kojic and M. Milosevic and N. Kojic and E.J. Koay and J.B. Fleming and M. Ferrari and A. Ziemys},
keywords = {Diffusion, Porous material, Biological tissue, Mass release curve, Equivalent diffusion coefficient, Multiscale model, Numerical homogenization},
abstract = {In diffusion governed by Fick’s law, the diffusion coefficient represents the phenomenological material parameter and is, in general, a constant. In certain cases of diffusion through porous media, the diffusion coefficient can be variable (i.e. non-constant) due to the complex process of solute displacements within microstructure, since these displacements depend on porosity, internal microstructural geometry, size of the transported particles, chemical nature, and physical interactions between the diffusing substance and the microstructural surroundings. In order to provide a simple and general approach of determining the diffusion coefficient for diffusion through porous media, we have introduced mass release curves as the constitutive curves of diffusion. The mass release curve for a selected direction represents cumulative mass (per surface area) passed in that direction through a small reference volume, in terms of time. We have developed a methodology, based on numerical Finite Element (FE) and Molecular Dynamics (MD) methods, to determine simple mass release curves of solutes through complex media from which we calculate the diffusion coefficient. The diffusion models take into account interactions between solute particles and microstructural surfaces, as well as hydrophobicity (partitioning). We illustrate the effectiveness of our approach on several examples of complex composite media, including an imaging-based analysis of diffusion through pancreatic cancer tissue. The presented work offers an insight into the role of mass release curves in describing diffusion through porous media in general, and further in case of complex composite media such as biological tissue.}
}
@article{ARDAGNA2021661,
title = {Editorial: Special issue on trusted Cloud-Edges computations},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {661-664},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.08.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20326819},
author = {Claudio A. Ardagna and Mauro Conti and Ernesto Damiani and Chia-Mu Yu}
}
@article{VAQUERO201920,
title = {Research challenges in nextgen service orchestration},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {20-38},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18303157},
author = {Luis M. Vaquero and Felix Cuadrado and Yehia Elkhatib and Jorge Bernal-Bernabe and Satish N. Srirama and Mohamed Faten Zhani},
keywords = {NVM, SDN, NFV, Orchestration, Large scale, Serverless, FaaS, Churn, Edge, Fog},
abstract = {Fog/edge computing, function as a service, and programmable infrastructures, like software-defined networking or network function virtualisation, are becoming ubiquitously used in modern Information Technology infrastructures. These technologies change the characteristics and capabilities of the underlying computational substrate where services run (e.g. higher volatility, scarcer computational power, or programmability). As a consequence, the nature of the services that can be run on them changes too (smaller codebases, more fragmented state, etc.). These changes bring new requirements for service orchestrators, which need to evolve so as to support new scenarios where a close interaction between service and infrastructure becomes essential to deliver a seamless user experience. Here, we present the challenges brought forward by this new breed of technologies and where current orchestration techniques stand with regards to the new challenges. We also present a set of promising technologies that can help tame this brave new world.}
}
@article{SYED2021107285,
title = {Analysis of high strength composite structure developed for low-carbon-low-manganese steel sheet by laser surface treatment},
journal = {Optics & Laser Technology},
volume = {143},
pages = {107285},
year = {2021},
issn = {0030-3992},
doi = {https://doi.org/10.1016/j.optlastec.2021.107285},
url = {https://www.sciencedirect.com/science/article/pii/S003039922100373X},
author = {B. Syed and P Maurya and S. Lenka and G. Padmanabham and SM Shariff},
keywords = {High strength composite steel structure, Laser surface hardening, Hardness, Tensile properties, Surface texture, Elastic modulus, Formability and Strengthening mechanism},
abstract = {In the present work, a high-strength composite steel structure developed by imparting laser surface hardening treatment on a low-carbon low-Manganese automotive steel sheet has been comprehensively analysed. The layered composite steel structure has been successfully developed by re-engineering the surface of the steel using diode laser hardening treatment up to a depth of 250–300 µm through its thickness. Hardness at the treated surface improved by 150% to that of its base due to formation of a mixture of hard phases constituting martensite and bainite along with retained ferrite. Indeed, coupling EBSD technique with Weibull distribution of various phase fractions determined by image quality helped analyse microstructure effectively. The tensile property of the layered composite steel sheet was found to yield significant improvements in both Yield Strength (YS) (40–44%) and Ultimate Tensile Strength (UTS) (19–21%) due to sandwich effect of composite layer constituting hardened layer and soft base accomplished by a strengthening mechanism associated with rule of mixtures concept. In fact, Young’s modulus of the composite steel sheet, determined from slope of tensile stress–strain diagram was found to be convergent with ultrasonic test result. Additionally, the crystallographic texturing effects of the hardened layer and untreated base measured using standard XRD technique re-confirmed their influence on Young’s Modulus and r-bar values obtained.}
}
@article{MENEZES2019568,
title = {Predictive, Prescriptive and Detective Analytics for Smart Manufacturing in the Information Age},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {1},
pages = {568-573},
year = {2019},
note = {12th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.06.123},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319302095},
author = {Brenno C. Menezes and Jeffrey D. Kelly and Adriano G. Leal and Galo C. {Le Roux}},
keywords = {Advanced analytics, Decision-making, Smart manufacturing, Industry automation},
abstract = {The solution of today’s complex decision-making for smart manufacturing are dependent on the ability to: a) realistically model the manufacturing system, b) easily and timely integrate valid and consistent plant data, c) solve the problem efficiently with reasonable computational efforts, and d) incorporate feedback to continuously improve the decision-making process over time. In such a context, advanced analytics such as the predictive, prescriptive and detective analytics are the foundation of smart manufacturing in the information age. Predictive analytics examines raw data to be augmented with the purpose of concluding the behaviour of the systems, by estimating and anticipating what is likely to happen within the forthcoming future. Prescriptive analytics automates the decision-making of any physical system concerning its design, planning, scheduling, control and operation using any combination of optimisation, heuristics, machine-learning and cyber-physical systems. Detective analytics makes diagnostics on data to improve both the predictive and prescriptive analytics. In the former, by identifying and eliminating gross-errors for better predictions. In the latter, by uncovering and rectifying infeasibilities and inconsistencies for optimal prescriptions. We construct a plot of the connections of the advanced analytics at their time-spaces considering the well-established, the current and the next generation of analytics techniques. An example of an automated application of advanced analytics considering a multi-unit real-time estimation and optimisation engine relying on data integration and integrity for better decision-making is highlighted.}
}
@article{PAPRZYCKI20213,
title = {Towards Edge-Fog-Cloud Continuum},
journal = {Procedia Computer Science},
volume = {179},
pages = {3},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921001228},
author = {Marcin Paprzycki},
keywords = {Cloud Computing, Cloud Continuum},
abstract = {Over time, two counteracting trends have been observed in the “world of computing”. One of them was a push from centralized towards decentralized solutions. The first wave of this process can be associated with the introduction of the Internet and personal computers and took place in early 1980th. The second was the move in the opposite direction. Here, the first wave of centralization can be associated with the ascent of cloud computing. These two seem to be similar to the thesis and antithesis, in Hegel’s philosophy. Interestingly, similarly to Hegel’s synthesis, we are approaching a unified model of edge-fog-cloud continuum. My talk will reflect on the journey and outline the proposed way forward.}
}
@article{MEKHONOSHINA2017655,
title = {Numerical modeling of aeroelastic behavior of GTU centrifugal compressor rotor},
journal = {Procedia Engineering},
volume = {201},
pages = {655-665},
year = {2017},
note = {3rd International Conference “Information Technology and Nanotechnology", ITNT-2017, 25-27 April 2017, Samara, Russia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.09.680},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817342133},
author = {E.V. Mekhonoshina and V.Ya. Modorskii},
keywords = {centrifugal compressor, gas transmittal unit, vibration, aeroelastic computation, computational experiment, pressure, stress, strain, displacement},
abstract = {This paper deals with simulating oscillatory modes for the centrifugal compressor rotor of the gas transmittal unit by means of computational experiments taking into account rigid type supports and aeroelastic processes in the wheels. Validation of the aeroelastic approach was carried out on the model problem of interaction between a supersonic flow and a deformable plate in the shock tube. For numerical simulation the ANSYS engineering analysis system was used in this work. The research methods are the finite volume method for gas dynamics and the finite element method for estimating components of the stress-strain state. Dependences of pressure and stress-strain state components on geometric, physical-mechanical, kinematic parameters of the rotor and characteristics of the working fluid in compressors of gas transmittal units are obtained.}
}
@article{GIL2013178,
title = {An enhanced Immersed Structural Potential Method for fluid–structure interaction},
journal = {Journal of Computational Physics},
volume = {250},
pages = {178-205},
year = {2013},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113003422},
author = {A.J. Gil and A. {Arranz Carreño} and J. Bonet and O. Hassan},
keywords = {Fluid–structure interaction, Immersed Boundary Method, Immersed Structural Potential Method},
abstract = {Within the group of immersed boundary methods employed for the numerical simulation of fluid–structure interaction problems, the Immersed Structural Potential Method (ISPM) was recently introduced (Gil et al., 2010) [1] in order to overcome some of the shortcomings of existing immersed methodologies. In the ISPM, an incompressible immersed solid is modelled as a deviatoric strain energy functional whose spatial gradient defines a fluid–structure interaction force field in the Navier–Stokes equations used to resolve the underlying incompressible Newtonian viscous fluid. In this paper, two enhancements of the methodology are presented. First, the introduction of a new family of spline-based kernel functions for the transfer of information between both physics. In contrast to classical IBM kernels, these new kernels are shown not to introduce spurious oscillations in the solution. Second, the use of tensorised Gaussian quadrature rules that allow for accurate and efficient numerical integration of the immersed structural potential. A series of numerical examples will be presented in order to demonstrate the capabilities of the enhanced methodology and to draw some key comparisons against other existing immersed methodologies in terms of accuracy, preservation of the incompressibility constraint and computational speed.}
}
@article{LONGO2020899,
title = {Apollon: Towards a citizen science methodology for urban environmental monitoring},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {899-912},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.06.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20303940},
author = {Antonella Longo and Marco Zappatore and Mario A. Bochicchio},
keywords = {Mobile crowd sensing, Citizen science, Smart cities, Internet of people},
abstract = {The collaborative power of ICT systems is a key enabler of social and technological advances providing multiple opportunities for public involvement in participatory activities, thanks to novel paradigms like citizen science and mobile crowd sensing. These paradigms, if applied according to specific methodologies, promise to increase the pervasive observation of urban environmental pollution either directly by human observers, or by means of crowd-sourcing data measurement tasks using sensors in smart phones or other mobile devices. We propose a platform, named Apollon, to enable scientists and others to take part in citizen science projects based on the exploitation of mobile devices. The platform has been implemented and validated in an educational context, in which students participate in urban environmental monitoring activities. In the paper, we describe the platform and the approach developed to produce successful experiments.}
}
@article{RUNNELS201563,
title = {Capturing plasticity effects in overdriven shocks on the finite scale},
journal = {Mathematics and Computers in Simulation},
volume = {111},
pages = {63-79},
year = {2015},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2014.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378475414003334},
author = {Scott R. Runnels},
keywords = {Shocks, Plasticity, Hardening, Hydrodynamics, Radial return},
abstract = {An ordinary differential equation (ODE) form of the radial return algorithm, which is essentially a Prandtl-Reuss material model, is combined with a strain-rate hardening model to produce an ODE that describes deviatoric stress through a prescribed density rise. An analytical solution is found to the resulting ODE for a specific choice of one of the hardening model’s parameters. That solution is used to prove that if the prescribed density rise is allowed to be infinitely thin, i.e., like a shock in the mathematical sense, the resulting deviatoric stress is still bounded. In other words, the singularity is integrable; integration of the radial return ODE regularizes the infinite strain rate and resulting yield stress in the presence of an ideal shock singularity. The analytical tools developed for this line of thinking are applied to study the variation of deviatoric stress through a nearly shock-like density rise using different density rise profiles, revealing the impact of the shape choice. The tools are also used to compute what rise times are needed to converge upon the correct value of deviatoric stress through a shock; the results indicate that most contemporary hydrocodes cannot be expected to achieve those rise times. A demonstration of connecting the analytical tools to a hydrocode, using surrogate numerical shock shapes, is provided thereby opening the door for using such surrogates to perform sub-grid computations of converged shock behavior for strain-rate hardening materials.}
}
@article{MARTINS2015334,
title = {The Programmable City},
journal = {Procedia Computer Science},
volume = {52},
pages = {334-341},
year = {2015},
note = {The 6th International Conference on Ambient Systems, Networks and Technologies (ANT-2015), the 5th International Conference on Sustainable Energy Information Technology (SEIT-2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.104},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915009047},
author = {Pedro M.N. Martins and Julie A. McCann},
keywords = {Ubiquitous Computing, Programming Languages, Macroprogramming, Internet of Things},
abstract = {The worldwide proliferation of mobile connected devices has brought about a revolution in the way we live, and will inevitably guide the way in which we design the cities of the future. However, designing city-wide systems poses a new set of challenges in terms of scale, manageability and citizen involvement. Solving these challenges is crucial to making sure that the vision of a programmable Internet of Things (IoT) becomes reality. In this article we will analyse these issues and present a novel programming approach to designing scalable systems for the Internet of Things, with an emphasis on smart city applications, that addresses these issues.}
}
@article{OLEKSIAK2017117,
title = {M2DC – Modular Microserver DataCentre with heterogeneous hardware},
journal = {Microprocessors and Microsystems},
volume = {52},
pages = {117-130},
year = {2017},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2017.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0141933116304574},
author = {Ariel Oleksiak and Michal Kierzynka and Wojciech Piatek and Giovanni Agosta and Alessandro Barenghi and Carlo Brandolese and William Fornaciari and Gerardo Pelosi and Mariano Cecowski and Robert Plestenjak and Justin Činkelj and Mario Porrmann and Jens Hagemeyer and René Griessl and Jan Lachmair and Meysam Peykanu and Lennart Tigges and Micha vor dem Berge and Wolfgang Christmann and Stefan Krupop and Alexandre Carbon and Loïc Cudennec and Thierry Goubier and Jean-Marc Philippe and Sven Rosinger and Daniel Schlitt and Christian Pieper and Chris Adeniyi-Jones and Javier Setoain and Luca Ceva and Udo Janssen},
keywords = {Microservers, Data centres, Heterogeneous architectures},
abstract = {The Modular Microserver DataCentre (M2DC) project investigates, develops and demonstrates a modular, highly-efficient, cost-optimized server architecture composed of heterogeneous microserver computing resources. The resulting server architecture will be able to be tailored to meet requirements from a wide range of application domains. M2DC is built on three main pillars: a flexible server architecture that can be easily customised, maintained and updated; advanced management strategies and system efficiency enhancements (SEE); well-defined interfaces to the surrounding software data centre ecosystem. In this paper, we focus in particular on the thermal management strategies and on the initial benchmarking of the Aarch64 ARM architecture.}
}
@article{VALLERO20151204,
title = {Cross-layer reliability evaluation, moving from the hardware architecture to the system level: A CLERECO EU project overview},
journal = {Microprocessors and Microsystems},
volume = {39},
number = {8},
pages = {1204-1214},
year = {2015},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2015.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0141933115000824},
author = {A. Vallero and S. Tselonis and N. Foutris and M. Kaliorakis and M. Kooli and A. Savino and G. Politano and A. Bosio and G. {Di Natale} and D. Gizopoulos and S. {Di Carlo}},
keywords = {Reliability evaluation, Fault injection, Statistical models},
abstract = {Advanced computing systems realized in forthcoming technologies hold the promise of a significant increase of computational capabilities. However, the same path that is leading technologies toward these remarkable achievements is also making electronic devices increasingly unreliable. Developing new methods to evaluate the reliability of these systems in an early design stage has the potential to save costs, produce optimized designs and have a positive impact on the product time-to-market. CLERECO European FP7 research project addresses early reliability evaluation with a cross-layer approach across different computing disciplines, across computing system layers and across computing market segments. The fundamental objective of the project is to investigate in depth a methodology to assess system reliability early in the design cycle of the future systems of the emerging computing continuum. This paper presents a general overview of the CLERECO project focusing on the main tools and models that are being developed that could be of interest for the research community and engineering practice.}
}
@article{LIEW201598,
title = {Mechanical properties and characteristics of microtubules: A review},
journal = {Composite Structures},
volume = {123},
pages = {98-108},
year = {2015},
issn = {0263-8223},
doi = {https://doi.org/10.1016/j.compstruct.2014.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S026382231400676X},
author = {K.M. Liew and Ping Xiang and L.W. Zhang},
keywords = {Computational modeling, Mechanical properties, Microtubules, Recent advances},
abstract = {This review focuses on modeling techniques developed for prediction of mechanical properties and characterization of microtubules, polyatomic structures that contain billions of different types of atoms. The challenge of modeling mechanical properties of microtubules is to achieve both computational efficiency and accuracy. Various kinds of techniques have been adopted and created in practice. Atomistic simulation method is useful for simulation of atomic structures, however, because of computational limitations, it is not realistic to adopt it for modeling of long microtubules which involve billions of atoms. Classical continuum mechanics does not incorporate fundamental atomic interactions. Bridging-scale techniques are developed to address the problems in traditional macroscopic and microscopic modeling, which combine the intrinsic interatomic potential and continuum mechanical solution frameworks, hence possess merits in both atomic simulation and computational continuum mechanics. In view of the fast development in this research topic, this article provides a general review of the past and recent advances in mechanical properties and characterization of microtubules.}
}
@article{ARDAGNA20201180,
title = {Special issue on Trusted Cloud-Edges Computations},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {1180-1183},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.060},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20324183},
author = {Claudio A. Ardagna and Mauro Conti and Chia-Mu Yu}
}
@article{GULTEKIN2016542,
title = {A phase-field approach to model fracture of arterial walls: Theory and finite element analysis},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {312},
pages = {542-566},
year = {2016},
note = {Phase Field Approaches to Fracture},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2016.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0045782516301645},
author = {Osman Gültekin and Hüsnü Dal and Gerhard A. Holzapfel},
keywords = {Biomechanics, Arterial wall, Fracture, Multi-field modeling, Crack phase-field, Anisotropic failure criterion},
abstract = {This study uses a recently developed phase-field approach to model fracture of arterial walls with an emphasis on aortic tissues. We start by deriving the regularized crack surface to overcome complexities inherent in sharp crack discontinuities, thereby relaxing the acute crack surface topology into a diffusive one. In fact, the regularized crack surface possesses the property of Gamma-Convergence, i.e. the sharp crack topology is restored with a vanishing length-scale parameter. Next, we deal with the continuous formulation of the variational principle for the multi-field problem manifested through the deformation map and the crack phase-field at finite strains which leads to the Euler–Lagrange equations of the coupled problem. In particular, the coupled balance equations derived render the evolution of the crack phase-field and the balance of linear momentum. As an important aspect of the continuum formulation we consider an invariant-based anisotropic constitutive model which is additively decomposed into an isotropic part for the ground matrix and an exponential anisotropic part for the two families of collagen fibers embedded in the ground matrix. In addition we propose a novel energy-based anisotropic failure criterion which regulates the evolution of the crack phase-field. The coupled problem is solved using a one-pass operator-splitting algorithm composed of a mechanical predictor step (solved for the frozen crack phase-field parameter) and a crack evolution step (solved for the frozen deformation map); a history field governed by the failure criterion is successively updated. Subsequently, a conventional Galerkin procedure leads to the weak forms of the governing differential equations for the physical problem. Accordingly, we provide the discrete residual vectors and a corresponding linearization yields the element matrices for the two sub-problems. Finally, we demonstrate the numerical performance of the crack phase-field model by simulating uniaxial extension and simple shear fracture tests performed on specimens obtained from a human aneurysmatic thoracic aorta. Model parameters are obtained by fitting the set of novel experimental data to the predicted model response; the finite element results agree favorably with the experimental findings.}
}
@article{BENITEZ201775,
title = {The mechanical behavior of skin: Structures and models for the finite element analysis},
journal = {Computers & Structures},
volume = {190},
pages = {75-107},
year = {2017},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2017.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045794916314183},
author = {José María Benítez and Francisco Javier Montáns},
keywords = {Biological tissues, Skin, Hyperelasticity, Viscoelasticity, Damage, Anisotropy},
abstract = {Soft biological tissues are complex materials with a large structural variety, with differences in behavior, but with some common characteristics. Skin is an archetypal soft tissue which presents many common characteristics to other soft biological tissues, like being a multilayer collagen-reinforced structure, with nonlinear behavior, anisotropy, viscosity, preconditioning effects, internal stresses and tissue growth and adaptation. Departing from a detailed description of the structures of the skin and the experimental evidence, we herein analyze the different modeling approaches in the literature for the distinct aspects of the skin behavior, with attention to the implementation in finite element codes.}
}
@article{KOCHOVSKI2021103562,
title = {Building applications for smart and safe construction with the DECENTER Fog Computing and Brokerage Platform},
journal = {Automation in Construction},
volume = {124},
pages = {103562},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103562},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521000133},
author = {Petar Kochovski and Vlado Stankovski},
keywords = {Smart construction, Internet of things, Cloud-to-edge computing, Fog computing, Blockchain, Artificial intelligence},
abstract = {Various smart applications are needed to address complex problems in construction falling under the broad categories of safety at work, construction site management, management of resources, waste and assets and construction progress monitoring. Fog computing emerges as a new computing paradigm for Edge-to-Cloud computing that integrates Internet of Things (IoT), Artificial Intelligence (AI), and Blockchain technologies to facilitate the development and operation of smart applications. However, a comprehensive methodology that applies Fog computing to construction projects is currently missing. In our work, we use the novel DECENTER Fog Computing and Brokerage Platform to address requirements for flexible use of AI methods in construction projects and develop a relevant methodology. Evaluation is performed through all application development phases at a real construction site in Ljubljana, Slovenia. Testing results show that the use of Fog computing contributes to high response rates, privacy and security when processing sensitive worker and company data.}
}
@article{BOUSSEJRA20191,
title = {aflak: Visual programming environment enabling end-to-end provenance management for the analysis of astronomical datasets},
journal = {Visual Informatics},
volume = {3},
number = {1},
pages = {1-8},
year = {2019},
note = {Proceedings of PacificVAST 2019},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X19300154},
author = {Malik Olivier Boussejra and Rikuo Uchiki and Yuriko Takeshima and Kazuya Matsubayashi and Shunya Takekawa and Makoto Uemura and Issei Fujishiro},
keywords = {Astronomy, Provenance, Visual programming, Visualization},
abstract = {This paper describes an extendable graphical framework, aflak, which provides a visualization and provenance management environment for the analysis of multi-spectral astronomical datasets. Via its node editor interface, aflak allows the astronomer to compose transforms on input datasets queryable from public astronomical data repositories, then to export the results of the analysis as Flexible Image Transport System (FITS) files, in a manner such that the full provenance of the output data be preserved and reviewable, and that the exported file be usable by other common astronomical analysis software. FITS is the standard of data interchange in astronomy. By embedding aflak’s provenance data into FITS files, we both achieve interoperability with existing software and full reproducibility of the process by which astronomers make discoveries.}
}
@article{SKARIN20206993,
title = {Cloud-based model predictive control with variable horizon⁎⁎This work has been partially funded by the Wallenberg AI, Autonomous Systems and Software Program (WASP), the ELLIIT strategic research area on IT and mobile communications, and the Nordforsk university hub on Industrial IoT (HI2OT).},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {6993-7000},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.437},
url = {https://www.sciencedirect.com/science/article/pii/S240589632030728X},
author = {Per Skarin and Johan Eker and Karl-Erik Årzén},
keywords = {Industry automation, Predictive control, Optimal control, Systems concept, System architectures, Networks, Adaptive systems, Parallelism},
abstract = {A novel method using the cloud to implement a variable horizon model predictive controller is presented. In case of sudden long delays and downtime, a graceful degradation is used. Robust, best effort strategies allow industrial grade use of the powerful, efficient, and quickly improving cloud ecosystems. The variable horizon strategy finds use in, for example, non-linear control problems, and the proposed method can be generalized to implement robust and scalable controllers that benefit from cloud technology. We show results from two horizon selection strategies, service degradation and connectivity issues.}
}
@article{MATSUDA201540,
title = {3-Dimensional Joint Torque Calculation of Compression Sportswear Using 3D-CG Human Model},
journal = {Procedia Engineering},
volume = {112},
pages = {40-45},
year = {2015},
note = {'The Impact of Technology on Sport VI' 7th Asia-Pacific Congress on Sports Technology, APCST2015},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.07.173},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815014228},
author = {Akihiro Matsuda and Hirokazu Tanaka and Hitoshi Aoki and Takatsugu Shimana},
keywords = {Compression sportswear, Anisotropic hyperelasticity, 3D-CG human model},
abstract = {Recently, there have been many developments in compression sportswear and swimwear. The main purpose of compression sportswear is to keep athletes’ muscles warm to prevent muscle strain and fatigue. Now, some compression sportswear has been developed to include a joint protection function. In this study, a design method for joint torque generated by compression sportswear such as competitive swimwear and compression long running tights is proposed. Information on the joint torque generated by sportswear would be useful for compression sportswear design. The proposed method calculates stress-strain relationships of sportswear fabrics using the anisotropic hyperelastic model, and calculates the frictional displacement between the human body and sportswear using a 3D-CG human model. In order to accurately calculate stress, anisotropic material modeling and a stress-softening model were applied to the mechanical characteristics of compression sportswear fabrics. The frictional displacement between the human body and swimwear during swimming was also considered for stress calculations. Typical sportswear fabrics exhibit anisotropic mechanical behavior depending on tensile direction. Also, the stiffness of the fabrics is softened by the maximum strain experienced in each warp and weft fiber. To accurately calculate the joint torques generated by compression sportswear, the anisotropy and stress softening of sportswear fabrics are considered for numerical modeling.}
}
@article{DAVYDOV2014260,
title = {Comparison of several staggered atomistic-to-continuum concurrent coupling strategies},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {277},
pages = {260-280},
year = {2014},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2014.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0045782514001388},
author = {D. Davydov and J-P. Pelteret and P. Steinmann},
keywords = {Concurrent multiscale methods, Atomic-to-continuum coupling methods, Molecular mechanics, Irving–Kirkwood–Noll procedure, Finite elements, Large strain},
abstract = {In this contribution several staggered schemes used to couple continuum mechanics (CM) and molecular mechanics (MM) are proposed. The described approaches are based on the atomistic-to-continuum correspondence, obtained by spatial averaging in the spirit of Irving and Kirkwood, and Noll. Similarities between this and other concurrent coupling schemes are indicated, thus providing a broad overview of different approaches in the field. The schemes considered here are decomposed into the surface-type (displacement or traction boundary conditions) and the volume-type. The latter restricts the continuum displacement field (and possibly its gradient) in some sense to the atomistic (discrete) displacements using Lagrange multipliers. A large-strain CM formulation incorporating Lagrange multipliers and a strategy to solve the resulting coupled linear system using an iterative solver is presented. Finally, the described coupling methods are numerically examined using two examples: uniaxial deformation and a plate with a hole relaxed under surface tension. Accuracy and convergence rates of each method are reported. It was found that the displacement (surface) coupling scheme and the Lagrangian (volume) scheme based on either discrete displacements or the H1 norm derived from continuous displacement fields provide the best performance.}
}
@article{SOUZA20181,
title = {Towards a proper service placement in combined Fog-to-Cloud (F2C) architectures},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {1-15},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.04.042},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17323051},
author = {V.B. Souza and X. Masip-Bruin and E. Marín-Tordera and S. Sànchez-López and J. Garcia and G.J. Ren and A. Jukan and A. {Juan Ferrer}},
keywords = {Service placement and execution, Resource allocation, Cloud & fog computing, Distributed systems, Quality of service},
abstract = {The Internet of Things (IoT) has empowered the development of a plethora of new services, fueled by the deployment of devices located at the edge, providing multiple capabilities in terms of connectivity as well as in data collection and processing. With the inception of the Fog Computing paradigm, aimed at diminishing the distance between edge-devices and the IT premises running IoT services, the perceived service latency and even the security risks can be reduced, while simultaneously optimizing the network usage. When put together, Fog and Cloud computing (recently coined as fog-to-cloud, F2C) can be used to maximize the advantages of future computer systems, with the whole greater than the sum of individual parts. However, the specifics associated with cloud and fog resource models require new strategies to manage the mapping of novel IoT services into the suitable resources. Despite few proposals for service offloading between fog and cloud systems are slowly gaining momentum in the research community, many issues in service placement, both when the service is ready to be executed admitted as well as when the service is offloaded from Cloud to Fog, and vice-versa, are new and largely unsolved. In this paper, we provide some insights into the relevant features about service placement in F2C scenarios, highlighting main challenges in current systems towards the deployment of the next-generation IoT services.}
}
@article{CAIAZZA2021108140,
title = {Measurement-driven design and runtime optimization in edge computing: Methodology and tools},
journal = {Computer Networks},
volume = {194},
pages = {108140},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108140},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621002085},
author = {Chiara Caiazza and Claudio Cicconetti and Valerio Luconi and Alessio Vecchio},
keywords = {Edge computing, ETSI MEC, Network measurements, GSMA platform operator},
abstract = {Edge computing is projected to become the dominant form of cloud computing in the future because of the significant advantages it brings to both users (less latency, higher throughput) and telecom operators (less Internet traffic, more local management). However, to fully unlock its potential at scale, system designers and automated optimization systems alike will have to monitor closely the dynamics of both processing and communication facilities. Especially the latter is often neglected in current systems since network performance in cloud computing plays only a minor role. In this paper, we propose the architecture of MECPerf, which is a solution to collect network measurements in a live edge computing domain, to be collected for offline provisioning analysis and simulations, or to be provided in real-time for on-line system optimization. MECPerf has been validated in a realistic testbed funded by the European Commission (Fed4Fire+), and we describe here a summary of the results, which are fully available as open data and through a Python library to expedite their utilization. This is demonstrated via a use case involving the optimization of a system parameter for migrating clients in a federated edge computing system adopting the GSMA platform operator concept.}
}
@article{GIANNONE2020107402,
title = {Orchestrating heterogeneous MEC-based applications for connected vehicles},
journal = {Computer Networks},
volume = {180},
pages = {107402},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107402},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620301997},
author = {Francesco Giannone and Pantelis A. Frangoudis and Adlen Ksentini and Luca Valcarenghi},
keywords = {Multi-access edge computing, 4G/5G systems, Connected vehicles, Video streaming, Quality of experience},
abstract = {In the near future, 5G-connected vehicles will be able to exchange messages with each other, with the roadside infrastructure, with back-end servers, and with the Internet. They will do so with reduced latency, increased reliability, and large throughput under high mobility and user density. Different services with different requirements, such as Advanced Driving Assistance (ADA) and High Definition (HD) Video Streaming, will share the same physical resources, such as the wireless channel. Thus, a rigid orchestration among them becomes necessary to prioritize network resource allocation. This study proposes a Connected Vehicle Service Orchestrator (CVSO) which optimizes the Quality of Experience (QoE) of an in-vehicle infotainment video delivery service, while taking into account the required bandwidth for coexisting high priority services, such as ADA. To this end, we provide an Integer Linear Programming (ILP) formulation for the problem of optimally assigning a video streaming bitrate/quality per user to maximize the overall QoE, considering information from the video service and the Radio Access Network (RAN) levels. Our system takes advantage of recent developments in the area of Multi-access Edge Computing (MEC). In particular, we have implemented the CVSO and other service-level components and have deployed them on top of a standards-compliant MEC platform that we have developed. We exploit MEC-native services such as the Radio Network Information Service (RNIS) to offer the CVSO the necessary level of RAN awareness. Experiments on a full LTE network testbed featuring our MEC platform demonstrate the performance improvements our system brings in terms of video QoE. Furthermore, we propose and evaluate different algorithms to solve the ILP, which exhibit different trade-offs between solution quality and execution time.}
}
@article{ALLA2019455,
title = {Gamification in IoT Application: A Systematic Mapping Study},
journal = {Procedia Computer Science},
volume = {151},
pages = {455-462},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.062},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305241},
author = {Abdelhadi Alla and Khalid Nafil},
keywords = {Internet of Things, IoT, Gamification, Game, Systematic Mapping Study},
abstract = {We are entering an era where everything is almost connected. According to Gartner, "we expect to see 20 billion internet-connected things by 2020". This evolution has engendered a colossal amount of data that traditional computer applications could no longer handle. We penetrate the epoch of smart applications: intelligent, contextual and proactive. On the other side, a trend that makes the hype is Gamification. It is now being applied to several domains especially those that targets human engagement, performance and sustainability. In this paper, through a systematic mapping study, we will present a breadth-first review of search papers that integrate gamification in Internet of Things (IoT) applications. The main aim is to provide a historical and geographic classification of research that combines IoT with gamification, the most challenging areas, and the artifacts that come out.}
}
@article{PAPADAKISVLACHOPAPADOPOULOS2019498,
title = {Collaborative SLA and reputation-based trust management in cloud federations},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {498-512},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18329248},
author = {Konstantinos Papadakis-Vlachopapadopoulos and Román Sosa González and Ioannis Dimolitsas and Dimitrios Dechouniotis and Ana Juan Ferrer and Symeon Papavassiliou},
keywords = {Cloud applications, SLA, Trust management, Federation},
abstract = {Industry and academia shift from the single cloud provider paradigm to cloud federations and alternative models, which orchestrate heterogeneous resources, such as Mobile Edge Computing and Fog Computing. In such complex environments, incorrect selection of deployment platform can lead to underwhelming application performance. This dictates the necessity of Service Level Agreements (SLA) and trust management services in order to enforce performance guarantees and enable customers to simultaneously evaluate their application’s performance and give performance indicators for future provider selection. In this paper, we propose a collaborative SLA and Reputation-based Trust Management (RTM) solution for federated cloud environment. The SLA service defines clearly the performance metrics and measures the actual performance of the deployed cloud applications. Based on the SLA, the RTM service of the collaborative solution leverages several technical and user’s experience metrics to compute the reliability of the cloud providers and the credibility of the customers. A proof of concept of the collaborative solution in a realistic federated environment is provided and validated. The corresponding experimental results demonstrate that it objectively computes the cloud providers’ reputation values under various scenarios.}
}
@article{TANAKA2014261,
title = {3-Dimensional Stress Calculation of Competitive Swimwear Using Anisotropic Hyperelastic Model Considering Stress Softening},
journal = {Procedia Engineering},
volume = {72},
pages = {261-266},
year = {2014},
note = {The Engineering of Sport 10},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2014.06.046},
url = {https://www.sciencedirect.com/science/article/pii/S1877705814005621},
author = {Hirokazu Tanaka and Takatsugu Shimana and Akihiro Matsuda},
keywords = {Swimwear, Anisotropic, Stress Softening, 3-dimentional Stress Calculation, torque of hip joint},
abstract = {Three dimensional stress calculation of competitive swimwear using anisotropic hyperelastic model considering stress softening was investigated in this paper. An anisotropic hyperelastic model considering stress softening of swimwear fabrics was introduced in order to reproduce the mechanical characteristics of swimwear fabrics on the analysis. The cyclic tensile loading test was carried out to evaluate the mechanical characteristics of the swimwear fabrics. From the test results, the mechanical characteristics of swimwear fabrics show strong-anisotropy and the stiffness of the fabrics shows hardening along with the increase of stretch. Also, the test results show reduction of stiffness which depended on the maximum deformation previously reached in the history of the swimwear fabrics. From the test results, material parameters of the anisotropic hyperelastic model and the stress softening model were approximated. The theoretical calculations were in good agreements with experimental data. In addition, the pressure measurement tests were conducted to measure the pressure of swimwear tightening the cylinder. The theoretical pressure calculated by the proposed model showed similar trend of pressure measurement tests. Finally, 3-dimentional stress calculation of swimwear was conducted using the anisotropic hyperelastic model considering stress softening. The stress calculation enabled the visualization of stress distributions of swimwear. In addition, the torque generated in right and left hip joints were calculated by stress calculation of swimwear. The stress calculation investigated in this study enabled the new design of competitive swimwear considering the torque generated in hip joint.}
}
@article{HEIDEN2021387,
title = {Framing Artificial Intelligence (AI) Additive Manufacturing (AM)},
journal = {Procedia Computer Science},
volume = {186},
pages = {387-394},
year = {2021},
note = {14th International Symposium "Intelligent Systems},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.161},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921009923},
author = {Bernhard Heiden and Volodymyr Alieksieiev and Matthias Volk and Bianca Tonino-Heiden},
keywords = {Artificial Intelligence, AI, Additive Manufacturing, AM, Production, Production Technology, Production Control, Production Engineering, System Theory, Graph Theory},
abstract = {Nowadays AM is a rapidly growing and emerging discipline in manufacturing, as well as AI is in informational applications. Both are related to logistical and self-referential/-copying concepts which make them scalable. What is in AM osmotic mass production spreading is in AI-related Cyber-Physical Systems (CPS) the osmotic computational approach. AI-AM self-propagatedly framed is itself an emerging field, which can be logically or systematically unified. The paper investigates firstly recent developments in the field of the AM process flow and how it is related to AI applications. The result is a list of logistical, organisational, and industrial process steps as well as modern and future AI-AM applications. The extended approach then gives prospect to a meta-perspectively embedded osmotic decentralized computing, as well as an osmotic manufacturing paradigm, which utilizes glocal functions, concerning local production as well as global distributed material and information transport nets and their connection graphs.}
}
@article{2021iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {179},
pages = {iii-ix},
year = {2021},
note = {5th International Conference on Computer Science and Computational Intelligence 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(21)00447-6},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921004476}
}
@article{CASADEI2021104081,
title = {Engineering collective intelligence at the edge with aggregate processes},
journal = {Engineering Applications of Artificial Intelligence},
volume = {97},
pages = {104081},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.104081},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620303389},
author = {Roberto Casadei and Mirko Viroli and Giorgio Audrito and Danilo Pianini and Ferruccio Damiani},
keywords = {Computational collective intelligence, Self-organisation, Distributed computing, Aggregate programming, Computational fields, Multi-agent systems},
abstract = {Edge computing promotes the execution of complex computational processes without the cloud, i.e., on top of the heterogeneous, articulated, and possibly mobile systems composed of IoT and edge devices. Such a pervasive smart fabric augments our environment with computing and networking capabilities. This leads to a complex and dynamic ecosystem of devices that should not only exhibit individual intelligence but also collective intelligence—the ability to take group decisions or process knowledge among autonomous units of a distributed environment. Self-adaptation and self-organisation mechanisms are also typically required to ensure continuous and inherent toleration of changes of various kinds, to distribution of devices, energy available, computational load, as well as faults. To achieve this behaviour in a massively distributed setting like edge computing demands, we seek for identifying proper abstractions, and engineering tools therefore, to smoothly capture collective behaviour, adaptivity, and dynamic injection and execution of concurrent distributed activities. Accordingly, we elaborate on a notion of “aggregate process” as a concurrent collective computation whose execution and interactions are sustained by a dynamic team of devices, whose spatial region can opportunistically vary over time. We ground this notion by extending the aggregate computing model and toolchain with new constructs to instantiate aggregate processes and regulate key aspects of their lifecycle. By virtue of an open-source implementation in the ScaFi framework, we show basic programming examples as well as case studies of edge computing, evaluated by simulation in realistic settings.}
}
@article{AI2020102393,
title = {Crack propagation and dynamic properties of coal under SHPB impact loading: Experimental investigation and numerical simulation},
journal = {Theoretical and Applied Fracture Mechanics},
volume = {105},
pages = {102393},
year = {2020},
issn = {0167-8442},
doi = {https://doi.org/10.1016/j.tafmec.2019.102393},
url = {https://www.sciencedirect.com/science/article/pii/S0167844219302678},
author = {Dihao Ai and Yuechao Zhao and Qifei Wang and Chengwu Li},
keywords = {, Coal, Crack propagation, Dynamic mechanical properties, Peridynamic model},
abstract = {To investigate the crack propagation and dynamic mechanical properties of coal under high strain rate loading, 24 sets of Brazilian disk (BD) coal specimens with vertical and horizontal beddings were made, and tests were conducted by a split Hopkinson pressure bar (SHPB) system. Under different impact velocities, a high-frame and high-resolution camera was employed to capture the fracture process, and two high-dynamic strain gauges were used to record the stress pulse signals simultaneously. Using one-dimensional stress wave theory, the dynamic mechanical properties of coal with different bedding directions under different impact velocities were analyzed and discussed. Experimental results indicate that bedding directions not only have a major influence on dynamic mechanical properties such as dynamic tensile strength, strain rate and strain energy but also have a great influence on the crack propagation path. Then, based on the image processing technique and fractal method, cracks conforming to the fractal have been proven, and the results further illustrate that the fractal dimension of cracks on the coal surface increased in the fracture process under SHPB loading. Finally, a numerical model based on bond-based Peridynamic theory was proposed to simulate crack propagation and dynamic mechanical properties of coal under the SHPB test, and the displacement-strain-stress fields were also calculated, which further reveal the fracture mechanism and dynamic behavior of coal under different impact loading conditions.}
}
@article{RAUSCH2021259,
title = {Optimized container scheduling for data-intensive serverless edge computing},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {259-271},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2030399X},
author = {Thomas Rausch and Alexander Rashed and Schahram Dustdar},
keywords = {Edge computing, Serverless, Container scheduling, Machine learning},
abstract = {Operating data-intensive applications on edge systems is challenging, due to the extreme workload and device heterogeneity, as well as the geographic dispersion of compute and storage infrastructure. Serverless computing has emerged as a compelling model to manage the complexity of such systems, by decoupling the underlying infrastructure and scaling mechanisms from applications. Although serverless platforms have reached a high level of maturity, we have found several limiting factors that inhibit their use in an edge setting. This paper presents a container scheduling system that enables such platforms to make efficient use of edge infrastructures. Our scheduler makes heuristic trade-offs between data and computation movement, and considers workload-specific compute requirements such as GPU acceleration. Furthermore, we present a method to automatically fine-tune the weights of scheduling constraints to optimize high-level operational objectives such as minimizing task execution time, uplink usage, or cloud execution cost. We implement a prototype that targets the container orchestration system Kubernetes, and deploy it on an edge testbed we have built. We evaluate our system with trace-driven simulations in different infrastructure scenarios, using traces generated from running representative workloads on our testbed. Our results show that (a) our scheduler significantly improves the quality of task placement compared to the state-of-the-art scheduler of Kubernetes, and (b) our method for fine-tuning scheduling parameters helps significantly in meeting operational goals.}
}
@article{FARBAR201499,
title = {Subsonic flow boundary conditions for the direct simulation Monte Carlo method},
journal = {Computers & Fluids},
volume = {102},
pages = {99-110},
year = {2014},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2014.06.025},
url = {https://www.sciencedirect.com/science/article/pii/S0045793014002643},
author = {Erin Farbar and Iain D. Boyd},
keywords = {Direct simulation Monte Carlo, Boundary conditions, Deposition modeling, Rarefied flow},
abstract = {The performance of different types of boundary conditions used in direct simulation Monte Carlo models of subsonic flows for single species flow fields is assessed in this study. An inlet boundary condition that permits the specification of pressure and temperature is investigated, as well as one that permits the specification of mass flow rate and temperature. A new inlet boundary condition is suggested that permits the total mass flow rate to be specified without requiring inter-processor communication during a parallel computation. Two types of outlet boundary conditions are investigated, one in which the outlet pressure is maintained by controlling the porosity of the outlet surface, and another in which particles are introduced at the outlet with properties such that the desired pressure is maintained. Each of the boundary conditions investigated produce flow field results for a single species, microchannel flow that are in good agreement with an analytical solution of the flow field and previously reported results. However, a simulation of a more complicated multi-species flow inside a vapor deposition reactor shows that some of the predicted flow field and surface properties are a function of the type of outlet boundary condition used.}
}
@article{SIMAO2019674,
title = {GC-Wise: A Self-adaptive approach for memory-performance efficiency in Java VMs},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {674-688},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18304898},
author = {J. Simão and S. Esteves and André Pires and L. Veiga},
keywords = {Memory management, Machine learning, Java virtual machine},
abstract = {High-level language runtimes are ubiquitous in every cloud deployment. From the geo-distributed heavy resources cloud provider to the new Fog and Edge deployment paradigms, all rely on these runtimes for portability, isolation and resource management. Across these clouds, an efficient resource management of several managed runtimes involves limiting the heap size of some VMs so that extra memory can be assigned to higher priority workloads. The challenges in this approach rely on the potential scale of systems and the need to make decisions in an application-driven way, because performance degradation can be severe, and therefore it should be minimized. Also, each tenant tends to repeat the execution of applications with similar memory-usage patterns, giving opportunity to reuse parameters known to work well for a given workload. This paper presents GC-Wise, a system to determine, at run-time, the best values for critical heap management parameters of the OpenJDK JVM, aiming to maximize memory-performance efficiency. GC-Wise comprises two main phases: 1) a training phase where it collects, with different heap resizing policies, representative execution metrics during the lifespan of a workload; and 2) an execution phase where an oracle matches the execution parameters of new workloads against those of already seen workloads, and enforces the best heap resizing policy. Distinctly from other works, the oracle can also decide upon unknown workloads. Using representative applications and different hardware setting (a resourceful server and a fog-like device), we show that our approach can lead to significant memory savings with low-impact on the throughput of applications. Furthermore, we show that we can predict with high accuracy the best heap resizing configuration in a relatively short period of time.}
}
@article{ZHOU2014259,
title = {Strength homogenization of matrix-inclusion composites using the linear comparison composite approach},
journal = {International Journal of Solids and Structures},
volume = {51},
number = {1},
pages = {259-273},
year = {2014},
issn = {0020-7683},
doi = {https://doi.org/10.1016/j.ijsolstr.2013.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0020768313003910},
author = {Meng-Meng Zhou and Günther Meschke},
keywords = {Strength homogenization, Multiscale model, Linear comparison composite, Matrix-inclusion composite, Micromechanics},
abstract = {A homogenization procedure to estimate the macroscopic strength of nonlinear matrix-inclusion composites with different strength characteristics of the matrix and inclusions, respectively, is presented in this paper. The strength up-scaling is formulated within the framework of the yield design theory and the linear comparison composite (LCC) approach, introduced by Ponte Castaneda (2002) and extended to frictional models by Ortega et al. (2011), which estimates the macroscopic strength of composite materials in terms of an optimally chosen linear thermo–elastic comparison composite with a similar underlying microstructure. In the paper various combinations for the underlying material behavior for the individual phases of the composite are considered: The matrix phase can be a quasi frictional material characterized either by a Drucker–Prager-type (hyperbolic) or an elliptical strength criterion, which predicts a strength limit also in hydrostatic compression, while the inclusion phase either may represent empty pores, pore voids filled with a pore fluid, rigid inclusions, or solid inclusions, whose strength characteristics also maybe described by a Drucker–Prager-type or an elliptical strength criterion. For generating the homogenized strength criterion efficiently in such general cases of matrix-inclusion composites, a novel algorithm is proposed in the paper. The validation of the proposed strength homogenization procedure for selected combinations of strength characteristics of the matrix material and the inclusions is conducted by comparisons with experimental results and alternative existing strength homogenization models.}
}
@article{FERRER2016140,
title = {Inter-cloud Research: Vision for 2020},
journal = {Procedia Computer Science},
volume = {97},
pages = {140-143},
year = {2016},
note = {2nd International Conference on Cloud Forward: From Distributed to Complete Computing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.08.292},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916321081},
author = {Ana Juan Ferrer},
keywords = {Inter-Cloud, Multi-Cloud, Hybrid Cloud},
abstract = {Inter-cloud Challenges, Expectations and Issues Cluster objective is to enable collaboration among European Research projects addressing topics of multi-cloud and inter-cloud. Today these projects analyze the question from diverse perspectives and focusing on specific parts of the problem. This position paper provides the work done in collaboration by all these projects to define research areas and challenges for 2020. It identifies a Cluster's vision of Inter-Cloud topics development by 2020, as well as, research areas in order to realize the provided vision. An extended version of this work is available on the Inter-Cloud Cluster position paper 1,2.}
}
@incollection{KUMAR202181,
title = {Chapter 6 - IoT services in healthcare industry with fog/edge and cloud computing},
editor = {Sanjay Kumar Singh and Ravi Shankar Singh and Anil Kumar Pandey and Sandeep S. Udmale and Ankit Chaudhary},
booktitle = {IoT-Based Data Analytics for the Healthcare Industry},
publisher = {Academic Press},
pages = {81-103},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-821472-5},
doi = {https://doi.org/10.1016/B978-0-12-821472-5.00017-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821472500017X},
author = {Dinesh Kumar and Ashish Kumar Maurya and Gaurav Baranwal},
keywords = {Internet of Things, Fog computing, Edge computing, Cloud computing, Healthcare industry},
abstract = {A continuous growth in human population has made availability of healthcare services a challenge. In recent past, various online healthcare applications have been proposed, but they were not reachable to common people because of unavailability of suitable and cheap handheld devices. A few years back, when Internet of Things (IoT) was introduced with clear architecture and smartphones came into the daily routine of humans, tremendous growth is observed in the IoT technology. Though smart city, smart farming, and smart parking are various applications of IoT, healthcare domain has emerged as one of the most popular domains of IoT technology. IoT along with cloud computing, fog computing, and mobile edge computing are some promising technologies behind the buildup of an advanced, digital, and smart healthcare system. Automated patient monitoring, activity tracking, measuring heart rate, calculating calorie burn/intake, etc. are some of the tasks performed by IoT devices attached to sensors in healthcare systems. In this chapter the role of fog and cloud computing in an IoT-based healthcare system is presented along with detailed technical aspects of each of the technologies for the realization of a complete and efficient IoT-based healthcare system. Several aspects of these technologies in healthcare such as the IoT-Fog-Cloud continuum, the standard platform that facilitates the communications among these different layers and types of fog devices, have been discussed. Along with this a separate discussion is given on how IoT-based healthcare can be integrated and implemented using emerging computing technologies. At last, various research challenges and future directions are provided.}
}
@article{MINANO2015272,
title = {A new approach to modeling isotropic damage for Mullins effect in hyperelastic materials},
journal = {International Journal of Solids and Structures},
volume = {67-68},
pages = {272-282},
year = {2015},
issn = {0020-7683},
doi = {https://doi.org/10.1016/j.ijsolstr.2015.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S0020768315002000},
author = {Mar Miñano and Francisco Javier Montáns},
keywords = {Damage, Hyperelasticity, Logarithmic strains, Mullins effect, Polymers, Biological tissues},
abstract = {In this work we present a new approach to damage mechanics in hyperelastic materials and an efficient numerical procedure for modeling the Mullins effect in isochoric, isotropic materials. The formulation is based on the idea that both the virgin loading and the damaged unloading–reloading behavior may be measured, but only the unloading–reloading curve corresponds to hyperelastic behavior. The damaged unloading–reloading curve is the true hyperelastic behavior and may be described by any suitable hyperelastic constitutive model. We employ a spline-based formulation which is known to exactly capture the behavior. The virgin loading curve, which does not correspond to hyperelastic behavior and involves damage evolution is only employed to compute the energy release rate. The procedure does not employ any material parameter (and hence no parameter-fitting procedure) or any explicit damage evolution function. We highlight similarities and differences of the present model with usual damage mechanics models and with pseudo-elasticity. As a result of the detailed computational procedure which simply involves the solution of a nonlinear scalar function, the virgin loading curve and the damaged unloading–reloading curves are exactly captured. The computational algorithm for three-dimensional implicit finite element analysis is also addressed in detail. Examples show that there is no significant increase in computational effort respect to a pure hyperelastic model.}
}
@article{JERNKVIST2014383,
title = {Multi-field modelling of hydride forming metals Part II: Application to fracture},
journal = {Computational Materials Science},
volume = {85},
pages = {383-401},
year = {2014},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2013.11.035},
url = {https://www.sciencedirect.com/science/article/pii/S0927025613007180},
author = {L.O. Jernkvist},
keywords = {Hydrogen, Hydride, Diffusion, Embrittlement, Fracture, Phase transformation, Zirconium, Titanium},
abstract = {In Part I of the present article, we formulated a continuum-based computational model for stress- and temperature-directed diffusion of hydrogen in metals that form brittle binary hydrides, such as Zr and Ti alloys. Among the space–time dependent parameters calculated by the model are the volume fraction and the mean orientation of hydride precipitates. These parameters are of importance for quantifying the embrittlement of hydrided materials. In this second part of the work, we use measured data for the strength and toughness of hydrided Zr alloys to correlate the local fracture properties of the two-phase (metal+hydride) material to the aforementioned parameters. The local fracture properties are used as space–time dependent input to a cohesive zone type submodel for fracture, which is fully integrated with the hydrogen transport model from Part I. The complete model is validated against fracture tests on hydrogen-charged Zr–2.5%Nb, a material used in nuclear reactor pressure tubes. More precisely, we study local embrittlement and crack initiation at a blunt and moderately stressed notch, resulting from gradual accumulation of hydrides at the notch during temperature cycling. We also simulate tests on crack initiation and growth by delayed hydride cracking, a subcritical crack growth mechanism with a complex temperature dependence. From the results of the simulations, we conclude that the model reproduces many observed features related to initiation and propagation of hydride induced cracks in the Zr–2.5%Nb material. In particular, it has the capacity to reproduce effects of the material’s temperature history on the fracture behaviour, which is important for many practical applications.}
}
@article{ZIETSCH2020351,
title = {Enabling smart manufacturing through a systematic planning framework for edge computing},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {31},
pages = {351-369},
year = {2020},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2020.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1755581720300754},
author = {Jakob Zietsch and Marcus Vogt and Benjamin D. Lee and Christoph Herrmann and Sebastian Thiede},
keywords = {Edge computing, IT resource planning, Decision support system, IT infrastructure},
abstract = {In the context of the fourth industrial revolution, an increasing number of advances in manufacturing result from smart integration of information technology (IT) and data-driven solutions in factories. An essential limiting factor for improvements is the capability of the IT system. In addition to the IT hardware specifications, the choice of an appropriate IT architecture (Cloud, Fog, or Edge Computing driven) determines the overall investment, environmental impact, and resulting net benefit. This article presents a comprehensive systematic approach that seeks to support the choice of an appropriate IT infrastructure. It is deliberately aimed at supporting especially non-IT experts, guiding the practitioner through a multi-phase process. The approach is applied to an exemplary use case in the field of industrial technical building equipment, but can also be applied to any application case involving the planning of the IT infrastructure in a manufacturing context. The results illustrate the applicability and the potential of appropriately selecting a suitable IT infrastructure.}
}
@article{HUBNER2019216,
title = {Optimization of the porous material described by the Biot model},
journal = {International Journal of Solids and Structures},
volume = {156-157},
pages = {216-233},
year = {2019},
issn = {0020-7683},
doi = {https://doi.org/10.1016/j.ijsolstr.2018.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0020768318303342},
author = {D. Hübner and E. Rohan and V. Lukeš and M. Stingl},
keywords = {Porous media, Shape optimization, Material optimization, Homogenization, Biot model, Sensitivity analysis, Two-scale modelling},
abstract = {The paper is devoted to the shape optimization of microstructures generating porous locally periodic materials saturated by viscous fluids. At the macroscopic level, the porous material is described by the Biot model defined in terms of the effective medium coefficients, involving the drained skeleton elasticity, the Biot stress coupling, the Biot compressibility coefficients, and by the hydraulic permeability of the Darcy flow model. By virtue of the homogenization, these coefficients are computed using characteristic responses of the representative unit cell consisting of an elastic solid skeleton and a viscous pore fluid. For the purpose of optimization, the sensitivity analysis on the continuous level of the problem is derived. We provide sensitivities of objective functions constituted by the Biot model coefficients with respect to the underlying pore shape described by a B-spline box which embeds the whole representative cell. We consider material design problems in the framework of which the layout of a single representative cell is optimized. Then we propose a sequential linearization approach to the two-scale problem in which local microstructures are optimized with respect to macroscopic design criteria. Numerical experiments are reported which include stiffness maximization with constraints guaranteeing minimum required permeability, and vice versa. Issues of the design and anisotropy and the spline box parametrization are discussed. In order to avoid remeshing, a geometric regularization technique based on injectivity constraints is applied.}
}
@article{ISKHAKOV2019507,
title = {Expansion and deterioration of concrete due to ASR: Micromechanical modeling and analysis},
journal = {Cement and Concrete Research},
volume = {115},
pages = {507-518},
year = {2019},
issn = {0008-8846},
doi = {https://doi.org/10.1016/j.cemconres.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0008884618301509},
author = {Tagir Iskhakov and Jithender J. Timothy and Günther Meschke},
keywords = {ASR, Microcracks, Porous materials, Homogenization, Damage, Continuum micromechanics},
abstract = {A multi-scale micromechanics model is proposed to describe the expansion and deterioration of concrete due to Alkali-Silica Reaction (ASR). The mechanics of ASR induced deterioration of a Representative Elementary Volume (REV) of concrete is modeled through a synthesis of distributed microcracking and mean-field homogenization. At the microscale, ASR-gel-pressure induced microcrack growth in and around the reactive aggregates is modeled using the framework of linear elastic fracture mechanics. Mean-field homogenization across multiple scales is used to obtain the overall expansion and degradation of the material. By specifying the spatial distribution of the pressurizing gel, two different ASR mechanisms associated with “slowly” and “rapidly” reactive aggregates can be modeled. Experimental data for concrete degradation as a function of the macroscopic expansion is found to lie within the theoretical upper and lower bounds that characterize the distribution of the gel in the aggregate or the cement paste.}
}
@article{RISTOV2021368,
title = {AFCL: An Abstract Function Choreography Language for serverless workflow specification},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {368-382},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20302648},
author = {Sasko Ristov and Stefan Pedratscher and Thomas Fahringer},
keywords = {AWS Step Functions, Cost, FaaS, IBM Composer, Performance},
abstract = {Serverless workflow applications or function choreographies (FCs), which connect serverless functions by data- and control-flow, have gained considerable momentum recently to create more sophisticated applications as part of Function-as-a-Service (FaaS) platforms. Initial experimental analysis of the current support for FCs uncovered important weaknesses, including provider lock-in, and limited support for important data-flow and control-flow constructs. To overcome some of these weaknesses, we introduce the Abstract Function Choreography Language (AFCL) for describing FCs at a high-level of abstraction, which abstracts the function implementations from the developer. AFCL is a YAML-based language that supports a rich set of constructs to express advanced control-flow (e.g. parallelFor loops, parallel sections, dynamic loop iterations counts) and data-flow (e.g multiple input and output parameters of functions, DAG-based data-flow). We introduce data collections which can be distributed to loop iterations and parallel sections that may substantially reduce the delays for function invocations due to reduced data transfers between functions. We also support asynchronous functions to avoid delays due to blocking functions. AFCL supports properties (e.g. expected size of function input data) and constraints (e.g. minimize execution time) for the user to optionally provide hints about the behavior of functions and FCs and to control the optimization by the underlying execution environment. We implemented a prototype AFCL environment that supports AFCL as input language with multiple backends (AWS Lambda and IBM Cloud Functions) thus avoiding provider lock-in which is a common problem in serverless computing. We created two realistic FCs from two different domains and encoded them with AWS Step Functions, IBM Composer and AFCL. Experimental results demonstrate that our current implementation of the AFCL environment substantially outperforms AWS Step Functions and IBM Composer in terms of development effort, economic costs, and makespan.}
}
@article{MASS201758,
title = {Topology optimization for additive manufacturing: Accounting for overhang limitations using a virtual skeleton},
journal = {Additive Manufacturing},
volume = {18},
pages = {58-73},
year = {2017},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214860417301045},
author = {Yoram Mass and Oded Amir},
keywords = {Additive manufacturing, Topology optimization, 3-D printing, Overhang angles},
abstract = {This article proposes a new method for reducing the amount of support material required for 3-D printing of complex designs generated by topology optimization. This procedure relies on solving sequentially two structural optimization problems – the first on a discrete truss-based model and the second on a continuum-based model. In the optimization of the discrete model, the maximum overhang limitation is imposed based on geometrical parameters. The optimized discrete pattern is then projected on to the continuum so that it influences the material distribution in the continuum optimization. The method is explained and investigated on a set of test cases in 2-D and subsequently demonstrated on examples in 3-D. Numerical results indicate that the designs obtained by this approach exhibit improved printability as they have fewer overhanging features. In some cases, practically no supporting material will be required for printing the optimized design.}
}
@incollection{2017473,
title = {Subject Index},
editor = {Houbing Song and Danda B. Rawat and Sabina Jeschke and Christian Brecher},
booktitle = {Cyber-Physical Systems},
publisher = {Academic Press},
address = {Boston},
pages = {473-484},
year = {2017},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-803801-7},
doi = {https://doi.org/10.1016/B978-0-12-803801-7.09986-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128038017099867}
}
@article{ROBERTS2014923,
title = {Application of the digital volume correlation technique for the measurement of displacement and strain fields in bone: A literature review},
journal = {Journal of Biomechanics},
volume = {47},
number = {5},
pages = {923-934},
year = {2014},
issn = {0021-9290},
doi = {https://doi.org/10.1016/j.jbiomech.2014.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0021929014000037},
author = {Bryant C. Roberts and Egon Perilli and Karen J. Reynolds},
keywords = {Digital volume correlation, Trabecular bone, Strain measurement, Displacement},
abstract = {Digital volume correlation (DVC) provides experimental measurements of displacements and strains throughout the interior of porous materials such as trabecular bone. It can provide full-field continuum- and tissue-level measurements, desirable for validation of finite element models, by comparing image volumes from subsequent µCT scans of a sample in unloaded and loaded states. Since the first application of DVC for measurement of strain in bone tissue, subsequent reports of its application to trabecular bone cores up to whole bones have appeared within the literature. An “optimal” set of procedures capable of precise and accurate measurements of strain, however, still remains unclear, and a systematic review focussing explicitly on the increasing number of DVC algorithms applied to bone or structurally similar materials is currently unavailable. This review investigates the effects of individual parameters reported within individual studies, allowing to make recommendations for suggesting algorithms capable of achieving high accuracy and precision in displacement and strain measurements. These recommendations suggest use of subsets that are sufficiently large to encompass unique datasets (e.g. subsets of 500µm edge length when applied to human trabecular bone cores, such as cores 10mm in height and 5mm in diameter, scanned at 15µm voxel size), a shape function that uses full affine transformations (translation, rotation, normal strain and shear strain), the robust normalized cross-correlation coefficient objective function, and high-order interpolation schemes. As these employ computationally burdensome algorithms, researchers need to determine whether they have the necessary computational resources or time to adopt such strategies. As each algorithm is suitable for parallel programming however, the adoption of high precision techniques may become more prevalent in the future.}
}
@article{YU201317,
title = {Soil–pipe interaction due to tunnelling: Assessment of Winkler modulus for underground pipelines},
journal = {Computers and Geotechnics},
volume = {50},
pages = {17-28},
year = {2013},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2012.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X12002297},
author = {Jian Yu and Chenrong Zhang and Maosong Huang},
keywords = {Winkler subgrade modulus, Tunnel excavation, Underground pipeline, Soil movement},
abstract = {One of the key problems in the implementation of Winkler models to analyze tunnelling effects on existing pipelines lies in the assessment of subgrade modulus under external soil displacement. In this paper, an expression of the Winkler subgrade modulus for a pipeline buried at arbitrary depth and subjected to free soil displacement with arbitrary curve shape is given. Using superposition principle and the Fourier integral, the subgrade modulus of an infinite beam resting on the surface of an elastic half space and buried infinitely are obtained respectively. Then the influence of embedment depth is estimated based on Mindlin and Kelvin solution. The validity of the proposed subgrade modulus is verified by comparison with the results from an elastic continuum solution and two centrifuge model tests for the responses of buried pipeline due to nearby tunnelling. Thereafter, parametric studies are shown to assess the accuracy of the proposed subgrade modulus by comparing with an elastic continuum solution in homogeneous and non-homogeneous soil stratum and the amount of error is estimated.}
}
@article{HERNANDEZNIEVES2021104327,
title = {CEBRA: A CasE-Based Reasoning Application to recommend banking products},
journal = {Engineering Applications of Artificial Intelligence},
volume = {104},
pages = {104327},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104327},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621001755},
author = {Elena Hernández-Nieves and Guillermo Hernández and Ana B. Gil-González and Sara Rodríguez-González and Juan M. Corchado},
keywords = {Case-based reasoning, Fog Computing, Virtual agents, Artificial intelligence, Fintech, Commercial banking},
abstract = {Following data ethics and respecting the clients’ privacy, the banking environment can use the client data that is available to them to offer personalized services to its clients. Intelligent recommender systems can support this attempt through specialized technological architectures. This article proposes the inclusion of CEBRA (CasE-Based Reasoning Application), a case-based reasoning system oriented to commercial banking, in a Fog Computing architecture coordinated by virtual agents. Throughout this article, the model of this architecture is presented and its life cycle is described, and improvements are proposed through the incorporation of several techniques in the retrieve and reuse phases, including the extraction of interests expressed by users on their social network profiles and collaborative filtering systems. A comprehensive case study has been carried out and a dataset of 60,000 cases has been generated to evaluate CEBRA. As a result, the Recommender System is presented, by including, the recommendation algorithm and a REST interface for its use. The recommendations are based on the user’s profile, previous ratings and/or additional knowledge such as the user’s contextual information. The proposal takes advantage of contextual information to support the promotion of banking and financial products, improving user satisfaction.}
}
@article{ZONI2021100450,
title = {An FPU design template to optimize the accuracy-efficiency-area trade-off},
journal = {Sustainable Computing: Informatics and Systems},
volume = {29},
pages = {100450},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100450},
url = {https://www.sciencedirect.com/science/article/pii/S2210537920301761},
author = {Davide Zoni and Andrea Galimberti and William Fornaciari},
keywords = {Floating Point Units (FPU), Accuracy-Cost-energy tradeoff, Run-time optimization},
abstract = {Modern embedded systems are in charge of an increasing number of tasks that extensively employ floating-point (FP) computations. The ever-increasing efficiency requirement, coupled with the additional computational effort to perform FP computations, motivates several microarchitectural optimizations of the FPU. This manuscript presents a novel modular FPU microarchitecture, which targets modern embedded systems and considers heterogeneous workloads including both best-effort and accuracy-sensitive applications. The design optimizes the EDP-accuracy-area figure of merit by allowing, at design-time, to independently configure the precision of each FP operation, while the FP dynamic range is kept common to the entire FPU to deliver a simpler microarchitecture. To ensure the correct execution of accuracy-sensitive applications, a novel compiler pass allows to substitute each FP operation for which a low-precision hardware support is offered with the corresponding soft-float function call. The assessment considers seven FPU variants encompassing three different state-of-the-art designs. The results on several representative use cases show that the binary32 FPU implementation offers an EDP gain of 15%, while, in case the FPU implements a mix of binary32 and bfloat16 operations, the EDP gain is 19%, the reduction in the resource utilization is 21% and the average accuracy loss is less than 2.5%. Moreover, the resource utilization of our FPU variants is aligned with the one of the FPU employing state-of-the-art, highly specialized FP hardware accelerators. Starting from the assessment, a set of guidelines is drawn to steer the design of the FP hardware support in modern embedded systems.}
}
@article{VAUGHN2019210,
title = {Statistically informed upscaling of damage evolution in brittle materials},
journal = {Theoretical and Applied Fracture Mechanics},
volume = {102},
pages = {210-221},
year = {2019},
issn = {0167-8442},
doi = {https://doi.org/10.1016/j.tafmec.2019.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167844218305068},
author = {N. Vaughn and A. Kononov and B. Moore and E. Rougier and H. Viswanathan and A. Hunter},
keywords = {Brittle fracture, Finite-discrete element method, Effective elastic moduli, Crack statistics},
abstract = {The presence and growth of micro-cracks degrade the strength of brittle solids, greatly impacting the overall material response. Hence, the evolution of these micro-cracks must be accounted for in models describing the relationship between stress and strain so that accurate predictions of material failure can be made. The evolution of individual cracks and crack networks can be simulated with high-fidelity microscale models utilizing highly resolved meshes that can be computationally expensive to a point in which it limits the simulation scale. Hence, for many engineering applications that require simulations of large components, continuum-scale models, which cannot explicitly resolve individual cracks and thus lose important physical information, are required. In this work, we bridge these two scales by developing and implementing a continuum-scale effective moduli constitutive model that is informed by crack statistics generated from a high-fidelity model resolved using a finite-discrete element method (FDEM) implementation. Using statistical information describing the evolution of crack lengths and orientations, this model can capture the effects of brittle damage evolution without the need to resolve individual cracks. We have successfully captured the stress-strain behavior of the high-fidelity simulations using the statistics-based constitutive model shown through direct comparison of stress-strain curves. The curves match within error bars present in the strain-softening portions of the stress-strain curve of the high-fidelity results due to the statistical variation of the initial pre-existing crack network. The stress-strain curves are also compared to experimental results for similar loading conditions and show good qualitative agreement.}
}
@article{BLAIS2016201,
title = {Development of an unresolved CFD–DEM model for the flow of viscous suspensions and its application to solid–liquid mixing},
journal = {Journal of Computational Physics},
volume = {318},
pages = {201-221},
year = {2016},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2016.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0021999116301358},
author = {Bruno Blais and Manon Lassaigne and Christoph Goniva and Louis Fradette and François Bertrand},
keywords = {Solid–liquid mixing, Multiphase flows, Computational fluid dynamics, Discrete element method, CFD–DEM},
abstract = {Although viscous solid–liquid mixing plays a key role in the industry, the vast majority of the literature on the mixing of suspensions is centered around the turbulent regime of operation. However, the laminar and transitional regimes face considerable challenges. In particular, it is important to know the minimum impeller speed (Njs) that guarantees the suspension of all particles. In addition, local information on the flow patterns is necessary to evaluate the quality of mixing and identify the presence of dead zones. Multiphase computational fluid dynamics (CFD) is a powerful tool that can be used to gain insight into local and macroscopic properties of mixing processes. Among the variety of numerical models available in the literature, which are reviewed in this work, unresolved CFD–DEM, which combines CFD for the fluid phase with the discrete element method (DEM) for the solid particles, is an interesting approach due to its accurate prediction of the granular dynamics and its capability to simulate large amounts of particles. In this work, the unresolved CFD–DEM method is extended to viscous solid–liquid flows. Different solid–liquid momentum coupling strategies, along with their stability criteria, are investigated and their accuracies are compared. Furthermore, it is shown that an additional sub-grid viscosity model is necessary to ensure the correct rheology of the suspensions. The proposed model is used to study solid–liquid mixing in a stirred tank equipped with a pitched blade turbine. It is validated qualitatively by comparing the particle distribution against experimental observations, and quantitatively by compairing the fraction of suspended solids with results obtained via the pressure gauge technique.}
}
@article{HUANG2020103350,
title = {Instability mechanism of shallow tunnel in soft rock subjected to surcharge loads},
journal = {Tunnelling and Underground Space Technology},
volume = {99},
pages = {103350},
year = {2020},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2020.103350},
url = {https://www.sciencedirect.com/science/article/pii/S0886779819311721},
author = {Feng Huang and Chuangzhou Wu and Bo-An Jang and Y Hong and Ning Guo and Wei Guo},
keywords = {Tunnel, Soft rock, Surcharge loading, Physical model test, Coupled numerical simulation, DIC method},
abstract = {The tunnel instability accidents caused by surcharge loading could even lead to ground settlement or collapse, damage to the existing municipal pipelines or inclination of the surgical buildings. Series of large-scale physical model test was conducted in this paper to reveal the failure mechanism of tunnel in soft rock subjected to surcharge loading. The non-contact full-field displacement measurement (i.e. Digital Image Correlation, DIC) was employed to obtain a detailed deformation of surrounding rock after excavation. A coupled numerical method combing the continuum (Finite Difference Method, FDM) and dis-continuum (Discrete Element Method, DEM) analysis was used to deal with the failure mechanism of tunnel excavation in rock mass. Good agreements between physical model test and numerical analysis approve the accuracy of the proposed FDM-DEM numerical model. It is found that the collapse mainly occurred in tunnel roof rather than in tunnel sidewall. The collapse zone shows a step-type increase with the increase of surcharge loads. The stress loosening zone (SLZ) in tunnel roof is larger than that in tunnel sidewall. The thickness of SLZ in tunnel roof can be used to design the length of the formed pressure anchors. The proposed large-scale model test with DIC method is applicable to the tunnel engineering study. Moreover, the results in this paper give some insights to secure excavation works and optimize supporting structures.}
}
@article{KOCHOVSKI2018182,
title = {Supporting smart construction with dependable edge computing infrastructures and applications},
journal = {Automation in Construction},
volume = {85},
pages = {182-192},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517304776},
author = {Petar Kochovski and Vlado Stankovski},
keywords = {Smart construction, Dependability, Internet of Things, Container-based systems, Edge computing},
abstract = {The Internet of Things (IoT) such as the use of robots, sensors, actuators, electronic signalization and a variety of other Internet enabled physical devices may provide for new advanced smart applications to be used in construction in the very near future. Such applications require real-time responses and are therefore time-critical. Therefore, in order to support collaboration, control, monitoring, supply management, safety and other construction processes, they have to meet dependability requirements, including requirements for high Quality of Service (QoS). Dependability and high QoS can be achieved by using adequate number and quality of computing resources, such as processing, memory and networking elements, geographically close to the smart environments. The goal of this study is to develop a practical edge computing architecture and design, which can be used to support smart construction environments with high QoS. This study gives particular attention to the solution design, which relies on latest cloud and software engineering approaches and technologies, and provides elasticity, interoperability and adaptation to companies' specific needs. Two edge computing applications supporting video communications and construction process documentation are developed and demonstrate a viable edge computing design for smart construction.}
}
@article{BAEK2021110385,
title = {Problem-fluent models for complex decision-making in autonomous materials research},
journal = {Computational Materials Science},
volume = {193},
pages = {110385},
year = {2021},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2021.110385},
url = {https://www.sciencedirect.com/science/article/pii/S0927025621001105},
author = {Soojung Baek and Kristofer G. Reyes},
keywords = {Autonomy, Machine learning, Artificial intelligence, Physics-aware},
abstract = {We review our recent work in the area of autonomous materials research, highlighting the coupling of machine learning methods and models and more problem-aware modeling. We review the general Bayesian framework for closed-loop design employed by many autonomous materials platforms. We then provide examples of our work on such platforms. We finally review our approaches to extend current statistical and ML models to better reflect problem specific structure including the use of physics-based models and incorporation operational considerations into the decision-making procedure.}
}
@article{ANSSARIBENAM2020103645,
title = {Rate-dependent mechanical behaviour of semilunar valves under biaxial deformation: From quasi-static to physiological loading rates},
journal = {Journal of the Mechanical Behavior of Biomedical Materials},
volume = {104},
pages = {103645},
year = {2020},
issn = {1751-6161},
doi = {https://doi.org/10.1016/j.jmbbm.2020.103645},
url = {https://www.sciencedirect.com/science/article/pii/S1751616119318466},
author = {Afshin Anssari-Benam and Yuan-Tsan Tseng and Gerhard A. Holzapfel and Andrea Bucchi},
keywords = {Semilunar valves, Rate-dependency, Mechanical behaviour, Physiological rate},
abstract = {In this study we investigate the rate-dependency of the mechanical behaviour of semilunar heart valves under biaxial deformation, from quasi-static to physiological loading rates. This work extends and complements our previous undertaking, where the rate-dependency in the mechanical behaviour of semilunar valve specimens was documented in sub-physiological rate domains (Acta Biomater. 2019; https://doi.org/10.1016/j.actbio.2019.02.008). For the first time we demonstrate herein that the stress-stretch curves obtained from specimens under physiological rates too are markedly different to those at sufficiently lower rates and at quasi-static conditions. The results importantly underline that the mechanical behaviour of semilunar heart valves is rate dependent, and the physiological mechanical behaviour of the valves may not be correctly obtained via material characterisation tests at arbitrary low deformation rates. Presented results in this work provide an inclusive dataset for material characterisation and modelling of semilunar heart valves across a 10,000 fold deformation rate, both under equi-biaxial and 1:3 ratio deformation rates. The important application of these results is to inform the development of appropriate mechanical testing protocols, as well as devising new models, for suitable determination of the rate-dependent constitutive mechanical behaviour of the semilunar valves.}
}
@article{GILL2021100391,
title = {A comprehensive study of simulation frameworks and research directions in fog computing},
journal = {Computer Science Review},
volume = {40},
pages = {100391},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100391},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000319},
author = {Monika Gill and Dinesh Singh},
keywords = {Fog computing, Edge, Simulation},
abstract = {Context:
Fog computing paradigm consists of resource constrained devices that support data processing and service provisioning at the edge of the network. Simulation frameworks play a key role in the design, development and validation of novel approaches for fog environment. The existing fog simulators model one or more aspects of fog environment and hence it becomes a tedious task to analyse and choose them as per the research requirements.
Objective:
This paper reviews the literature of simulation tools for fog computing and aims to help the novice researchers to explore and assess fog related proposals.
Method:
The study has employed a systematic search procedure to identify relevant articles published in the duration of 2015-2020.
Results:
The relevant publications are evaluated to highlight their strengths and underline the limitations. A comparative analysis of studies based on eight characteristic and few non technical features is presented. The scope for improvement in fog simulators is reported. Lastly, the prevailing research challenges in fog that can be addressed with reviewed simulation frameworks are detailed out.
Conclusion:
The paper has identified an increased interest in the development of novel and extended fog simulators thus emphasizing their importance. Also, the need to develop more advanced fog simulators that can model a wider range of fog environments is recognized. Directions are given for future work.}
}
@article{LEE2020102712,
title = {Constitutive-damage modeling and computational implementation for simulation of elasto-viscoplastic-damage behavior of polymeric foams over a wide range of strain rates and temperatures},
journal = {International Journal of Plasticity},
volume = {130},
pages = {102712},
year = {2020},
issn = {0749-6419},
doi = {https://doi.org/10.1016/j.ijplas.2020.102712},
url = {https://www.sciencedirect.com/science/article/pii/S0749641919305418},
author = {Jeong-Ho Lee and Dong-Man Ryu and Chi-Seung Lee},
keywords = {Polymeric foams, Constitutive behavior, Elastic-viscoplastic material, Finite elements, Numerical algorithms},
abstract = {The material behavior and damage of polymeric foams (e.g., polyurethane foam and polystyrene foam) are extremely complex under compression, depending on strain rates and temperatures. In the present study, the elasto-viscoplastic-damage behavior of polymeric foams over a wide range of strain rates and temperature is evaluated and predicted using the modified Gurson-Tvergaard-Needleman-Lemaitre (GTNL) model and the modified Khan-Huang-Liang (KHL) model. A modified GTNL model is introduced to describe phenomena of decreasing void volume fraction and elastic modulus for polymeric foams under uniaxial compression. The modified KHL model is proposed to evaluate the nonlinear strain rate- and temperature-dependent material behavior of polymeric foams. The full derivation of the implicit integration algorithm of the proposed modified GTNL-KHL model is introduced, and a user-defined material (UMAT) subroutine for commercial finite element analysis code (i.e., ABAQUS) is established. Using the UMAT, the damage characteristics (e.g., void volume fraction and elastic modulus changes) and the nonlinear material behavior (e.g., linear elastic, softening/plateau and densification) of polymeric foams under uniaxial compression with various strain rates and temperatures are successfully simulated.}
}
@article{LI2021103855,
title = {Rate dependency mechanism of crystalline rocks induced by impacts: Insights from grain-scale fracturing and micro heterogeneity},
journal = {International Journal of Impact Engineering},
volume = {155},
pages = {103855},
year = {2021},
issn = {0734-743X},
doi = {https://doi.org/10.1016/j.ijimpeng.2021.103855},
url = {https://www.sciencedirect.com/science/article/pii/S0734743X21000427},
author = {X.F. Li and H.B. Li and G.K. Zhang and M.H. Ju and Jian Zhao},
keywords = {Strain rate dependency, Impact, Multiscale fracturing, Grain-based model, Rock heterogeneity},
abstract = {Rocks in nature contain a large number of defects and exhibit strong heterogeneity on grain scale. In this study, a novel three-dimensional multiscale method is proposed to investigate the dynamic behaviors and microfracturing in granitic rocks. In this numerical method, the heterogeneity in mineral components is reproduced by a series of a space filling Voronoi tessellations and particle filling in subgrains as computational nodal points to allow for transgranular fracturing. The rupture strength, temporal deformation fields, and failure patterns are compared with the experimental results to verify the reasonability and accuracy of the proposed method. Then, the underlying mechanism of grain-scale fracturing leading to fractal fracture surfaces, pervasively grain pulverization and deflection/penetration cracking model are discussed. It's shown that the fracture surface roughness is fractal dominated by two competitive mechanisms. The crack initiation time of intergranular tensile crack, transgranular tensile crack and shear cracks is gradually increased. The ratio of the number of tensile cracks exceeds 90% and that value of intergranular crack decreases from 56% to 33% as the strain rate increases. The appearance of multicracks activation and the transition from intergranular fracturing to transgranular fracturing is the underlying mechanism of rate dependency for granitic rocks.}
}
@article{MITRAN2013193,
title = {Continuum-kinetic-microscopic model of lung clearance due to core-annular fluid entrainment},
journal = {Journal of Computational Physics},
volume = {244},
pages = {193-211},
year = {2013},
note = {Multi-scale Modeling and Simulation of Biological Systems},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.01.037},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113000909},
author = {Sorin Mitran},
keywords = {Multiscale, Lattice Boltzmann, Lattice Fokker–Plank, Core-annular, Lung clearance},
abstract = {The human lung is protected against aspirated infectious and toxic agents by a thin liquid layer lining the interior of the airways. This airway surface liquid is a bilayer composed of a viscoelastic mucus layer supported by a fluid film known as the periciliary liquid. The viscoelastic behavior of the mucus layer is principally due to long-chain polymers known as mucins. The airway surface liquid is cleared from the lung by ciliary transport, surface tension gradients, and airflow shear forces. This work presents a multiscale model of the effect of airflow shear forces, as exerted by tidal breathing and cough, upon clearance. The composition of the mucus layer is complex and variable in time. To avoid the restrictions imposed by adopting a viscoelastic flow model of limited validity, a multiscale computational model is introduced in which the continuum-level properties of the airway surface liquid are determined by microscopic simulation of long-chain polymers. A bridge between microscopic and continuum levels is constructed through a kinetic-level probability density function describing polymer chain configurations. The overall multiscale framework is especially suited to biological problems due to the flexibility afforded in specifying microscopic constituents, and examining the effects of various constituents upon overall mucus transport at the continuum scale.}
}
@article{MTHUNZI2020620,
title = {Cloud computing security taxonomy: From an atomistic to a holistic view},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {620-644},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19300871},
author = {Siyakha N. Mthunzi and Elhadj Benkhelifa and Tomasz Bosakowski and Chirine Ghedira Guegan and Mahmoud Barhamgi},
abstract = {Countless discussions around security challenges affecting cloud computing are often large textual accounts, which can be cumbersome to read and prone to misinterpretation. The growing reliance on cloud computing means that not only should we focus on evaluating its security challenges but devote greater attention towards how challenges are viewed and communicated. With many cloud computing implementations in use and a growing evolution of the cloud paradigm (including fog, edge and cloudlets), comprehending, correlating and classifying diverse perspectives to security challenges increasingly becomes critical. Current classifications are only suited for limited use; both as effective tools for research and countermeasures design. The taxonomic approach has been used as a modeling technique towards classifying concepts across many domains. This paper surveys multiple perspectives of cloud security challenges and systematically develops corresponding graphical taxonomy based upon meta-synthesis of important cloud security concepts in literature. The contributions and significance of this work are as follows: (1) a holistic view simplifies visualization for the reader by providing illustrative graphics of existing textual perspectives, highlighting entity relationships among cloud entities/players thereby exposing security areas at every layer of the cloud. (2) a holistic taxonomy that facilitates the design of enforcement or corrective countermeasures based upon the source or origin of a security incident. (3) a holistic taxonomy highlights security boundary and identifies apt areas to implement security countermeasures.}
}
@article{MANOLIS2021149,
title = {Mechanical models and numerical simulations in nanomechanics: A review across the scales},
journal = {Engineering Analysis with Boundary Elements},
volume = {128},
pages = {149-170},
year = {2021},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0955799721000898},
author = {George D. Manolis and Petia S. Dineva and Tsviatko Rangelov and Dimitris Sfyris},
keywords = {Nanomechanics, Atomic level, Multi-scale models, Continuum mechanics, Numerical methods},
abstract = {This work gives an overview of the theoretical background and of the numerical modelling framework used to describe the mechanical properties and the response of materials on scales ranging from the atomistic, through the microstructure and all the way up to the macroscale. In order to describe the dual nature of the structure of matter, which is continuous when viewed at large length scales and discrete when viewed at the atomic scale, plus the interdependence of these scales, multiscale modelling is required to complement the continuum and the atomistic models. More specifically, what we aim for in this review is to present and discuss the following basic conceptual models, as well as the methodologies that accompany them: (a) discrete models such as ab initio, atomistic / molecular, mesoscopic; (b) continuum mechanics models (CMM) comprising pure CMM, non-local elasticity CMM, higher-order strain gradient and higher-order nonlocal strain gradient elasticity CMM, and surface elasticity CMM; (c) multiscale material models (MMM). Since the field of nanomechanics is currently a rapidly expanding research area, the presented state-of-the art is by no means exhaustive. It simply outlines the research efforts that go behind formulating numerical models for the solution of problems in nanomechanics. Despite the advantages that boundary element methods (BEM) have in solving problems at the physical scale, either as stand-alone or in combination with finite element methods (FEM), their application to multiscale modelling is still limited, despite the promise they seem to hold.}
}
@article{SITTONCANDANEDO2019278,
title = {A review of edge computing reference architectures and a new global edge proposal},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {278-294},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1930264X},
author = {Inés Sittón-Candanedo and Ricardo S. Alonso and Juan M. Corchado and Sara Rodríguez-González and Roberto Casado-Vara},
keywords = {Edge computing, Internet of Things, Blockchain, Reference architecture, Industry 4.0, Agroindustry},
abstract = {Edge Computing represents the activities of IoT (Internet of Things) devices at the border or limit of the network connected to the remote Cloud. The latest research in this field has intended to demonstrate that Edge Computing architectures are the optimal solution to minimising latency, improving privacy and reducing bandwidth costs in IoT-based scenarios. This article is a review of the Edge Computing technology and its reference architectures proposed by the Edge Computing Consortium, Intel-SAP, the FAR-Edge Project and the Industrial Internet Consortium for Industry 4.0. Moreover, this article presents a proposal for a tiered architecture with a modular approach that allows to manage the complexity of solutions not only for Industry 4.0 environments but also for other scenarios such as smart cities, smart energy, healthcare or precision agrotechnology. The main contributions of the proposed architecture reside in the security and privacy provided by blockchain technologies. Finally, the proposed reference architecture is tested by building an IoT platform in a smart agroindustry scenario to reduce bandwidth costs between the Edge and the Cloud.}
}
@article{JEBAHI2020102617,
title = {Strain gradient crystal plasticity model based on generalized non-quadratic defect energy and uncoupled dissipation},
journal = {International Journal of Plasticity},
volume = {126},
pages = {102617},
year = {2020},
issn = {0749-6419},
doi = {https://doi.org/10.1016/j.ijplas.2019.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0749641919302694},
author = {Mohamed Jebahi and Lei Cai and Farid Abed-Meraim},
keywords = {Strain gradient crystal plasticity, Size effects, Internal length scales, Non-quadratic defect energy, Uncoupled dissipation},
abstract = {The present paper proposes a flexible Gurtin-type strain gradient crystal plasticity (SGCP) model based on generalized non-quadratic defect energy and uncoupled constitutive assumption for dissipative processes. A power-law defect energy, with adjustable order-controlling index n, is proposed to provide a comprehensive investigation into the energetic length scale effects under proportional and non-proportional loading conditions. Results of this investigation reveal quite different effects of the energetic length scale, depending on the value of n and the type of loading. For n ≥ 2, regardless of the loading type, the energetic length scale has only influence on the rate of the classical kinematic hardening, as reported in several SGCP works. However, in the range of n < 2, this parameter leads to unusual nonlinear kinematic hardening effects with inflection points in the macroscopic mechanical response, resulting in an apparent increase of the yield strength under monotonic loading. More complex effects, with additional inflection points, are obtained under non-proportional loading conditions, revealing new loading history memory-like effects of the energetic length scale. Concerning dissipation, to make the dissipative effects more easily controllable, dissipative processes due to plastic strains and their gradients are assumed to be uncoupled. Separate formulations, expressed using different effective plastic strain measures, are proposed to describe such processes. Results obtained using these formulations show the great flexibility of the proposed model in controlling some major dissipative effects, such as elastic gaps. A simple way to remove these gaps under certain non-proportional loading conditions is provided. Application of the proposed uncoupled formulations to simulate the mechanical response of a sheared strip has led to accurate prediction of the plastic strain distributions, which compare very favorably with those predicted using discrete dislocation mechanics.}
}
@article{AMORES2021108591,
title = {A finite strain non-parametric hyperelastic extension of the classical phenomenological theory for orthotropic compressible composites},
journal = {Composites Part B: Engineering},
volume = {212},
pages = {108591},
year = {2021},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2020.108591},
url = {https://www.sciencedirect.com/science/article/pii/S1359836820336374},
author = {Víctor J. Amores and Francisco J. {San Millán} and Ismael Ben-Yelun and Francisco J. Montáns},
keywords = {Orthotropy, Composites, Hyperelasticity, Auxetic foams, Bi-modulus materials},
abstract = {The phenomenological linear theory of orthotropic compressible materials is employed systematically in the engineering and scientific analysis of a large scope of bulk and composite materials. However, at finite elastic strains, multiscale and fiber-based parametric models are typically employed, which material parameters are fitted to macroscopic experimental data using optimization procedures. Phenomenological extensions of the linear theory capable of effectively modelling such a large scope of materials are not available. What we present in this work is a simple extension of the linear theory to finite strains such that at every deformation level, the infinitesimal theory is fully recovered. The model is based in non-parametric spline complementary energies employing an energy decomposition compatible with the classical infinitesimal expression at all strain levels. No material parameter is explicitly involved because the spline-based stress energies are numerically computed (not fitted) directly from experimental data. We show the applicability of the model to capture (1) the behavior of orthotropic bimodulus materials consistent with hyperelasticity (a novel formulation also presented herein), (2) orthotropic auxetic foams, and (3) composites at finite strains.}
}
@article{ARORA2021102917,
title = {Hydrogen assisted crack initiation in metals under monotonic loading: A new experimental approach},
journal = {Theoretical and Applied Fracture Mechanics},
volume = {112},
pages = {102917},
year = {2021},
issn = {0167-8442},
doi = {https://doi.org/10.1016/j.tafmec.2021.102917},
url = {https://www.sciencedirect.com/science/article/pii/S0167844221000252},
author = {Aman Arora and Rakesh Kumar and Harpreet Singh and Dhiraj K. Mahajan},
keywords = {Hydrogen embrittlement, Digital image correlation, Electron backscattered diffraction, Crack initiation, Stress maps},
abstract = {Hydrogen is foreseen as a promising energy carrier that can control global warming by reducing CO2 emissions. However, hydrogen is associated with an embrittlement phenomenon that imparts substantial damage to the infrastructure by reducing the ductility, fracture strength, strength bearing capacity, etc., of metallic components. Therefore, understanding hydrogen-induced crack initiation mechanisms in metals are of prime importance. Greater insights into this critical phenomenon are expected if the hydrogen-induced crack initiation can be correlated with the local microstructure and corresponding stress-strain state towards their propensity for hydrogen accumulation. With this motivation, in this work, crack initiation is studied for the uncharged and hydrogen charged nickel specimens during in-situ tensile loading under the scanning electron microscope. By assuming the material to be elastic at low strains, a novel approach is implemented for generating the microstructural stress maps through strain and stiffness tensor extracted at each point in the region of interest on the specimen surface using high-resolution digital image correlation (HR-DIC) and Euler angles (given by electron backscattered diffraction data), respectively. Based on this analysis at low strain, the crack initiation sites for uncharged and hydrogen charged nickel specimens are correlated with microstructural maps of maximum Schmid factor, elastic modulus in the loading direction, hydrostatic stress, von Mises stress, and triaxiality factor. The analysis highlighted two independent factors responsible for hydrogen enhanced decohesion (HEDE) based intergranular failure observed only at the random grain boundaries, (i) strain localization due to hydrogen enhanced localized plasticity (HELP) mechanism of hydrogen embrittlement, and (ii) hydrostatic stress-based hydrogen diffusion to the crack initiation sites. These critical insights thus can help to design hydrogen embrittlement resistant metals. In addition, the novel experimental approach can be used to calibrate advance micromechanical models while providing quantitative estimate of the hydrogen distribution in realistic metallic microstructure responsible for hydrogen-assisted crack initiation with deformation.}
}
@article{ALWASEL202017,
title = {IoTSim-SDWAN: A simulation framework for interconnecting distributed datacenters over Software-Defined Wide Area Network (SD-WAN)},
journal = {Journal of Parallel and Distributed Computing},
volume = {143},
pages = {17-35},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S074373151930886X},
author = {Khaled Alwasel and Devki Nandan Jha and Eduardo Hernandez and Deepak Puthal and Mutaz Barika and Blesson Varghese and Saurabh Kumar Garg and Philip James and Albert Zomaya and Graham Morgan and Rajiv Ranjan},
keywords = {Software-Defined Wide Area Network (SD-WAN), Software-Defined Network (SDN), Classical WAN, Internet of Things (IoT)},
abstract = {Software-defined networking (SDN) has evolved as an approach that allows network administrators to program and initialize, control, change and manage networking components (mostly at L2-L3 layers) of the OSI model. SDN is designed to address the programmability shortcomings of traditional networking architectures commonly used in cloud datacenters (CDC). Deployment of SDN solutions have demonstrated significant improvements in areas such as flow optimization and bandwidth allocation in a CDC. However, the benefits are significantly less explored when considering Software-Defined Wide Area Networks (SD-WAN) architectures in the context of delivering solutions by networking multiple CDCs. To support the testing and bench-marking of data-driven applications that rely on data ingestion and processing (e.g., Smart Energy Cloud, Content Delivery Networks) across multiple cloud datacenters, this paper presents the simulator, IoTSim-SDWAN. To the best of our knowledge, IoTSim-SDWAN is the first simulator that facilitates the modeling, simulating, and evaluating of new algorithms, policies, and designs in the context of SD-WAN ecosystems and SDN-enabled multiple cloud datacenters. Finally, IoTSim-SDWAN simulator is evaluated for network performance and energy to illustrate the difference between classical WAN and SD-WAN environments. The obtained results show that SD-WAN surpasses the classical WAN in terms of accelerating traffic flows and reducing power consumption.}
}
@article{FORMICA2018126,
title = {Computational efficiency and accuracy of sequential nonlinear cyclic analysis of carbon nanotube nanocomposites},
journal = {Advances in Engineering Software},
volume = {125},
pages = {126-135},
year = {2018},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2018.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818302928},
author = {Giovanni Formica and Franco Milicchio and Walter Lacarbonara},
keywords = {Three-dimensional finite element modeling, Time integration schemes, Differential evolution optimization, Hysteretic damping, CNT Nanocomposites},
abstract = {The accuracy and efficiency of a numerical strategy for sequential nonlinear cyclic analyses of carbon nanotube nanocomposites are investigated. The computational approach resorts to a nonlinear 3D finite element implementation that seeks to solve the cyclic hysteretic response of the nanocomposite. A variant of the Newton-Raphson method within a time integration scheme is proposed whereby the elastic tangent matrix is chosen as iteration matrix without paying the price of its iterative update. This is especially rewarding in the context of the employed mechanical model which exhibits hysteresis manifested through a discontinuous change in the stiffness at the reversal points where the loading direction is reversed. Key implementation aspects – such as the integration of the nonlinear 3D equations of motion, the numerical accuracy/efficiency as a function of the time step or the mesh size – are discussed. In particular, efficiency is regarded as performing fast computations especially when the number of cyclic analyses becomes large. By making use of laptop CPU cores, a good speed of computations is achieved not only through parallelization but also employing a caching procedure for the iteration matrix.}
}
@incollection{HOLZAPFEL2017101,
title = {Chapter 5 - Modeling of Damage in Soft Biological Tissues},
editor = {Yohan Payan and Jacques Ohayon},
booktitle = {Biomechanics of Living Organs},
publisher = {Academic Press},
address = {Oxford},
pages = {101-123},
year = {2017},
volume = {1},
series = {Translational Epigenetics},
issn = {25425358},
doi = {https://doi.org/10.1016/B978-0-12-804009-6.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128040096000055},
author = {Gerhard A. Holzapfel and Behrooz Fereidoonnezhad},
keywords = {Soft tissue, Constitutive model, Damage, Continuum damage mechanics, Pseudo-elasticity, Hyperelasticity, Computational model},
abstract = {The mechanical responses of biological tissues are well characterized by hyperelastic or viscoelastic models within the physiological loading range. However, during supra-physiological mechanical loading, as occurs during interventional procedures such as balloon angioplasty and arterial clamping, damage may occur in the tissue. The continuum and computational treatments of damage in soft biological tissues have attracted considerable attention over the recent years. In this chapter, we review the state of the art of this challenging area. We summarize and critically discuss various damage models, which are based on continuum damage mechanics, the theory of pseudo-elasticity, and the softening hyperelasticity approach. Finally, we provide a recent account of finite element implementations and highlight open questions, challenges, and methods for further development.}
}
@article{FARAHANI201684,
title = {Extending a radial point interpolation meshless method to non-local constitutive damage models},
journal = {Theoretical and Applied Fracture Mechanics},
volume = {85},
pages = {84-98},
year = {2016},
note = {XV Portuguese Conference on Fracture and Fatigue},
issn = {0167-8442},
doi = {https://doi.org/10.1016/j.tafmec.2016.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167844216301100},
author = {Behzad V. Farahani and J. Belinha and F.M. {Andrade Pires} and António J.M. Ferreira and P.M.G.P. Moreira},
keywords = {Non-local damage model, Radial point interpolation meshless method, Concrete structures},
abstract = {This work extends the radial point interpolation method (RPIM) to the analysis of concrete structures using an elastic continuum damage constitutive model. Here, the theoretical basis of the material model and the computational procedure are described with detail. Firstly, the plane stress meshless formulation is adapted to a rate-independent standard local damage criterion where both compressive and tensile damage evolution are settled respecting a Helmholtz free energy function. Afterwards, the non-local damage formalism is established based on the foregoing local model. Subsequently, the internal variable fields (such as local and non-local damage parameters) are obtained within the return-mapping damage algorithm. This study employs the Newton-Raphson algorithm to achieve the non-linear damage solution. The numerical stability and performance of the proposed standard and non-local models are assessed using the results of experimental tests available in the literature.}
}
@article{GARCIAGRAJALES2017147,
title = {Continuum mechanical modeling of axonal growth},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {314},
pages = {147-163},
year = {2017},
note = {Special Issue on Biological Systems Dedicated to William S. Klug},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2016.07.032},
url = {https://www.sciencedirect.com/science/article/pii/S0045782516308064},
author = {Julián Andrés García-Grajales and Antoine Jérusalem and Alain Goriely},
keywords = {Axonal guidance, Axonal growth, Continuum modeling, Axonal elongation},
abstract = {Axonal growth is a complex phenomenon in which many intra- and extra-cellular signals collaborate simultaneously. Two different compartments can be identified in the growing axon: the growth cone, the leading tip that guides and steers the axon, and the axonal shaft, connecting the soma to the growth cone. The complex relations between both compartments and how their interaction leads the axon to its final synaptic target remain a topic of intense scrutiny. Here, we present a continuum and computational model for the development of the axonal shaft. Two different regions are considered: the axoplasm, filled with microtubules, and the surrounding cortical membrane, consisting mainly of F-actin, Myosin II motor proteins and the membrane. Based on the theory of morphoelasticity, the deformation gradient is decomposed into anelastic and viscoelastic parts. The former corresponds to either a growth tensor for the axoplasm, or a composition of growth and contractile tensors for the cortical membrane. The biophysical evolution for the anelastic parts is obtained at the constitutive level, in which the polymerization and depolymerization of microtubules and F-actin drive the growth, while the contractility is due to the pulling exerted by the Myosin II on the F-actin and depends on the stress. The coupling between cytoskeletal dynamics and mechanics is naturally derived from the equilibrium equations. The framework is exploited in two representative scenarios in which an external force is applied to the axonal shaft either along the axis or off the axis. In the first case three states are found: growth, collapse and stall. In the second case, axonal turning is observed. This framework is suitable to investigate the complex relationship between the local mechanical state, the cytoskeletal polymerization/depolymerization rates, and the contractility of the cortical membrane in axonal guidance.}
}
@article{SAEIK2021108177,
title = {Task offloading in Edge and Cloud Computing: A survey on mathematical, artificial intelligence and control theory solutions},
journal = {Computer Networks},
volume = {195},
pages = {108177},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108177},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621002322},
author = {Firdose Saeik and Marios Avgeris and Dimitrios Spatharakis and Nina Santi and Dimitrios Dechouniotis and John Violos and Aris Leivadeas and Nikolaos Athanasopoulos and Nathalie Mitton and Symeon Papavassiliou},
keywords = {Edge Computing, Task offloading, Resource allocation, Control theory, Mathematical optimization, Artificial intelligence},
abstract = {Next generation communication networks are expected to accommodate a high number of new and resource-voracious applications that can be offered to a large range of end users. Even though end devices are becoming more powerful, the available local resources cannot cope with the requirements of these applications. This has created a new challenge called task offloading, where computation intensive tasks need to be offloaded to more resource powerful remote devices. Naturally, the Cloud Computing is a well-tested infrastructure that can facilitate the task offloading. However, Cloud Computing as a centralized and distant infrastructure creates significant communication delays that cannot satisfy the requirements of the emerging delay-sensitive applications. To this end, the concept of Edge Computing has been proposed, where the Cloud Computing capabilities are repositioned closer to the end devices at the edge of the network. This paper provides a detailed survey of how the Edge and/or Cloud can be combined together to facilitate the task offloading problem. Particular emphasis is given on the mathematical, artificial intelligence and control theory optimization approaches that can be used to satisfy the various objectives, constraints and dynamic conditions of this end-to-end application execution approach. The survey concludes with identifying open challenges and future directions of the problem at hand.}
}
@article{ZHU2019190,
title = {An implicit unified gas-kinetic scheme for unsteady flow in all Knudsen regimes},
journal = {Journal of Computational Physics},
volume = {386},
pages = {190-217},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119300841},
author = {Yajun Zhu and Chengwen Zhong and Kun Xu},
keywords = {Unified gas-kinetic scheme, Implicit method, Unsteady flow, Multiscale transport},
abstract = {The unified gas-kinetic scheme (UGKS) is a direct modeling method for multiple scale transport. Based on the ratio of time step to the particle collision time, the local evolution solution on the mesh size and time step scales is used in the construction of the multiscale method. For a flow problem covering multiple flow regimes, such as the hypersonic flow around a flying vehicle in near space, the UGKS is able to capture the highly compressed Navier-Stokes solution in one region and fully expanded free molecular flow in another region, with significant variations of the ratio between the time step and the local particle collision time around the vehicle. For an explicit UGKS, the time step in the whole computational domain is determined by the CFL condition. With implicit and multigrid techniques, the efficiency of the UGKS [1], [2] has been improved by two orders of magnitude for steady state computation. However, for unsteady flow computation, due to the CFL condition the global time step used in the explicit UGKS may be limited by the smallest cell size in the computational domain. As a result, for a largely stretched non-uniform mesh the global time step becomes very small and the ratio of the time step to the local particle collision time may get a very small value. Under such a circumstance, even though the UGKS is a multiscale method, the real physics represented in the explicit UGKS may be constrained to the kinetic scale transport only, and the advantage of the multiscale nature in UGKS has not been fully utilized. In order to solve the multiscale unsteady flow problem efficiently, the time step restriction from a global CFL condition has to be released. In this paper, we will develop an implicit UGKS (IUGKS) for unsteady flows by alternatively solving the macroscopic and microscopic governing equations within a time step iteratively. With a pre-defined uniform large evolution time step, the local CFL number varies greatly in different region, such as on the order 1 in the large numerical cell size region, and 100 in the small cell size region. In order to preserve coherent flow evolution and keep the multiscale nature, the time averaged numerical flux across a cell interface is still evaluated by the explicit UGKS under the local CFL condition. Therefore, the multiscale property of the UGKS modeling has been kept over non-uniform meshes. With improved temporal discretization, the current IUGKS can automatically go back to the explicit UGKS and obtain identical solutions when the time step of the implicit scheme gets to that of an explicit one. Many numerical examples are included to validate the scheme for both continuum and rarefied flows with a large variation of artificially generated mesh size. The IUGKS has a second order accuracy and presents reasonably good results for unsteady flow computation, and its efficiency has been improved by dozens of times in comparison with the explicit UGKS.}
}
@incollection{ZIEMYS2018209,
title = {Chapter 7 - Multiscale models for transport and biodistribution of therapeutics in cancer},
editor = {Davide Manca},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {42},
pages = {209-237},
year = {2018},
booktitle = {Quantitative Systems Pharmacology},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63964-6.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444639646000076},
author = {Arturas Ziemys and Milos Kojic and Miljan Milosevic and Bernhard Schrefler and Mauro Ferrari},
keywords = {Diffusion, Convection, Finite element method, Molecular dynamics, Pharmacokinetics, Doxorubicin, Tumor microenvironment},
abstract = {The chapter discusses the computational framework dedicated to simulate time-dependent biodistribution with focus on cancer. While systemic pharmacokinetic and biodistribution problems are extensively studied, the factors considered here are drug transport from drug vectors, drug transport and biodistribution in the tumor microenvironment, whole organ, and including the biodistribution effect on tumor growth. Continuum-based computational strategies are presented. Selected example solutions demonstrate the applicability of this methodology to in vitro and in vivo conditions.}
}
@article{ZHAO201730,
title = {Simulation-aided constitutive law development – Assessment of low triaxiality void nucleation models via extended finite element method},
journal = {Journal of the Mechanics and Physics of Solids},
volume = {102},
pages = {30-45},
year = {2017},
issn = {0022-5096},
doi = {https://doi.org/10.1016/j.jmps.2017.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0022509616306688},
author = {Jifeng Zhao and Oleg Y. Kontsevoi and Wei Xiong and Jacob Smith},
keywords = {Extended finite element method (XFEM), Gurson-Tvergaard-Needleman (GTN) model, Void nucleation, First principles calculations, Density functional theory (DFT), Interface decohesion, Shear localization, Statistical volume element (SVE)},
abstract = {In this work, a multi-scale computational framework has been established in order to investigate, refine and validate constitutive behaviors in the context of the Gurson-Tvergaard-Needleman (GTN) void mechanics model. The eXtended Finite Element Method (XFEM) has been implemented in order to (1) develop statistical volume elements (SVE) of a matrix material with subscale inclusions and (2) to simulate the multi-void nucleation process due to interface debonding between the matrix and particle phases. Our analyses strongly suggest that under low stress triaxiality the nucleation rate of the voids f˙ can be well described by a normal distribution function with respect to the matrix equivalent stress (σe), as opposed to that proposed (σ¯+1/3σkk) in the original form of the single void GTN model. The modified form of the multi-void nucleation model has been validated based on a series of numerical experiments with different loading conditions, material properties, particle shape/size and spatial distributions. The utilization of XFEM allows for an invariant finite element mesh to represent varying microstructures, which implies suitability for drastically reducing complexity in generating the finite element discretizations for large stochastic arrays of microstructure configurations. The modified form of the multi-void nucleation model is further applied to study high strength steels by incorporating first principles calculations. The necessity of using a phenomenological interface separation law has been fully eliminated and replaced by the physics-based cohesive relationship obtained from Density Functional Theory (DFT) calculations in order to provide an accurate macroscopic material response.}
}
@article{KEMP2015553,
title = {ISO 27018 and personal information in the cloud: First year scorecard},
journal = {Computer Law & Security Review},
volume = {31},
number = {4},
pages = {553-555},
year = {2015},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2015.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S026736491500093X},
author = {Richard Kemp},
keywords = {Data protection, Privacy, Data security, Big data, Cloud services, Software as a service, Standards, ISO 27018},
abstract = {As ISO 27018 – the first international standard on the protection of personal data in the public cloud – approaches its first birthday, it is emerging as a source of reassurance for Cloud Service Customers contracting with ISO 27018 certified Cloud Service Providers anywhere across the spectrum of Cloud services where personal data could be involved.}
}
@article{GAO2021598,
title = {Video transcoding for adaptive bitrate streaming over edge-cloud continuum},
journal = {Digital Communications and Networks},
volume = {7},
number = {4},
pages = {598-604},
year = {2021},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2020.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352864820302935},
author = {Guanyu Gao and Yonggang Wen},
keywords = {Video transcoding, Adaptive bitrate streaming, Quality of service, Resource allocation, Edge-cloud},
abstract = {Video transcoding is to create multiple representations of a video for content adaptation. It is deemed as a core technique in Adaptive BitRate (ABR) streaming. How to manage video transcoding affects the performance of ABR streaming in various aspects, including operational cost, streaming delays, Quality of Experience (QoE), etc. Therefore, the problems of implementing video transcoding in ABR streaming must be systematically studied to improve the overall performance of the streaming services. These problems become more worthy of investigation with the emergence of the edge-cloud continuum, which makes the resource allocation for video transcoding more complicated. To this end, this paper provides an investigation of the main technical problems related to video transcoding in ABR streaming, including designing a rate profile for video transcoding, providing resources for video transcoding in clouds, and caching multi-bitrate video contents in networks, etc. We analyze these problems from the perspective of resource allocation in the edge-cloud continuum and cast them into resource and Quality of Service (QoS) optimization problems. The goal is to minimize resource consumption while guaranteeing the QoS for ABR streaming. We also discuss some promising research directions for the ABR streaming services.}
}
@article{FENG2021100006,
title = {Smart grid encounters edge computing: opportunities and applications},
journal = {Advances in Applied Energy},
volume = {1},
pages = {100006},
year = {2021},
issn = {2666-7924},
doi = {https://doi.org/10.1016/j.adapen.2020.100006},
url = {https://www.sciencedirect.com/science/article/pii/S2666792420300068},
author = {Cheng Feng and Yi Wang and Qixin Chen and Yi Ding and Goran Strbac and Chongqing Kang},
keywords = {Smart grid, Edge computing, ICT solutions, Data analytics, Sustainable computing},
abstract = {Smart grids (SGs) are reforming towards utilizing massive data for operations and services. During this reform, the information and communication technologies (ICTs) play a critical role, especially for the computing model, which determines how data analytics in SG can be executed. Edge computing (EC), a novel computing paradigm innovation, has high potential to help with the digitization of SG. This paper seeks to provide a comprehensive review of interdisciplinary research on EC applications in SG. First, an in-depth analysis of EC, including the definition, architectures, characteristics, and key enabling technologies is conducted from the perspective of SG application requirements. Afterward, this paper systematically explores application scenarios of EC in SG based on its characteristics. Beyond that, the synergistic effect of the integration of EC and SG is also assessed: SG not only benefits from EC but also supports EC to become sustainable. Challenges and open questions are then investigated.}
}
@article{KOCHOVSKI2019747,
title = {Trust management in a blockchain based fog computing platform with trustless smart oracles},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {747-759},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301281},
author = {Petar Kochovski and Sandi Gec and Vlado Stankovski and Marko Bajec and Pavel D. Drobintsev},
keywords = {Trust, Fog, Blockchain, Smart contract, Smart oracle},
abstract = {Trust is a crucial aspect when cyber-physical systems have to rely on resources and services under ownership of various entities, such as in the case of Edge, Fog and Cloud computing. The DECENTER’s Fog Computing Platform is developed to support Big Data pipelines, which start from the Internet of Things (IoT), such as cameras that provide video-streams for subsequent analysis. It is used to implement Artificial Intelligence (AI) algorithms across the Edge-Fog-Cloud computing continuum which provide benefits to applications, including high Quality of Service (QoS), improved privacy and security, lower operational costs and similar. In this article, we present a trust management architecture for DECENTER that relies on the use of blockchain-based Smart Contracts (SCs) and specifically designed trustless Smart Oracles. The architecture is implemented on Ethereum ledger (testnet) and three trust management scenarios are used for illustration. The scenarios (trust management for cameras, trusted data flow and QoS based computing node selection) are used to present the benefits of establishing trust relationships among entities, services and stakeholders of the platform.}
}
@article{KHAN2020102537,
title = {A survey of subscription privacy on the 5G radio interface - The past, present and future},
journal = {Journal of Information Security and Applications},
volume = {53},
pages = {102537},
year = {2020},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2020.102537},
url = {https://www.sciencedirect.com/science/article/pii/S2214212620300235},
author = {Haibat Khan and Keith M. Martin},
keywords = {5G, Anonymity, GSM, LTE, Mobile networks, Privacy, UMTS, Unlinkability},
abstract = {End-user privacy in mobile telephony systems is nowadays of great interest because of the envisaged hyper-connectivity and the potential of the unprecedented services (virtual reality, machine-type communication, vehicle-to-everything, IoT, etc.) being offered by the new 5G system. This paper reviews the state of subscription privacy in 5G systems. As the work on 5G Release 15 – the first full set of 5G standards – has recently been completed, this seems to be an appropriate occasion for such a review. The scope of the privacy study undertaken is limited to the wireless part of the 5G system which occurs between the service provider’s base station and the subscriber’s mobile phone. Although 5G offers better privacy guarantees than its predecessors, this work highlights that there still remain significant issues which need rectifying. We undertook an endeavor to (i) compile the privacy vulnerabilities that already existed in the previous mobile telephony generations. Thereafter, (ii) the privacy improvements offered by the recently finalized 5G standard were aggregated. Consequently, (iii) we were able to highlight privacy issues from previous generations that remain unresolved in 5G Release 15. For completeness, (iv) we also explore new privacy attacks which surfaced after the publication of the 5G standard. To address the identified privacy gaps, we also present future research directions in the form of proposed improvements.}
}
@article{SIENIEK20141027,
title = {Fast Graph Transformation based Direct Solver Algorithm for Regular Three Dimensional Grids},
journal = {Procedia Computer Science},
volume = {29},
pages = {1027-1038},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.092},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914002695},
author = {Marcin Sieniek},
keywords = {step-and-flash imprint lithography, multi-scale, graph transformations, multi-frontal solver},
abstract = {This paper presents a graph-transformation-based multifrontal direct solver with an optimization technique that allows for a significant decrease of time complexity in some multi-scale simulations of the Step and Flash Imprint Lithography (SFIL). The multi-scale simulation consists of a macro-scale linear e lasticity model with thermal expansion coefficient and a nano-scale molecular statics model. The algorithm is exemplified with a photopolimerization simulation that involves densification of a polymer inside a feature followed by shrinkage of the feature after removal of the template. The solver is optimized thanks to a mechanism of reusing sub -domains with similar geometries and similar material properties. The graph transformation formalism is used to describe the algorithm - such an approach helps automatically localize sub-domains that can be reused.}
}
@article{ALSHAMRANI20224687,
title = {IoT and artificial intelligence implementations for remote healthcare monitoring systems: A survey},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part A},
pages = {4687-4701},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001385},
author = {Mazin Alshamrani},
keywords = {RHM, H-IoT, Healthcare, Machine learning, Monitoring systems},
abstract = {The Internet of Things (IoT) and artificial intelligence (AI) are two of the fastest-growing technologies in the world. With more people moving to cities, the concept of a smart city is not foreign. The idea of a smart city is based on transforming the healthcare sector by increasing its efficiency, lowering costs, and putting the focus back on a better patient care system. Implementing IoT and AI for remote healthcare monitoring (RHM) systems requires a deep understanding of different frameworks in smart cities. These frameworks occur in the form of underlying technologies, devices, systems, models, designs, use cases, and applications. The IoT-based RHM system mainly employs both AI and machine learning (ML) by gathering different records and datasets. On the other hand, ML methods are broadly used to create analytic representations and are incorporated into clinical decision support systems and diverse healthcare service forms. After carefully examining each factor in clinical decision support systems, a unique treatment, lifestyle advice, and care strategy are proposed to patients. The technology used helps to support healthcare applications and analyze activities, body temperature, heart rate, blood glucose, etcetera. Keeping this in mind, this paper provides a survey that focuses on the identification of the most relevant health Internet of things (H-IoT) applications supported by smart city infrastructure. This study also evaluates related technologies and systems for RHM services by understanding the most pertinent monitoring applications based on several models with different corresponding IoT-based sensors. Finally, this research contributes to scientific knowledge by highlighting the main limitations of the topic and recommending possible opportunities in this research area.}
}
@article{ZKIK2019977,
title = {Secure Multipath Mutation SMPM in Moving Target Defense Based on SDN},
journal = {Procedia Computer Science},
volume = {151},
pages = {977-984},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.137},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919306015},
author = {Karim ZKIK and Anass Sebbar and Youssef Baddi and Mohammed Boulmalf},
keywords = {Moving target defense, Random Route Mutation, Software-defined networking, Pathfinder, Multipath Mutation, Security, Privacy},
abstract = {Software-defined networking (SDN) refers to a network architecture where the transfer state in the data plane is managed by a remote control plane in a centralized manner. SDN offer many advantage in terms of flexibility and automation to administrator but it suffer from many security issues. In other hand, Random Route Mutation (RRM) and path diversity represent one of the important research focuses about moving target defense (MTD). The main idea of using this technic, is to change periodically (or basing on events) used routes between sender and receiver in order to enhance mutation efficiency and decrease attackers capabilities to launch effective eavesdropping, denial of service or man in the middle attack. Using RRM and multi path technics can be very interesting in order to secure SDN and to detect and prevent intrusions. In this paper it is propose a new framework called SMPM which aims to secure and prevent intrusion by modeling SDN architectures and using a pathfinder algorithm called RRM-Pathfinder. The proposed framework calculates all possible paths from given source to destination and then, based on some criteria such as capacity, Overlap, Security and QoS, it selects and identifies the most cost-effective routes. The use of SMPM allow also to dynamically route packets using all pre-calculated paths which will permit to avoid sniffing and poisoning attacks such as Arp spoof and the man in the middle attacks and to ensure more confidentiality, integrity and privacy.}
}
@article{SONBOL202028,
title = {EdgeKV: Decentralized, scalable, and consistent storage for the edge},
journal = {Journal of Parallel and Distributed Computing},
volume = {144},
pages = {28-40},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520302884},
author = {Karim Sonbol and Öznur Özkasap and Ibrahim Al-Oqily and Moayad Aloqaily},
keywords = {Edge computing, Distributed systems, Key–value store, DHT, Consistency},
abstract = {Edge computing moves the computation closer to the data and the data closer to the user to overcome the high latency communication of cloud computing. Storage at the edge allows data access with high speeds that enable latency-sensitive applications in areas such as autonomous driving and smart grid. However, several distributed services are typically designed for the cloud and building an efficient edge-enabled storage system is challenging because of the distributed and heterogeneous nature of the edge and its limited resources. In this paper, we propose EdgeKV, a decentralized storage system designed for the network edge. EdgeKV offers fast and reliable storage, utilizing data replication with strong consistency guarantees. With a location-transparent and interface-based design, EdgeKV can scale with a heterogeneous system of edge nodes. We implement a prototype of the EdgeKV modules in Golang and evaluate it in both the edge and cloud settings on the Grid’5000 testbed. We utilize the Yahoo! Cloud Serving Benchmark (YCSB) to analyze the system’s performance under realistic workloads. Our evaluation results show that EdgeKV outperforms the cloud storage setting with both local and global data access with an average write response time and throughput improvements of 26% and 19% respectively under the same settings. Our evaluations also show that EdgeKV can scale with the number of clients, without sacrificing performance. Finally, we discuss the energy efficiency improvement when utilizing edge resources with EdgeKV instead of a centralized cloud.}
}
@article{PERRY2021111470,
title = {Computing continuum-level explosive shock and detonation response over a wide pressure range from microstructural details},
journal = {Combustion and Flame},
volume = {231},
pages = {111470},
year = {2021},
issn = {0010-2180},
doi = {https://doi.org/10.1016/j.combustflame.2021.111470},
url = {https://www.sciencedirect.com/science/article/pii/S0010218021002133},
author = {W. Lee Perry and Amanda L. Duque and Joseph T. Mang and David B. Culp},
keywords = {Microstructure, Mesoscale, Surface area, Reactive flow, SURF, Ignition and growth},
abstract = {This paper builds upon our previous work where we defined distinct physically-based parameters of the Scaled Uniform Reactive Flow (SURF) model of Shaw and Menikoff for heterogeneous high explosives (HE) reactive flow calculations. We call this Physically-Informed version of the model PiSURF, or πSURF. Here, we re-derived the model in a manner that resulted in a dependency on the fraction of the overall specific surface area (associated with the void space in a consolidated sample) that is ‘activated’ by a shock wave. This perspective is appealing and consistent with the mechanism of combustion in this class of materials at lower pressures. We apply the model to three case studies having slight perturbations in the nominal microstructure of the explosive PBX 9502, characterized by a void volume distribution in the 0.1 nm–10 µm void size range, over a wide pressure range (0.01–0.6 Mbar/1–60 GPa). We concluded that the model provided meaningful predictions that agreed with our intuitive expectations, qualitatively supported by experimental evidence, for the entire relevant pressure range in a manner consistent with a more generalized picture of combustion in heterogeneous materials. This approach takes a significant step forward towards the broader goal of a priori calculation of the hydrodynamic behavior of HE from measurable microstructural details.}
}
@article{CHEN2014100,
title = {Multiscale Fluid Mechanics and Modeling},
journal = {Procedia IUTAM},
volume = {10},
pages = {100-114},
year = {2014},
note = {Mechanics for the World: Proceedings of the 23rd International Congress of Theoretical and Applied Mechanics, ICTAM2012},
issn = {2210-9838},
doi = {https://doi.org/10.1016/j.piutam.2014.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S2210983814000133},
author = {Shiyi Chen and Moran Wang and Zhenhua Xia},
keywords = {fluid mechanics, multiscale modeling, microfluidics and nanofluidics, physical constraints, constrained large eddy simulation},
abstract = {In recent years, there has been a tremendous growth of activity on multiscale modeling and computation. In particular, the multiscale hybrid numerical methods are those that combine multiple models defined at fundamentally different length and time scales within the same overall spatial and temporal domain. For examples, a framework of hybrid continuum and molecular dynamics multiscale method has been developed to simulate micro- and nanoscale fluid flows, which combines the continuum computational fluid dynamics (CFD) or the mesoscopic lattice Boltzmann method for the bulk flow region and the atomistic molecular dynamics for the interface region. The similar idea of constrained variation has also been used in developing multiscale fluid turbulent models for constrained dynamic subgrid-scale stress model, Reynolds stress constrained large eddy simulation (RSC-LES) for wall-bounded turbulent flows with massive separation and heat flux constrained large eddy simulation. For RSC-LES, our model is able to solve the traditional log-layer mismatch problem in RANS/LES approaches and can predict mean velocity, turbulent stress and skin friction coefficients more accurate than pure dynamic large eddy models and traditional detached eddy simulation using the same grid resolution. Our results demonstrate the capability of multiscale simulation methods for complex fluid systems and the necessity of physical constraints on the multiscale methods.}
}
@article{TANEJA2020105286,
title = {Machine learning based fog computing assisted data-driven approach for early lameness detection in dairy cattle},
journal = {Computers and Electronics in Agriculture},
volume = {171},
pages = {105286},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105286},
url = {https://www.sciencedirect.com/science/article/pii/S016816991931840X},
author = {Mohit Taneja and John Byabazaire and Nikita Jalodia and Alan Davy and Cristian Olariu and Paul Malone},
keywords = {Smart dairy farming, Fog computing, Internet of Things (IoT), Cloud computing, Smart farm, Data analytics, Microservices, Machine learning, Clustering, Classification, Data-driven},
abstract = {Timely lameness detection is one of the major and costliest health problems in dairy cattle that farmers and practitioners haven't yet solved adequately. The primary reason behind this is the high initial setup costs, complex equipment and lack of multi-vendor interoperability in currently available solutions. On the other hand, human observation based solutions relying on visual inspections are prone to late detection with possible human error, and are not scalable. This poses a concern with increasing herd sizes, as prolonged or undetected lameness severely compromises cows' health and welfare, and ultimately affects the milk productivity of the farm. To tackle this, we have developed an end-to-end IoT application that leverages advanced machine learning and data analytics techniques to monitor the cattle in real-time and identify lame cattle at an early stage. The proposed approach has been validated on a real world smart dairy farm setup consisting of a dairy herd of 150 cows in Waterford, Ireland. Using long-range pedometers specifically designed for use in dairy cattle, we monitor the activity of each cow in the herd. The accelerometric data from these sensors is aggregated at the fog node to form a time series of behavioral activities, which are further analyzed in the cloud. Our hybrid clustering and classification model identifies each cow as either Active, Normal or Dormant, and further, Lame or Non-Lame. The detected lameness anomalies are further sent to farmer's mobile device by way of push notifications. The results indicate that we can detect lameness 3 days before it can be visually captured by the farmer with an overall accuracy of 87%. This means that the animal can either be isolated or treated immediately to avoid any further effects of lameness. Moreover, with fog based computational assistance in the setup, we see an 84% reduction in amount of data transferred to the cloud as compared to the conventional cloud based approach.}
}
@article{CHAKRABORTY201654,
title = {Numerical simulation of axisymmetric drop formation using a coupled level set and volume of fluid method},
journal = {International Journal of Multiphase Flow},
volume = {84},
pages = {54-65},
year = {2016},
issn = {0301-9322},
doi = {https://doi.org/10.1016/j.ijmultiphaseflow.2016.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0301932215300392},
author = {I. Chakraborty and M. Rubio-Rubio and A. Sevilla and J.M. Gordillo},
keywords = {Numerical simulation, CLSVOF, Drop formation, Dripping, P2S response, Jetting, Transition},
abstract = {Numerical simulations have been carried out to examine the axisymmetric formation of drops of Newtonian liquid injected from a vertical orifice under constant flow conditions into the ambient air. The numerical simulation was performed by solving axisymmetric Navier–Stokes equations with a coupled level-set and volume-of-fluid (CLSVOF) method. In this work, the dynamics of the formation of drops are investigated over a range of the Ohnesorge number Oh=0.01,0.023 and 0.13, and the Bond number Bo=0.33,0.5 and 2.205, as the Weber number We increases. The different responses of drop formation such as period-1 dripping with (P1S) or without satellite drops (P1), complex dripping (CD) and jetting (J) are discussed. The different responses of drop formation were identified quantitatively from the time history of growing length of drop at the orifice. The transition of different responses is shown on the map which exhibits the variation of limiting length of drop at breakup or the volume of the detached primary drop with We while keeping Oh and Bo fixed. The numerical investigation of liquid jet formation in terms of the evolution of growing length of jet under different computational grid sizes was discussed. It is proposed that the almost stable liquid jet formation can be found as the mesh size decreases. The accuracy of the present computed results is assessed by comparisons with the previous investigations. Furthermore, it is shown that at high Bo=2.205, low Oh=0.023 and We=0.0177, the system exhibits period-2 with satellite drop (P2S) response which was not reported before in literature.}
}
@article{IRANI201767,
title = {A discrete dislocation analysis of hydrogen-assisted mode-I fracture},
journal = {Mechanics of Materials},
volume = {105},
pages = {67-79},
year = {2017},
issn = {0167-6636},
doi = {https://doi.org/10.1016/j.mechmat.2016.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167663616305312},
author = {N. Irani and J.J.C. Remmers and V.S. Deshpande},
keywords = {Hydrogen embrittlement, Stressed-assisted diffusion, Discrete dislocation plasticity},
abstract = {Fracture of engineering alloys in the presence of hydrogen commonly occurs by decohesion along grain boundaries via a mechanism known as hydrogen induced decohesion (HID). This mechanism is investigated here by analysing the mode-I fracture of a single crystal with plastic flow in the crystal described by discrete dislocation plasticity (DDP) and material separation (decohesion) modelled using a cohesive zone formulation. The motion of dislocations is assumed to be unaffected by hydrogen diffusion. While the cohesive strength is assumed to be reduced proportional to the local hydrogen concentration. Two limiting cases are analysed: (i) the fast diffusion limit where the hydrogen within the material is assumed to be at chemical equilibrium throughout the loading so that there is a high hydrogen concentration in the regions of high hydrostatic stress around dislocations and near the crack tip and (ii) the slow diffusion limit where we assume that there is no appreciable hydrogen diffusion over the duration of loading and thus the hydrogen concentration remains spatially uniform as in a stress-free material. The lower cohesive strength at high hydrogen concentrations results in reduced dislocation activity around the crack tip and a reduction in the material toughness. In fact, at the highest hydrogen concentrations analysed here, crack growth primarily occurs in an elastic manner. However, surprisingly the calculations predicted that the toughness in the fast diffusion case was no more than 12% lower compared to the slow diffusion case suggesting that the stress concentrations due to the dislocation structures and the crack tip fields have only a minor effect on the toughness reduction in the presence of hydrogen. The DDP calculations are finally used to investigate the sensitivity of the material toughness to the grain boundary cohesive strength. The calculations show that the toughness of materials with a small cohesive opening at the peak cohesive traction are more sensitive to hydrogen loading. We speculate that this result might be used as a guide in grain boundary engineering to design alloys that are less sensitive to hydrogen embrittlement by the HID mechanism.}
}
@article{CHEN2015661,
title = {Hybrid Discrete-continuum Model for Granular Flow},
journal = {Procedia Engineering},
volume = {102},
pages = {661-667},
year = {2015},
note = {New Paradigm of Particle Science and Technology Proceedings of The 7th World Congress on Particle Technology},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.01.160},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815001617},
author = {Xizhong Chen and Junwu Wang},
keywords = {granular flow, hybrid method, discrete model, continuum model, Poiseuille flow},
abstract = {We present a hybrid discrete-continuum model for multi-scale simulation of granular flow. In this method, the domain is decomposed into a discrete sub-domain, where individual particles are tracked using discrete element method, and a continuum sub-domain is solved using the Navier-Stokes equation combined with kinetic theory of granular flow. The spatial coupling between continuum method and discrete method is achieved through an overlap region, in which both methods’ variables are shared with each other. The feasibility of the hybrid discrete-continuum model is demonstrated through the simulation of a velocity-driven granular Poiseuille flow with mono-disperse, smooth (frictionless) particles.}
}
@article{MENEZES20192494,
title = {Identification and Design of Industry 4.0 Opportunities in Manufacturing: Examples from Mature Industries to Laboratory Level Systems},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {2494-2500},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.581},
url = {https://www.sciencedirect.com/science/article/pii/S240589631931568X},
author = {Brenno C. Menezes and Jeffrey D. Kelly and Adriano G. Leal},
keywords = {Industry 4.0, Smart manufacturing systems, High-end sensing, Advanced analytics},
abstract = {Considering the nascent application of advanced technologies in industry within the context of the new industrial revolution, we propose a methodology for identification and design (ID) of smart operations in manufacturing. The ID study considers the elements of the Industry 4.0 (I4) such as autonomous robots, advanced analytics, systems integration, high-end sensing, big data, internet of things, cloud computing and human-machine interactions. Toward achievable I4-based production, the proposed IDoI4 (ID-of-I4) approach integrates I4 fundamental elements regarding a) modelling and solution algorithms (MSA), b) information and communication technologies (ICT), c) high-performance computing (HPC), and d) mechatronics (MEC). These four groups of the I4 elements are examined in the industrial cases covering from mature industries to laboratory level systems to determine their current technological states and gaps into the I4 stage. The examples highlighted are crushed-ore stockpile level control in the mining field, resin bed cleaning timetabling in water demineralisation treatment, compositional data-driven real-time optimisation of hydrocarbon streams and diverse I4 basics in the next generation of biorefineries. In the main example, supported by a high-end sensing apparatus to measure crushed-ore stockpile levels in real-time (live inventory as a controlled variable by a target), a hybrid dynamic control prescribes (every 4 minutes) discrete positions and time-slots of the shuttle-conveyor tripper car mechatronics that creates the stockpiles. From such IDoI4 methodology, a table on the MSA, ICT, HPC and MEC ground bases summarises how such technologies are integrated to the industrial examples considering research, development and deployment in stable, demanded and highly demanded stages of the technologies into the I4 mandate.}
}
@article{FORTI2021605,
title = {Lightweight self-organising distributed monitoring of Fog infrastructures},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {605-618},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19334582},
author = {Stefano Forti and Marco Gaglianese and Antonio Brogi},
keywords = {Fog computing, Lightweight monitoring, Internet of Things, Peer-to-peer architectures},
abstract = {Monitoring will play an enabling role in the orchestration of next-gen Fog applications. Particularly, monitoring of Fog computing infrastructures should deal with platform heterogeneity, scarce resource availability at the edge, and high dynamicity all along the Cloud-IoT continuum. In this article, we describe FogMon, a C++ distributed monitoring prototype targeting Fog computing infrastructures. FogMon monitors hardware resources at different Fog nodes, end-to-end network QoS between such nodes, and connected IoT devices. Besides, it features a self-organising peer-to-peer topology with self-restructuring mechanisms, and differential monitoring updates, which ensure scalability, fault-tolerance and low communication overhead. Experiments on a real testbed show how the footprint of FogMon is limited and how its self-restructuring topology makes it resilient to infrastructure dynamicity.}
}
@article{STEFFENEL201872,
title = {Improving Data Locality in P2P-based Fog Computing Platforms},
journal = {Procedia Computer Science},
volume = {141},
pages = {72-79},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.151},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318015},
author = {Luiz Angelo Steffenel and Manuele Kirsch Pinheiro},
keywords = {fog computing, P2P overlay, data locality, big data, distributed hash table},
abstract = {Fog computing extends the Cloud Computing paradigm to the edge of the network, relying on computing services intelligently distributed to best meet the applications needs such as low communication latency, data caching or confidentiality reinforcement. While P2P is especially prone to implement Fog computing platforms, it usually lacks important elements such as controlling where the data is stored and who will handle the computing tasks. In this paper we propose both a mapping approach for data-locality and a location-aware scheduling for P2P-based middlewares, improving the data management performance on fog environments. Experimental results comparing the data access performances demonstrate the interest of such techniques.}
}
@article{BHATTACHARYYA2020111759,
title = {Multiscale progressive damage analysis of CFRP composites using a mechanics based constitutive relation},
journal = {Composite Structures},
volume = {235},
pages = {111759},
year = {2020},
issn = {0263-8223},
doi = {https://doi.org/10.1016/j.compstruct.2019.111759},
url = {https://www.sciencedirect.com/science/article/pii/S0263822319325310},
author = {Rudraprasad Bhattacharyya and Prodyot K. Basu},
keywords = {Progressive damage analysis, Composite laminate, Multiscale modeling, Damage mechanics},
abstract = {A continuum damage mechanics-based material model for the constituent phases of a carbon fiber reinforced polymer composite material is presented. This constitutive model is applied to Eigenstrain based Reduced-order Homogenization framework for multiscale modeling. A key contribution is that all model parameters are directly or indirectly calibrated using experimental data and these model parameters can be readily updated by describing the evolution of damage as a function of the strain state. The constitutive model for polymer matrix captures mixed mode from pure uniaxial to pure shear loading without using any conventional failure criterion. The accuracy of the constitutive model is demonstrated by comparing experimental results of unnotched and open-hole laminates. In addition, a demonstration is presented to alleviate spurious residual stiffness appearing from the reduction of model order of the microstructure.}
}
@article{CAVIGLIONE2021108010,
title = {Kernel-level tracing for detecting stegomalware and covert channels in Linux environments},
journal = {Computer Networks},
volume = {191},
pages = {108010},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108010},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621001249},
author = {Luca Caviglione and Wojciech Mazurczyk and Matteo Repetto and Andreas Schaffhauser and Marco Zuppelli},
keywords = {eBPF, Syscall and network tracing, Stegomalware, Covert channels, Detection},
abstract = {Modern malware is becoming hard to spot since attackers are increasingly adopting new techniques to elude signature- and rule-based detection mechanisms. Among the others, steganography and information hiding can be used to bypass security frameworks searching for suspicious communications between processes or exfiltration attempts through covert channels. Since the array of potential carriers is very large (e.g., information can be hidden in hardware resources, various multimedia files or network flows), detecting this class of threats is a scarcely generalizable process and gathering multiple behavioral information is time-consuming, lacks scalability, and could lead to performance degradation. In this paper, we leverage the extended Berkeley Packet Filter (eBPF), which is a recent code augmentation feature provided by the Linux kernel, for programmatically tracing and monitoring the behavior of software processes in a very efficient way. To prove the flexibility of the approach, we investigate two realistic use cases implementing different attack mechanisms, i.e., two processes colluding via the alteration of the file system and hidden network communication attempts nested within IPv6 traffic flows. Our results show that even simple eBPF programs can provide useful data for the detection of anomalies, with a minimal overhead. Furthermore, the flexibility to develop and run such programs allows to extract relevant features that could be used for the creation of datasets for feeding security frameworks exploiting AI.}
}
@article{YOUSEFPOUR2019289,
title = {All one needs to know about fog computing and related edge computing paradigms: A complete survey},
journal = {Journal of Systems Architecture},
volume = {98},
pages = {289-330},
year = {2019},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1383762118306349},
author = {Ashkan Yousefpour and Caleb Fung and Tam Nguyen and Krishna Kadiyala and Fatemeh Jalali and Amirreza Niakanlahiji and Jian Kong and Jason P. Jue},
keywords = {Fog computing, Edge computing, Cloud computing, Internet of things (IoT), Cloudlet, Mobile edge computing, Multi-access edge computing, Mist computing},
abstract = {With the Internet of Things (IoT) becoming part of our daily life and our environment, we expect rapid growth in the number of connected devices. IoT is expected to connect billions of devices and humans to bring promising advantages for us. With this growth, fog computing, along with its related edge computing paradigms, such as multi-access edge computing (MEC) and cloudlet, are seen as promising solutions for handling the large volume of security-critical and time-sensitive data that is being produced by the IoT. In this paper, we first provide a tutorial on fog computing and its related computing paradigms, including their similarities and differences. Next, we provide a taxonomy of research topics in fog computing, and through a comprehensive survey, we summarize and categorize the efforts on fog computing and its related computing paradigms. Finally, we provide challenges and future directions for research in fog computing.}
}
@article{CAPODIECI2021481,
title = {Improving emergency response in the era of ADAS vehicles in the Smart City},
journal = {ICT Express},
volume = {7},
number = {4},
pages = {481-486},
year = {2021},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2021.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405959521000382},
author = {Nicola Capodieci and Roberto Cavicchioli and Filippo Muzzini and Leonard Montagna},
keywords = {Smart City, Multi-agent systems, Simulation},
abstract = {Management of emergency vehicles can be fostered within a Smart City, i.e. an urban environment in which many IoT devices are orchestrated by a distributed intelligence able to suggest to road users the best course of action in different traffic situations. By extending MATSim (Multi-Agent Transport Simulation Software), we design and test appropriate mitigation strategies when traffic accidents occur within an existing urban area augmented with V2V (Vehicle-to-Vehicle), V2I (Vehicle-to-Infrastructure) capabilities and Advanced Driving Assisted cars (ADAS). Further, we propose traffic congestion models and related mechanisms for improving the necessary time for emergency vehicles to respond to accidents.}
}
@article{KHEBBEB2020101821,
title = {A Maude-Based rewriting approach to model and verify Cloud/Fog self-adaptation and orchestration},
journal = {Journal of Systems Architecture},
volume = {110},
pages = {101821},
year = {2020},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101821},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120301132},
author = {Khaled Khebbeb and Nabil Hameurlain and Faiza Belala},
keywords = {Self-adaptation, Orchestration, Fog computing, Cloud computing, Formal methods, Rewriting logic, Linear temporal logic, Maude},
abstract = {In the IoT-Fog-Cloud landscape, IoT devices are connected to numerous software applications in order to fully operate. Some applications are deployed on the Fog layer, providing low-latency access to resource, whilst others are deployed on the Cloud to provide important resource capabilities and process heavy computation. In this distributed landscape, the deployment infrastructure has to adapt to the highly dynamic requirements of the IoT layer. However, due to their intrinsic properties, the Fog layer may lack of providing sufficient amount of resource while the Cloud layer fails ensuring low-latency requirements. In this paper, we present a rewriting-based approach to design and verify the Cloud-Fog self-adaption and orchestration behaviors in order to manage infrastructure reconfiguration towards achieving low-latency and resources quantity trade-offs. We rely of the formal specification language Maude to provide an executable solution of these behaviors basing on the rewriting logic and we express properties with linear temporal logic (LTL) to qualitatively verify the adaptations correctness.}
}
@article{JEFERRY2015227,
title = {Challenges Emerging from Future Cloud Application Scenarios},
journal = {Procedia Computer Science},
volume = {68},
pages = {227-237},
year = {2015},
note = {1st International Conference on Cloud Forward: From Distributed to Complete Computing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.238},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915030835},
author = {Keith Jeferry and George Kousiouris and Dimosthenis Kyriazis and Jörn Altmann and Augusto Ciuffoletti and Ilias Maglogiannis and Paolo Nesi and Bojan Suzic and Zhiming Zhao},
keywords = {Cloud computing, future application scenarios, challenges, complete computing},
abstract = {The cloud computing paradigm encompasses several key differentiating elements and technologies, tackling a number of inefficiencies, limitations and problems that have been identified in the distributed and virtualized computing domain. Nonetheless, and as it is the case for all emerging technologies, their adoption led to the presentation of new challenges and new complexities. In this paper we present key application areas and capabilities of future scenarios, which are not tackled by current advancements and highlight specific requirements and goals for advancements in the cloud computing domain. We discuss these requirements and goals across different focus areas of cloud computing, ranging from cloud service and application integration, development environments and abstractions, to interoperability and relevant to it aspects such as legislation. The future application areas and their requirements are also mapped to the aforementioned areas in order to highlight their dependencies and potential for moving cloud technologies forward and contributing towards their wider adoption.}
}
@article{YVONNET2011614,
title = {Finite element model of ionic nanowires with size-dependent mechanical properties determined by ab initio calculations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {200},
number = {5},
pages = {614-625},
year = {2011},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2010.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0045782510002665},
author = {J. Yvonnet and A. Mitrushchenkov and G. Chambaud and Q.-C. He},
keywords = {Surface stress, Size effects, Nanowires,  calculations},
abstract = {A finite element procedure for modeling crystalline nanostructures such as nanowires is proposed. The size effects exhibited by nanoobjects are captured by taking into account a surface energy, following the classical Gurtin–Murdoch surface elasticity theory. An appropriate variational form and a finite element approach are provided to model and solve relevant problems numerically. We describe a simplified technique based on projection operators for constructing the surface elements. The methodology is completed with a computational procedure based on ab initio calculations to extract elastic coefficients of general anisotropic surfaces. The FEM continuum model is validated by comparisons with complete ab initio models of nanowires with different diameters where size-dependent mechanical properties are observed. The FEM continuum model can then be used to model similar nanostructures in ranges of sizes or geometries where analytical or atomistic model is limited. The validated model is applied to the analysis of size effects in the bending of an AlN nanowire.}
}
@article{20067300,
title = {Contents of Volume 195},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {195},
number = {52},
pages = {7300-7314},
year = {2006},
note = {Computational Modelling of Concrete},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(06)00279-9},
url = {https://www.sciencedirect.com/science/article/pii/S0045782506002799}
}
@article{LEAMY2007874,
title = {Bulk dynamic response modeling of carbon nanotubes using an intrinsic finite element formulation incorporating interatomic potentials},
journal = {International Journal of Solids and Structures},
volume = {44},
number = {3},
pages = {874-894},
year = {2007},
issn = {0020-7683},
doi = {https://doi.org/10.1016/j.ijsolstr.2006.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S0020768306001855},
author = {Michael J. Leamy},
keywords = {Nanotube, Nanoscale material, Finite element, Dynamic, Nonlinear, Computational nanomechanics, Reduced order},
abstract = {This article presents an efficient explicit dynamic formulation for modeling curved and twisted carbon nanotubes (CNT’s) based on a recently-developed intrinsic continua description (i.e., the dynamic state given by curvatures, strains, and velocities only) [Hodges, D.H., 2003. Geometrically-exact, intrinsic theory for dynamics of curved and twisted, anisotropic beams. AIAA Journal 41(6), 1131–1137.] together with a finite element discretization incorporating atomistic potentials. This approach offers several advantages primarily related to the model’s computational efficiency: (1) the resulting partial differential equations governing motion are in first-order form (i.e., have first-order time derivatives only), (2) the system nonlinearities appear at low order, (3) the intrinsic description incorporating curvature allows low-order interpolation functions to describe generally curved and twisted nanotube center-lines, (4) inter-element displacements, slopes, and curvatures are matched at the element boundaries, and (5) finite rotational variables are absent, along with their inherit complexities. In addition, the developed model and finite element discretization are able to capture the nanotube’s bulk (equivalently, zero-temperature) dynamic response, without the expense of calculating the dynamic response of individual atoms as per Molecular Dynamics models. Simulation results are presented which illustrate the bulk dynamic response of a typical CNT to axial, bending, and torsional loading. Results from the simulations are compared to similar results available in the literature, and close agreement is documented.}
}
@article{YAN200797,
title = {Recent Advances in Computational Simulation of Macro-, Meso-, and Micro-Scale Biomimetics Related Fluid Flow Problems},
journal = {Journal of Bionic Engineering},
volume = {4},
number = {2},
pages = {97-107},
year = {2007},
issn = {1672-6529},
doi = {https://doi.org/10.1016/S1672-6529(07)60021-3},
url = {https://www.sciencedirect.com/science/article/pii/S1672652907600213},
author = {Y.Y. Yan},
keywords = {biomimetics, computational simulation, macro-, meso-, micro-scale, hydrophobic, surfaces},
abstract = {Over the last decade, computational methods have been intensively applied to a variety of scientific researches and engineering designs. Although the computational fluid dynamics (CFD) method has played a dominant role in studying and simulating transport phenomena involving fluid flow and heat and mass transfers, in recent years, other numerical methods for the simulations at meso-and micro-scales have also been actively applied to solve the physics of complex flow and fluid-interface interactions. This paper presents a review of recent advances in multi-scale computational simulation of biomimetics related fluid flow problems. The state-of-the-art numerical techniques, such as lattice Boltzmann method (LBM), molecular dynamics (MD), and conventional CFD, applied to different problems such as fish flow, electro-osmosis effect of earthworm motion, and self-cleaning hydrophobic surface, and the numerical approaches are introduced. The new challenging of modelling biomimetics problems in developing the physical conditions of self-clean hydrophobic surfaces is discussed.}
}
@article{DUTTAROY20082263,
title = {Biomechanical modelling of normal pressure hydrocephalus},
journal = {Journal of Biomechanics},
volume = {41},
number = {10},
pages = {2263-2271},
year = {2008},
issn = {0021-9290},
doi = {https://doi.org/10.1016/j.jbiomech.2008.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0021929008001966},
author = {Tonmoy Dutta-Roy and Adam Wittek and Karol Miller},
keywords = {Normal pressure hydrocephalus, Brain, Biomechanics},
abstract = {This study investigates the mechanics of normal pressure hydrocephalus (NPH) growth using a computational approach. We created a generic 3-D brain mesh of a healthy human brain and modelled the brain parenchyma as single phase and biphasic continuum. In our model, hyperelastic constitutive law and finite deformation theory described deformations within the brain parenchyma. We used a value of 155.77Pa for the shear modulus (μ) of the brain parenchyma. Additionally, in our model, contact boundary definitions constrained the brain outer surface inside the skull. We used transmantle pressure difference to load the model. Fully nonlinear, implicit finite element procedures in the time domain were used to obtain the deformations of the ventricles and the brain. To the best of our knowledge, this was the first 3-D, fully nonlinear model investigating NPH growth mechanics. Clinicians generally accept that at most 1mm of Hg transmantle pressure difference (133.416Pa) is associated with the condition of NPH. Our computations showed that transmantle pressure difference of 1mm of Hg (133.416Pa) did not produce NPH for either single phase or biphasic model of the brain parenchyma. A minimum transmantle pressure difference of 1.764mm of Hg (235.44Pa) was required to produce the clinical condition of NPH. This suggested that the hypothesis of a purely mechanical basis for NPH growth needs to be revised. We also showed that under equal transmantle pressure difference load, there were no significant differences between the computed ventricular volumes for biphasic and incompressible/nearly incompressible single phase model of the brain parenchyma. As a result, there was no major advantage gained by using a biphasic model for the brain parenchyma. We propose that for modelling NPH, nearly incompressible single phase model of the brain parenchyma was adequate. Single phase treatment of the brain parenchyma simplified the mathematical description of the NPH model and resulted in significant reduction of computational time.}
}
@article{GOKTEPE20101661,
title = {A generic approach towards finite growth with examples of athlete's heart, cardiac dilation, and cardiac wall thickening},
journal = {Journal of the Mechanics and Physics of Solids},
volume = {58},
number = {10},
pages = {1661-1680},
year = {2010},
issn = {0022-5096},
doi = {https://doi.org/10.1016/j.jmps.2010.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022509610001328},
author = {Serdar Göktepe and Oscar John Abilez and Ellen Kuhl},
keywords = {Biological growth, Biological material, Constitutive behavior, Finite elements, Numerical algorithms},
abstract = {The objective of this work is to establish a generic continuum-based computational concept for finite growth of living biological tissues. The underlying idea is the introduction of an incompatible growth configuration which naturally introduces a multiplicative decomposition of the deformation gradient into an elastic and a growth part. The two major challenges of finite growth are the kinematic characterization of the growth tensor and the identification of mechanical driving forces for its evolution. Motivated by morphological changes in cell geometry, we illustrate a micromechanically motivated ansatz for the growth tensor for cardiac tissue that can capture both strain-driven ventricular dilation and stress-driven wall thickening. Guided by clinical observations, we explore three distinct pathophysiological cases: athlete's heart, cardiac dilation, and cardiac wall thickening. We demonstrate the computational solution of finite growth within a fully implicit incremental iterative Newton–Raphson based finite element solution scheme. The features of the proposed approach are illustrated and compared for the three different growth pathologies in terms of a generic bi-ventricular heart model.}
}
@article{ADELZADEH2008186,
title = {Computational modeling of the interaction of two edge cracks, and two edge cracks interacting with a nanovoid, via an atomistic finite element method},
journal = {Computational Materials Science},
volume = {42},
number = {2},
pages = {186-193},
year = {2008},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2007.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0927025607001917},
author = {M. Adelzadeh and H.M. Shodja and H. Rafii-Tabar},
keywords = {Interacting cracks and nanovoid, Brittle-to-ductile transition, SIF, Interatomic potential function, Atomistic finite element method, Dislocation emission},
abstract = {The competition and interaction of two edge cracks within the triangular lattice of an fcc material are addressed. We have also examined the effect of presence of a nanovoid in the vicinity of one of the crack-tips, on the competition of the cracks. An atomic scale finite element method (AFEM) [B. Liu, Y. Huang, H. Jiang, S. Qu, K.C. Hwang, The atomic-scale finite element method, Comput. Methods Appl. Mech. Eng. 193 (2004) 1849–1864], based on the Morse interatomic potential, is employed to explore the events in the (111) plane. Particular attention is given to the phenomenon of brittle-to-ductile transition (BDT) that occurs during crack propagation.}
}
@incollection{ANDREA2009875,
title = {Paradoxes, Self-Reference and Truth in the 20th Century},
editor = {Dov M. Gabbay and John Woods},
series = {Handbook of the History of Logic},
publisher = {North-Holland},
volume = {5},
pages = {875-1013},
year = {2009},
booktitle = {Logic from Russell to Church},
issn = {1874-5857},
doi = {https://doi.org/10.1016/S1874-5857(09)70020-2},
url = {https://www.sciencedirect.com/science/article/pii/S1874585709700202},
author = {Cantini Andrea}
}
@article{LOCKERBY2013344,
title = {Time-step coupling for hybrid simulations of multiscale flows},
journal = {Journal of Computational Physics},
volume = {237},
pages = {344-365},
year = {2013},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2012.11.032},
url = {https://www.sciencedirect.com/science/article/pii/S0021999112007127},
author = {Duncan A. Lockerby and Carlos A. Duque-Daza and Matthew K. Borg and Jason M. Reese},
keywords = {Multiscale simulations, Unsteady micro/nano flows, Hybrid methods, Scale separation},
abstract = {A new method is presented for the exploitation of time-scale separation in hybrid continuum-molecular models of multiscale flows. Our method is a generalisation of existing approaches, and is evaluated in terms of computational efficiency and physical/numerical error. Comparison with existing schemes demonstrates comparable, or much improved, physical accuracy, at comparable, or far greater, efficiency (in terms of the number of time-step operations required to cover the same physical time). A leapfrog coupling is proposed between the ‘macro’ and ‘micro’ components of the hybrid model and demonstrates potential for improved numerical accuracy over a standard simultaneous approach. A general algorithm for a coupled time step is presented. Three test cases are considered where the degree of time-scale separation naturally varies during the course of the simulation. First, the step response of a second-order system composed of two linearly-coupled ODEs. Second, a micro-jet actuator combining a kinetic treatment in a small flow region where rarefaction is important with a simple ODE enforcing mass conservation in a much larger spatial region. Finally, the transient start-up flow of a journal bearing with a cylindrical rarefied gas layer. Our new time-stepping method consistently demonstrates as good as or better performance than existing schemes. This superior overall performance is due to an adaptability inherent in the method, which allows the most-desirable aspects of existing schemes to be applied only in the appropriate conditions.}
}
@incollection{20073,
title = {Subject Index},
editor = {I. Milne and R.O. Ritchie and B. Karihaloo},
booktitle = {Comprehensive Structural Integrity},
publisher = {Pergamon},
address = {Oxford},
pages = {3-169},
year = {2007},
isbn = {978-0-08-043749-1},
doi = {https://doi.org/10.1016/B978-008043749-1/09152-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780080437491091526}
}
@article{LI2013011001,
title = {Computational multiscale methods for granular materials},
journal = {Theoretical and Applied Mechanics Letters},
volume = {3},
number = {1},
pages = {011001},
year = {2013},
issn = {2095-0349},
doi = {https://doi.org/10.1063/2.1301101},
url = {https://www.sciencedirect.com/science/article/pii/S2095034915302002},
author = {Xikui Li and Yuanbo Liang and Youyao Du and Ke Wan and Qinglin Duan},
keywords = {granular material, discrete particle assembly, gradient Cosserat continuum, computational homogenization, bridge scale method, damage characterization},
abstract = {The fine-scale heterogeneity of granular material is characterized by its polydisperse microstructure with randomness and no periodicity. To predict the mechanical response of the material as the microstructure evolves, it is demonstrated to develop computational multiscale methods using discrete particle assembly-Cosserat continuum modeling in micro- and macro- scales, respectively. The computational homogenization method and the bridge scale method along the concurrent scale linking approach are briefly introduced. Based on the weak form of the Hu-Washizu variational principle, the mixed finite element procedure of gradient Cosserat continuum in the frame of the second-order homogenization scheme is developed. The meso-mechanically informed anisotropic damage of effective Cosserat continuum is characterized and identified and the microscopic mechanisms of macroscopic damage phenomenon are revealed.}
}
@article{MATSUDA2013349,
title = {3D-CG Based Stress Calculation of Competitive Swimwear Using Anisotropic Hyperelastic Model},
journal = {Procedia Engineering},
volume = {60},
pages = {349-354},
year = {2013},
note = {6th Asia-Pacific Congress on Sports Technology (APCST)},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2013.07.070},
url = {https://www.sciencedirect.com/science/article/pii/S1877705813011211},
author = {Akihiro Matsuda and Hiromu Tanabe and Takeya Nagaoka and Motomu Nakashima and Takatsugu Shimana and Kazuhiro Omori},
keywords = {Anisotropic hyperelasticity, Swimmear, Stress calculation},
abstract = {A new design method for designing competitive swimsuits using 3-dimensional stress calculation was investigated in this paper. An anisotropic hyperelastic modeling of swimwear fabric was developed from the results of tensile loading tests. 3-dimensional stress distributions of swimwear in swimming motions were calculated by the anisotropic hyperelastic model. The displacement fields of 3D-CG human model which reproduce swimming motion of human body were applied to stress calculation. For the material modeling, the uniaxial cyclic tensile loading tests were conducted to obtain the mechanical characteristics of competitive swimsuits. From loading test results, the mechanical characteristics of swimwear fabrics show strong-anisotropy and the stiffness of the fabric shows hardening along with the increase of stretch. The cyclic tensile loading test results shows reduction of stiffness which depended on the maximum deformation previously reached in the history of the material. To take the strong anisotropy and stress reduction into account for the material modeling, a stress-softening model for anisotropic hyperelastic model using stiffness ratio of warp or weft was proposed. The results of the simulation showed good agreement with that of pressure test in a design range of competitive swimsuits. Finally, 3-dimensional stress distributions of swimwear were calculated by the proposed anisotropic hyperelastic model. A polygonal model of the swimwear was prepared and deformation of swimwear was adapted to the skin of 3D-CG human model in swimming motions. From the results, stress distributions were able to be visualized on the 3D-CG swimwear model.}
}
@article{ZOHDI2012206,
title = {Modeling and simulation of electrification delivery in functionalized textiles in electromagnetic fields},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {245-246},
pages = {206-216},
year = {2012},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2012.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S004578251200223X},
author = {T.I. Zohdi},
keywords = {Textiles, Electromagnetism, Multiphysics, Dynamics},
abstract = {This work investigates the deformation of electrified textiles in the presence of an externally supplied magnetic field (Bext). The electrification is delivered by running current (J) through the fibers from an external power source. Of primary interest is to ascertain the resulting electromagnetic forces imposed on the fabric, and the subsequent deformation, due to the terms J×Bext and PE, where P is the charge density, E is the electric field and the current given by J=σ(E+v×Bext), where σ is the fabric conductivity, and v is the fabric velocity. As the fabric deforms, the current changes direction and magnitude, due to the fact that it flows through the fabric. The charge density is dictated by Gauss’ law, ∇·D=P, where D=ϵE,ϵ is the electrical permittivity and D is the electric field flux. In order to simulate such a system, one must solve a set of coupled equations governing the charge distribution, current flow and system dynamics. The deformation of the fabric, as well as the charge distribution and current flow, are dictated by solving the coupled system of differential equations for the motion of lumped masses, which are coupled through the fiber-segments under the action of electromagnetically-induced forces acting on a reduced order network model. In the work, reduced order models are developed for (a) Gauss’ law (∇·D=P), (b) the conservation of current/charge, ∇·J+∂P∂t=0, and (c) the system dynamics, ∇·T+f=ρdvdt, where T is the Cauchy stress and f represents the induced body forces, which are proportional to PE+J×Bext. A temporally-adaptive, recursive, staggering scheme is developed to solve this strongly coupled system of equations. We also consider the effects of progressive fiber damage/rupture during the deformation process, which leads to changes (reduction) in the electrical conductivity and permittivity throughout the network. Numerical examples are given, as well as extensions to thermal effects, which are induced by the current-induced Joule-heating.}
}
@article{WU2011387,
title = {Space Debris Reentry Analysis Methods and Tools},
journal = {Chinese Journal of Aeronautics},
volume = {24},
number = {4},
pages = {387-395},
year = {2011},
issn = {1000-9361},
doi = {https://doi.org/10.1016/S1000-9361(11)60046-0},
url = {https://www.sciencedirect.com/science/article/pii/S1000936111600460},
author = {Ziniu WU and Ruifeng HU and Xi QU and Xiang WANG and Zhe WU},
keywords = {space debris, reentry, ablation, breakup, risk assessment, aerodynamics},
abstract = {The reentry of uncontrolled spacecraft may be broken into many pieces of debris at an altitude in the range of 75-85 km. The surviving fragments could pose great hazard and risk to ground and people. In recent years, methods and tools for predicting and analyzing debris reentry and ground risk assessment have been studied and developed in National Aeronautics and Space Administration (NASA), European Space Agency (ESA) and other organizations, including the group of the present authors. This paper reviews the current progress on this topic of debris reentry briefly. We outline the Monte Carlo method for uncertainty analysis, breakup prediction, and parameters affecting survivability of debris. The existing analysis tools can be classified into two categories, i.e. the object-oriented and the spacecraft-oriented methods, the latter being more accurate than the first one. The past object-oriented tools include objects of only simple shapes. For more realistic simulation, here we present an object-oriented tool debris reentry and ablation prediction system (DRAPS) developed by the present authors, which introduces new object shapes to 15 types, as well as 51 predefined motions and relevant aerodynamic and aerothermal models. The aerodynamic and aerothermal models in DRAPS are validated using direct simulation Monte Carlo (DSMC) method.}
}
@article{YU20061906,
title = {A simple mixed finite element for static limit analysis},
journal = {Computers & Structures},
volume = {84},
number = {29},
pages = {1906-1917},
year = {2006},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2006.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0045794906002495},
author = {X. Yu and F. Tin-Loi},
keywords = {Limit analysis, Mixed finite element, Plasticity, Collapse loads},
abstract = {In this paper, we investigate the behavior of a simple mixed finite element for the limit analysis of plane structures. In particular, its ability to overcome incompressibility locking in plane strain situations is investigated. The element is constructed from a piecewise constant displacement field and a piecewise bilinear stress field, and is used within a mathematical programming based discrete representation of the classical static formulation. Several benchmark examples of both plane stress and plane strain situations are solved to illustrate the predictive accuracy and to assess the large-scale capability of the element. The results are compared with those obtained by a recent sophisticated enhanced strain mixed element formulation.}
}
@article{REESE20101276,
title = {Finite element-based multi-phase modelling of shape memory polymer stents},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {199},
number = {21},
pages = {1276-1286},
year = {2010},
note = {Multiscale Models and Mathematical Aspects in Solid and Fluid Mechanics},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2009.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S004578250900262X},
author = {S. Reese and M. Böl and D. Christ},
keywords = {Large deformations, Finite element method, Rubber–glass transition, Phase transformation},
abstract = {In the western hemisphere almost the half of all events of death are caused by cardiovascular diseases, e.g. strokes and heart attacks. The latter are consequences of arteriosclerosis leading to abnormal deposits (plaque) in blood vessels. In order to avoid the serious symptoms discussed in the above or to hold affected blood vessels open, tubular structures made of metallic or polymeric materials (stents) are implanted. In the paper we discuss the modelling of a new kind of stents, so-called shape memory polymer (SMP) stents. The first part of the paper is devoted to the thermo-mechanical modelling of these materials. Aspects as the transition from entropy to energy elasticity are included. The constitutive equations are derived in the framework of large strains. We follow both, a purely macroscopic as well as a micromechanically motivated approach. In the second part of the work representative examples based on realistic stent structures are used to validate the model.}
}
@article{CHENG2011191,
title = {Multi-scale simulations of in-depth pyrolysis of charring ablative thermal protection material},
journal = {Computers & Fluids},
volume = {45},
number = {1},
pages = {191-196},
year = {2011},
note = {22nd International Conference on Parallel Computational Fluid Dynamics (ParCFD 2010)},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2010.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S0045793010003087},
author = {Gary C. Cheng and Balaji Shankar Venkatachari and Ioana Cozmuta},
keywords = {TPS, CFD, Reactive MD, Atomistic analysis, Pyrolysis gas, Surface ablation},
abstract = {Charring ablative thermal protection systems have been commonly used to protect the payload of a hypersonic or space exploration vehicle from exposure to high heat loads. The physical phenomena associated with the pyrolysis of the charring ablative material are very complex. The existing surface ablation models were built upon various assumptions, which introduce large uncertainties in the engineering design process and disable the direct assessment of uncertainties at engineering level. The current study proposes a multi-scale numerical model to simulate the in-depth pyrolysis process of a charring ablator. First, a numerical model for simulating the transport and chemical reactions of the pyrolysis gas as a continuum through the porous char layer of an ablative material is developed, validated, and verified. The continuum numerical model is designed to account for the effects of in-depth heat and mass transfer, and chemical kinetics of the pyrolysis gas, which are missing from the existing surface ablation models. Next, atomistic simulations are used as a tool to determine the pyrolysis gas composition entering the char layer from the virgin material, and to identify the main reaction pathways for the interaction between the pyrolysis gases. The pyrolysis gas composition and kinetic chemistry model are intended to be used to reduce the uncertainty associated with the continuum numerical model used to simulate transport and reaction of the pyrolysis gas through the char in the surface ablation process. The current status of the multi-scale pyrolysis model development and results from the validation and verification studies are presented in the paper.}
}
@article{HEDRIH2006167,
title = {TRANSVERSAL VIBRATIONS OF THE AXIALLY MOVING SANDWICH DOUBLE BELT SYSTEM WITH CREEP LAYER},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {11},
pages = {167-172},
year = {2006},
note = {2nd IFAC Workshop on Fractional Differentiation and its Applications},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20060719-3-PT-4902.00030},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015364934},
author = {Katica (Stevanović) Hedrih},
keywords = {Moving sandwich double belt system, partial fractional order differential coupled equations, analytical solutions, multi-frequency vibrations},
abstract = {Based on the knowledge from literature on free transversal vibrations of the one axially moving string, in this paper, coupled partial fractional order differential equations of the transversal vibrations of the axially moving sandwich double belt system with creep layer are derived and analytically solved.}
}
@article{GASSER2006617,
title = {Modeling the propagation of arterial dissection},
journal = {European Journal of Mechanics - A/Solids},
volume = {25},
number = {4},
pages = {617-633},
year = {2006},
note = {6th EUROMECH Solid Mechanics Conference},
issn = {0997-7538},
doi = {https://doi.org/10.1016/j.euromechsol.2006.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0997753806000465},
author = {T. Christian Gasser and Gerhard A. Holzapfel},
keywords = {Artery, Cohesive zone model, Dissection, Peeling, Crack propagation, Partition of unity finite element method},
abstract = {Arterial dissections are frequently observed in clinical practice and during road traffic accidents. In particular, the lamellarly arrangement of elastin, collagen, in addition to smooth muscle cells in the middle arterial layer, the media, favors dissection failure. Experimental studies and related biomechanical models are rare in the literature. Finite strain kinematics is employed, and the discontinuity in the displacement field accounts for tissue separation. Dissection is regarded as a gradual process in which separation between incipient material surfaces is resisted by cohesive traction. Two variational statements together with their consistent linearizations form the basis for a finite element implementation. We combine the cohesive crack concept with the partition of unity finite element method, where nodal degrees of freedom adjacent to the discontinuity are enhanced. The developed continuum mechanical and numerical frameworks allow the analysis of the propagation of dissections within general nonlinear boundary-value problems, where the constitutive description for the continuous and the cohesive material is considered independent from each other. The continuous material is modeled as a fiber-reinforced composite with the fibers corresponding to the collagenous component which are assumed to be embedded in a non-collagenous isotropic groundmatrix. Dispersion of the collagen fiber orientation is considered in a continuum sense by one structure parameter. A novel cohesive potential per unit undeformed area is used to derive a traction separation law appropriate for the description of the mechanical properties of medial dissection. The cohesive stiffness contribution to the element stiffness matrix is explicitly derived. In particular, the dissection propagation of a rectangular strip of a human aortic media is investigated. Cohesive material properties are quantified by comparing the experimentally measured load with the computed dissection load.}
}
